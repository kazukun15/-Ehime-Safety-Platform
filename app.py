# =============================================
# Ehime Incident/Disaster Map â€“ v2 (Gazetteer + UI polish)
# - Streamlit app
# - Data: Ehime Police "äº‹ä»¶äº‹æ•…é€Ÿå ±" (scrape)
# - NLP: Google Gemini 2.5 Flash (structured JSON)
# - Gazetteer-first geocoding with fuzzy matching; fallback to Nominatim
# - Modern UI/UX: sidebar filters, chips, cards, confidence color scale
# - Initial map center: Ehime Prefectural Office (~33.8390, 132.7650)
# - GTFS intentionally removed per spec
# ---------------------------------------------
# How to run:
#   pip install -r requirements.txt
#   export GEMINI_API_KEY="..."
#   streamlit run app_v2.py
# ---------------------------------------------

import os
import re
import json
import math
import time
import random
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import requests
import pandas as pd
import streamlit as st
import pydeck as pdk
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process as rf_process

APP_TITLE = "æ„›åª›ï¼šäº‹ä»¶ãƒ»äº‹æ•…ãƒ»ç½å®³ ãƒãƒƒãƒ— (v2)"
EHIME_POLICE_URL = "https://www.police.pref.ehime.jp/sokuho/sokuho.htm"
USER_AGENT = "EhimeCivic/1.0; contact: localgov"
REQUEST_TIMEOUT = 15

# Initial center (Ehime Prefectural Government Office vicinity)
EHIME_PREF_LAT = 33.8390
EHIME_PREF_LON = 132.7650

GEMINI_MODEL = "gemini-2.5-flash"
SLEEP_RANGE = (0.8, 1.5)

# -------- Streamlit page config & CSS --------
st.set_page_config(page_title=APP_TITLE, layout="wide")
st.markdown(
    """
    <style>
      :root { --card-bg:#11111110; }
      .big-title {font-size:1.6rem; font-weight:700;}
      .chip {display:inline-block; padding:4px 10px; border-radius:999px; margin:0 6px 6px 0; background:#eceff1; font-size:0.9rem;}
      .chip.on {background:#1e88e5; color:#fff}
      .subtle {color:#666}
      .legend {font-size:0.9rem;}
      .stButton>button {border-radius:999px;}
      .feed-card {background:var(--card-bg); padding:12px 14px; border-radius:12px; border:1px solid #e0e0e0;}
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(f"<div class='big-title'>ğŸ—ºï¸ {APP_TITLE}</div>", unsafe_allow_html=True)

# -------- Sidebar controls --------
st.sidebar.header("è¡¨ç¤ºé …ç›®")
show_accidents = st.sidebar.checkbox("äº‹æ•…æƒ…å ±", True)
show_crimes = st.sidebar.checkbox("çŠ¯ç½ªæƒ…å ±", True)
show_disasters = st.sidebar.checkbox("ç½å®³æƒ…å ±(è­¦å ±ç­‰)", True)

st.sidebar.header("å–å¾—ãƒ»ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼")
if st.sidebar.button("çœŒè­¦é€Ÿå ±ã‚’å†å–å¾—"):
    st.session_state.pop("_incidents_cache", None)
    st.session_state.pop("_analysis_cache", None)

gazetteer_path = st.sidebar.text_input("ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢CSVãƒ‘ã‚¹", "data/gazetteer_ehime.csv")
use_fuzzy = st.sidebar.checkbox("ã‚†ã‚‰ãå¯¾å¿œï¼ˆãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒï¼‰", True)
min_fuzzy_score = st.sidebar.slider("æœ€å°ã‚¹ã‚³ã‚¢", 60, 95, 78)

st.sidebar.caption("â€» å‚è€ƒæƒ…å ±ã€‚ç·Šæ€¥æ™‚ã¯110/119ã€‚ç¾åœ°ã®æŒ‡ç¤ºã‚’å„ªå…ˆã€‚å€‹äººæƒ…å ±ã¯è¡¨ç¤ºã—ã¾ã›ã‚“ã€‚")

# -------- HTTP utils --------

def http_get(url: str) -> str:
    r = requests.get(url, timeout=REQUEST_TIMEOUT, headers={"User-Agent": USER_AGENT})
    r.raise_for_status()
    return r.text

# -------- Scraper: Ehime Police --------
@dataclass
class IncidentItem:
    source_url: str
    heading: str
    station: Optional[str]
    incident_date: Optional[str]
    body: str
    fetched_at: str


def parse_ehime_police_page(html: str) -> List[IncidentItem]:
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    lines = [ln for ln in text.split("\n") if ln.strip()]

    blocks: List[Dict] = []
    current = None
    for ln in lines:
        if ln.startswith("â– "):
            if current:
                blocks.append(current)
            current = {"heading": ln.strip(), "body_lines": []}
        else:
            if current:
                current["body_lines"].append(ln.strip())
    if current:
        blocks.append(current)

    out: List[IncidentItem] = []
    today = datetime.now().date()
    cy = today.year

    for b in blocks:
        heading = b.get("heading", "")
        body = " ".join(b.get("body_lines", [])).strip()
        m_date = re.search(r"ï¼ˆ?(\d{1,2})æœˆ(\d{1,2})æ—¥", heading)
        m_station = re.search(r"ï¼ˆ\d{1,2}æœˆ\d{1,2}æ—¥\s*([^\sï¼‰]+)ï¼‰", heading)

        incident_date = None
        if m_date:
            m = int(m_date.group(1)); d = int(m_date.group(2))
            y = cy; cand = datetime(y, m, d).date()
            if cand > today: y -= 1
            incident_date = datetime(y, m, d).date().isoformat()

        station = m_station.group(1) if m_station else None
        out.append(IncidentItem(
            source_url=EHIME_POLICE_URL,
            heading=heading,
            station=station,
            incident_date=incident_date,
            body=body,
            fetched_at=datetime.now().astimezone().isoformat(timespec="seconds"),
        ))
    return out

# -------- Gemini (google-generativeai) --------

def gemini_client():
    import google.generativeai as genai
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        st.error("GEMINI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(GEMINI_MODEL)


def gemini_analyze_items(items: List[IncidentItem]) -> List[Dict]:
    model = gemini_client()
    sys_prompt = (
        "ã‚ãªãŸã¯æ—¥æœ¬ã®è¡Œæ”¿å‘ã‘NLPã‚¢ãƒŠãƒ©ã‚¤ã‚¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸé€Ÿå ±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å³å¯†ãªJSONã‚’è¿”ã—ã¾ã™ã€‚"
        "äº‹å®Ÿã®è£œå®Œã¯ç¦æ­¢ã€‚å‡ºåŠ›ã¯ application/json ã®ã¿ã€‚æ¬ æ¸¬ã¯ nullã€‚"
    )
    results: List[Dict] = []
    for it in items:
        user_prompt = f"""
ã€å‡ºåŠ›JSONãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€‘
category: "äº¤é€šäº‹æ•…"|"ç«ç½"|"äº‹ä»¶"|"æ­»äº¡äº‹æ¡ˆ"|"çªƒç›—"|"è©æ¬º"|"ãã®ä»–"
municipality: å¸‚ç”ºæ‘å / ä¸æ˜ã¯ null
place_strings: æ–½è¨­ãƒ»åœ°åå€™è£œã®é…åˆ—
road_refs: é“è·¯å‚ç…§é…åˆ—ï¼ˆä¾‹: å›½é“/çœŒé“ï¼‰
occurred_date: YYYY-MM-DD / ä¸æ˜ã¯ null
occurred_time_text: "æœ/æ˜¼/å¤œ/æœªæ˜/â—‹æ™‚â—‹åˆ†é ƒ" ç­‰ / ä¸æ˜ã¯ null
summary_ja: 120å­—ä»¥å†…ã®è¦ç´„ï¼ˆåŸæ–‡æº–æ‹ ãƒ»æ†¶æ¸¬ç¦æ­¢ï¼‰
confidence: 0.0ã€œ1.0
raw_heading: è¦‹å‡ºã—
raw_snippet: é‡è¦éƒ¨åˆ†ã®åŸæ–‡ï¼ˆ100ã€œ200å­—ï¼‰

ã€æ—¢çŸ¥ãƒ¡ã‚¿ã€‘ç½²å:{it.station} æ¨å®šæ—¥:{it.incident_date} å¹´ç¯„å›²:{datetime.now().year}-01-01ã€œ{datetime.now().date().isoformat()}
ã€å…¥åŠ›ã€‘è¦‹å‡ºã—:{it.heading}\næœ¬æ–‡:{it.body}
"""
        gen_cfg = {
            "temperature": 0.2,
            "top_p": 0.9,
            "top_k": 40,
            "max_output_tokens": 1024,
            "response_mime_type": "application/json",
        }
        try:
            resp = model.generate_content([
                {"role":"system","parts":[sys_prompt]},
                {"role":"user","parts":[user_prompt]},
            ], generation_config=gen_cfg)
            txt = (resp.text or "").strip()
            data = json.loads(txt) if txt else {}
        except Exception:
            data = {}

        data.setdefault("category", "ãã®ä»–")
        data.setdefault("municipality", None)
        data.setdefault("place_strings", [])
        data.setdefault("road_refs", [])
        data.setdefault("occurred_date", it.incident_date)
        data.setdefault("occurred_time_text", None)
        data.setdefault("summary_ja", None)
        data.setdefault("confidence", 0.4)
        data.setdefault("raw_heading", it.heading)
        data.setdefault("raw_snippet", it.body[:200])
        data["source_url"] = it.source_url
        data["fetched_at"] = it.fetched_at

        results.append(data)
        time.sleep(random.uniform(*SLEEP_RANGE))
    return results

# -------- Gazetteer (CSV) --------
@st.cache_data(show_spinner=False)
def load_gazetteer(csv_path: str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(csv_path)
        # expected columns: name, alt_names, type, lon, lat, area_m2(optional)
        for col in ["name","type","lon","lat"]:
            if col not in df.columns:
                st.warning("ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢ã®åˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: name,type,lon,lat ã¯å¿…é ˆ")
                return None
        df["alt_names"] = df.get("alt_names", "").fillna("")
        return df
    except Exception:
        return None


def gazetteer_lookup(place: str, gaz: pd.DataFrame, use_fuzzy: bool, min_score: int) -> Optional[Tuple[float,float,str]]:
    # 1) direct substring or exact
    m = gaz[(gaz["name"].str.contains(place, na=False)) | (gaz["alt_names"].str.contains(place, na=False))]
    if not m.empty:
        r = m.iloc[0]
        return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore

    # 2) fuzzy (against name + alt_names concatenated)
    if use_fuzzy:
        choices = (gaz["name"] + " | " + gaz["alt_names"].fillna(""))
        match = rf_process.extractOne(place, choices, scorer=fuzz.token_set_ratio)
        if match and match[1] >= min_score:
            idx = match[2]
            r = gaz.iloc[idx]
            return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore

    return None

# Nominatim fallback (respect rate) â€” for production, replace with licensed API

def geocode_nominatim(name: str, municipality: Optional[str]) -> Optional[Tuple[float,float]]:
    try:
        q = f"{name} {municipality or ''} æ„›åª›çœŒ æ—¥æœ¬".strip()
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": q, "format": "jsonv2", "limit": 1}
        r = requests.get(url, params=params, timeout=15, headers={"User-Agent": USER_AGENT})
        r.raise_for_status()
        arr = r.json()
        if isinstance(arr, list) and arr:
            lat = float(arr[0]["lat"])  # type: ignore
            lon = float(arr[0]["lon"])  # type: ignore
            return lon, lat
    except Exception:
        return None
    return None

# circle polygon for pydeck

def circle_coords(lon: float, lat: float, radius_m: int = 300, n: int = 64) -> List[List[float]]:
    coords: List[List[float]] = []
    r_earth = 6378137.0
    dlat = radius_m / r_earth
    dlon = radius_m / (r_earth * math.cos(math.radians(lat)))
    for i in range(n):
        ang = 2 * math.pi * i / n
        lat_i = lat + math.degrees(dlat * math.sin(ang))
        lon_i = lon + math.degrees(dlon * math.cos(ang))
        coords.append([lon_i, lat_i])
    coords.append(coords[0])
    return coords

# radius rules

def decide_radius_m(match_type: str) -> int:
    if match_type == "facility":
        return 300
    if match_type == "intersection":
        return 250
    if match_type in ("town","chome"):
        return 600
    if match_type in ("oaza","aza"):
        return 900
    if match_type in ("city","municipality"):
        return 2000
    return 800

# -------- Optional JMA warnings (placeholder) --------

def fetch_jma_warnings_prefecture(pref_code: str = "38") -> List[Dict]:
    try:
        # Placeholder: return [] to avoid unreliable endpoints in MVP
        return []
    except Exception:
        return []

# -------- Data pipeline (cache) --------
@st.cache_data(ttl=300)
def load_incidents() -> List[IncidentItem]:
    html = http_get(EHIME_POLICE_URL)
    return parse_ehime_police_page(html)

@st.cache_data(ttl=300)
def analyze_incidents(items: List[IncidentItem]) -> pd.DataFrame:
    data = gemini_analyze_items(items)
    return pd.DataFrame(data)

# -------- Run pipeline --------
with st.spinner("çœŒè­¦é€Ÿå ±ã®å–å¾—ãƒ»è§£æä¸­..."):
    items = load_incidents()
    an_df = analyze_incidents(items)

# Sidebar date filter
an_df["occurred_date"] = pd.to_datetime(an_df["occurred_date"], errors="coerce")
min_date = an_df["occurred_date"].min()
max_date = an_df["occurred_date"].max()
if pd.notna(min_date) and pd.notna(max_date):
    dr = st.sidebar.date_input("ç™ºç”Ÿæ—¥ãƒ•ã‚£ãƒ«ã‚¿", value=(min_date.date(), max_date.date()))
    if isinstance(dr, tuple) and len(dr) == 2:
        d0, d1 = pd.to_datetime(dr[0]), pd.to_datetime(dr[1])
        an_df = an_df[(an_df["occurred_date"] >= d0) & (an_df["occurred_date"] <= d1)]

# Category chips
cats = sorted([c for c in an_df["category"].dropna().unique().tolist() if c])
st.write(" ".join([f"<span class='chip on'>{c}</span>" for c in cats]), unsafe_allow_html=True)

# Gazetteer load
gaz_df = load_gazetteer(gazetteer_path) if gazetteer_path else None

# ---- Build map features with gazetteer-first geocoding ----
features = []
for _, row in an_df.iterrows():
    if row.get("category") in ("äº¤é€šäº‹æ•…","äº‹ä»¶","çªƒç›—","è©æ¬º","æ­»äº¡äº‹æ¡ˆ"):
        if not (show_accidents or show_crimes):
            continue
    # ç½å®³ã¯åˆ¥é€”ï¼ˆJMAï¼‰

    municipality: Optional[str] = row.get("municipality")
    places: List[str] = row.get("place_strings") or []

    lonlat: Optional[Tuple[float,float]] = None
    mtype = "unknown"

    # try gazetteer
    if gaz_df is not None:
        for ptxt in places:
            g = gazetteer_lookup(ptxt, gaz_df, use_fuzzy, min_fuzzy_score)
            if g:
                lonlat = (g[0], g[1])
                mtype = g[2]
                break
        if not lonlat and municipality:
            g = gazetteer_lookup(municipality, gaz_df, use_fuzzy, min_fuzzy_score)
            if g:
                lonlat = (g[0], g[1])
                mtype = g[2]

    # fallback to OSM if still missing
    if not lonlat:
        for ptxt in places:
            pt = geocode_nominatim(ptxt, municipality)
            if pt:
                lonlat = pt
                mtype = "facility"
                break
            time.sleep(1.0)
        if not lonlat and municipality:
            pt = geocode_nominatim(municipality, None)
            if pt:
                lonlat = pt
                mtype = "city"
            time.sleep(1.0)

    if not lonlat:
        continue

    lon, lat = lonlat
    radius_m = decide_radius_m(mtype)
    conf = float(row.get("confidence", 0.4))
    color = [255, 140, 0, int(40 + min(160, conf*160))]  # opacity by confidence

    props = {
        "title": row.get("category", "ãã®ä»–"),
        "summary": row.get("summary_ja"),
        "municipality": municipality,
        "source": row.get("source_url"),
        "fetched_at": row.get("fetched_at"),
        "confidence": conf,
        "radius_m": radius_m,
        "match_type": mtype,
        "raw_heading": row.get("raw_heading"),
    }
    features.append({
        "type": "Feature",
        "geometry": {"type": "Polygon", "coordinates": [circle_coords(lon, lat, radius_m)]},
        "properties": {**props, "_fill": color},
    })

geojson = {"type":"FeatureCollection","features":features}

# ---- Map ----
view_state = pdk.ViewState(latitude=EHIME_PREF_LAT, longitude=EHIME_PREF_LON, zoom=9)
layer_incidents = pdk.Layer(
    "GeoJsonLayer",
    data=geojson,
    pickable=True,
    stroked=True,
    filled=True,
    get_line_width=2,
    get_line_color=[230,90,20],
    get_fill_color="properties._fill",
    auto_highlight=True,
)

layers = []
if show_accidents or show_crimes:
    layers.append(layer_incidents)

tooltip = {"html": "<b>{title}</b><br/>{summary}<br/><span class='subtle'>{municipality}</span><br/>åŠå¾„:{radius_m}m / conf:{confidence}",
           "style": {"backgroundColor":"#111","color":"white"}}

deck = pdk.Deck(layers=layers, initial_view_state=view_state, tooltip=tooltip)

col_map, col_feed = st.columns([7,5], gap="large")
with col_map:
    st.pydeck_chart(deck, use_container_width=True)
    st.markdown(
        """
        <div class='legend'>
          <span class='chip'>å‡¡ä¾‹</span> å††ã¯ã€Œè¿‘ä¼¼ç¯„å›²ã€ã§ã™ï¼ˆãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚
          å‡ºå…¸URLã¨å–å¾—æ™‚åˆ»ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚
        </div>
        """,
        unsafe_allow_html=True,
    )

with col_feed:
    st.subheader("ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤è¦ç´„ï¼ˆé€Ÿå ±ï¼‰")
    # quick filters
    cat_filter = st.multiselect("ã‚«ãƒ†ã‚´ãƒªçµè¾¼", options=cats, default=cats)

    feed = an_df.copy()
    if cat_filter:
        feed = feed[feed["category"].isin(cat_filter)]

    # Search box for text
    q = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆè¦ç´„/åŸæ–‡ï¼‰")
    if q:
        feed = feed[feed.apply(lambda r: q in (r.get("summary_ja") or "") or q in (r.get("raw_snippet") or ""), axis=1)]

    # Card renderer
    for _, r in feed.iterrows():
        st.markdown("<div class='feed-card'>", unsafe_allow_html=True)
        st.markdown(f"**{r.get('category','ãã®ä»–')}**")
        st.caption(r.get("summary_ja") or "è¦ç´„ãªã—")
        st.caption(f"{r.get('municipality') or 'å¸‚ç”ºæ‘ä¸æ˜'} / å–å¾—: {r.get('fetched_at')} / conf: {r.get('confidence')}")
        st.link_button("å‡ºå…¸ã‚’é–‹ã", r.get("source_url"), help="çœŒè­¦ãƒšãƒ¼ã‚¸")
        st.markdown("</div>", unsafe_allow_html=True)

    if show_disasters:
        st.markdown("---")
        st.subheader("ç½å®³æƒ…å ±ï¼ˆè­¦å ±ãƒ»æ³¨æ„å ±ï¼‰")
        jma = fetch_jma_warnings_prefecture("38")
        if not jma:
            st.caption("JMAè­¦å ±ã®å–å¾—ã¯æœªè¨­å®šã§ã™ã€‚ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæä¾›å¾Œã«å®Ÿè£…ã—ã¾ã™ã€‚")
        else:
            for w in jma:
                st.write(w)

# Export area
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    if st.button("è§£æçµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
        js = an_df.to_json(force_ascii=False, orient="records", indent=2)
        st.download_button("download incidents.json", js, file_name="incidents.json", mime="application/json")
with col2:
    st.caption("Â© Data: æ„›åª›çœŒè­¦ äº‹ä»¶äº‹æ•…é€Ÿå ± / Geocoding: Gazetteerâ†’OSM(Nominatim). ã“ã®ã‚¢ãƒ—ãƒªã¯å‚è€ƒæƒ…å ±ã§ã™ã€‚")

# -------- requirements.txt (reference) --------
# streamlit
# pandas
# pydeck
# requests
# beautifulsoup4
# google-generativeai>=0.8.0
# rapidfuzz
