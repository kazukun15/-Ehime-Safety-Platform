# -*- coding: utf-8 -*-
# æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  v9.9-A8 (ticker: seamless loop)
# - ãƒˆãƒƒãƒ—ã®é›»å…‰æ²ç¤ºæ¿ã‚’ç„¡é™ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ï¼‰
# - é€Ÿåº¦ã¯ 45sï¼ˆCSSã® animation-duration ã‚’å¤‰æ›´ã™ã‚Œã°èª¿æ•´å¯ï¼‰
# - æ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼šJARTICç‚¹/ç·šã€OSMã‚¹ãƒŠãƒƒãƒ—(0ã€œ10km)ã€å±é™ºäº¤å·®ç‚¹ON/OFFã€çœŒè­¦é€Ÿå ±â†’ä½ç½®æ¨å®š ç­‰
#
# ä¾å­˜:
#   pip install streamlit pydeck pandas requests httpx beautifulsoup4 rapidfuzz h3 streamlit-autorefresh

import os, re, math, time, json, sqlite3, threading, unicodedata, hashlib
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from io import StringIO

import httpx
import requests
import pandas as pd
import streamlit as st
import pydeck as pdk
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process as rf_process
import h3
from streamlit_autorefresh import st_autorefresh

# === Gemini (optional) ===
try:
    import google.generativeai as genai
    _HAS_GEMINI = True
except Exception:
    _HAS_GEMINI = False

APP_TITLE = "æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ "
SUBTITLE = "Save Your Self"
EHIME_POLICE_URL = "https://www.police.pref.ehime.jp/sokuho/sokuho.htm"
USER_AGENT = "ESP/9.9-A8 (ticker_loop+snap_subpath_0_10km)"
TIMEOUT = 15
TTL_HTML = 600
MAX_WORKERS = 6

EHIME_PREF_LAT = 33.8390
EHIME_PREF_LON = 132.7650
EHIME_BBOX = (132.2, 33.0, 133.7, 34.2)  # (minLon, minLat, maxLon, maxLat)

FUTURE_BUFFER_SCALE = 1.8
ZOOM_LIKE = 10
FANOUT_THRESHOLD = 4
LABEL_SCALE = 1.0
MAX_LABELS = 400

CATEGORY_PATTERNS = [
    ("äº¤é€šäº‹æ•…", r"äº¤é€š.*äº‹æ•…|è‡ªè»¢è»Š|ãƒã‚¹|äºŒè¼ª|ä¹—ç”¨|è¡çª|äº¤å·®ç‚¹|å›½é“|çœŒé“|äººèº«äº‹æ•…"),
    ("ç«ç½",     r"ç«ç½|å‡ºç«|å…¨ç„¼|åŠç„¼|å»¶ç„¼"),
    ("æ­»äº¡äº‹æ¡ˆ", r"æ­»äº¡|æ­»äº¡äº‹æ¡ˆ"),
    ("çªƒç›—",     r"çªƒç›—|ä¸‡å¼•|ç›—"),
    ("è©æ¬º",     r"è©æ¬º|é‚„ä»˜é‡‘|æŠ•è³‡è©æ¬º|ç‰¹æ®Šè©æ¬º"),
    ("äº‹ä»¶",     r"å¨åŠ›æ¥­å‹™å¦¨å®³|æ¡ä¾‹é•å|æš´è¡Œ|å‚·å®³|è„…è¿«|å™¨ç‰©æå£Š|é’å°‘å¹´ä¿è­·"),
]
CITY_NAMES = [
    "æ¾å±±å¸‚","ä»Šæ²»å¸‚","æ–°å±…æµœå¸‚","è¥¿æ¡å¸‚","å¤§æ´²å¸‚","ä¼Šäºˆå¸‚","å››å›½ä¸­å¤®å¸‚",
    "è¥¿äºˆå¸‚","æ±æ¸©å¸‚","ä¸Šå³¶ç”º","ä¹…ä¸‡é«˜åŸç”º","æ¾å‰ç”º","ç ¥éƒ¨ç”º","å†…å­ç”º",
    "ä¼Šæ–¹ç”º","æ¾é‡ç”º","é¬¼åŒ—ç”º","æ„›å—ç”º","å®‡å’Œå³¶å¸‚","å…«å¹¡æµœå¸‚"
]
MUNI_CENTERS = {
    "æ¾å±±å¸‚":(132.7650,33.8390),"ä»Šæ²»å¸‚":(133.0000,34.0660),"æ–°å±…æµœå¸‚":(133.2830,33.9600),
    "è¥¿æ¡å¸‚":(133.1830,33.9180),"å¤§æ´²å¸‚":(132.5500,33.5000),"ä¼Šäºˆå¸‚":(132.7010,33.7550),
    "å››å›½ä¸­å¤®å¸‚":(133.5500,33.9800),"è¥¿äºˆå¸‚":(132.5000,33.3660),"æ±æ¸©å¸‚":(132.8710,33.7930),
    "ä¸Šå³¶ç”º":(133.2000,34.2600),"ä¹…ä¸‡é«˜åŸç”º":(132.9040,33.5380),"æ¾å‰ç”º":(132.7110,33.7870),
    "ç ¥éƒ¨ç”º":(132.7870,33.7350),"å†…å­ç”º":(132.6580,33.5360),"ä¼Šæ–¹ç”º":(132.3560,33.4880),
    "æ¾é‡ç”º":(132.7570,33.2260),"é¬¼åŒ—ç”º":(132.8800,33.2280),"æ„›å—ç”º":(132.5660,33.0000),
    "å®‡å’Œå³¶å¸‚":(132.5600,33.2230),"å…«å¹¡æµœå¸‚":(132.4230,33.4620),
}

TILESETS: Dict[str, Dict] = {
    "æ¨™æº–":     {"url":"https://tile.openstreetmap.org/{z}/{x}/{y}.png", "max_zoom":19},
    "æ·¡è‰²":     {"url":"https://cyberjapandata.gsi.go.jp/xyz/pale/{z}/{x}/{y}.png", "max_zoom":18},
    "åœ°å½¢":     {"url":"https://a.tile.opentopomap.org/{z}/{x}/{y}.png", "max_zoom":17},
    "äººé“æ”¯æ´": {"url":"https://tile-a.openstreetmap.fr/hot/{z}/{x}/{y}.png", "max_zoom":19},
    "èˆªç©ºå†™çœŸ": {"url":"https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg", "max_zoom":18},
}

# ----------------------------------------------------------------------------
# UI / CSS
# ----------------------------------------------------------------------------
st.set_page_config(page_title=APP_TITLE, layout="wide")
st.markdown("""
<style>
  :root{ --bg:#0b0f14; --panel:#0f141b; --panel2:#121924; --text:#e8f1ff; --muted:#8aa0b6; --border:#2b3a4d; --a:#007aff; --b:#00b894; }
  @media (prefers-color-scheme: light){ :root{ --bg:#f7fafc; --panel:#ffffff; --panel2:#f1f5f9; --text:#0f2230; --muted:#586b7a; --border:#dfe7ef; --a:#005acb; --b:#009a7a; } }
  html, body, .stApp { background: var(--bg); color: var(--text); }
  .topbar{ position: sticky; top:0; z-index:10; padding:14px 16px; margin:-16px -16px 14px -16px; border-bottom:1px solid var(--border); background:var(--panel); }
  .brand{ display:flex; align-items:center; gap:10px; font-weight:800; font-size:1.05rem; }
  .brand .id{ width:28px; height:28px; border-radius:8px; display:grid; place-items:center; background: linear-gradient(135deg,var(--a),var(--b)); color:#00131a; font-weight:900; }
  .subnote{ color: var(--muted); font-size:.85rem; margin-top:4px}
  .panel { background: var(--panel); border:1px solid var(--border); border-radius: 14px; padding: 10px 12px; }
  .legend { font-size:.95rem; background:var(--panel); border:1px solid var(--border); border-radius:12px; padding:10px 12px;}
  .legend .item { display:inline-flex; align-items:center; margin-right:14px; margin-bottom:6px}
  .dot { width:12px; height:12px; border-radius:50%; display:inline-block; margin-right:6px; border:1px solid #0003}
  .feed-card {background:var(--panel); padding:12px 14px; border-radius:14px; border:1px solid var(--border); margin-bottom:10px;}
  .feed-scroll {max-height:62vh; overflow-y:auto; padding-right:6px}
  @media (max-width: 640px){ .feed-scroll{max-height:48vh} }
  a { color: var(--a); }
  .riskbar{height:10px; border-radius:6px; background:linear-gradient(90deg,#ffd4d4,#ff6b6b,#d90429);}
  .risklbl{display:flex; justify-content:space-between; font-size:.85rem; color: var(--muted); margin-top:4px}
  .hud { position: relative; height:0; }
  .hud-inner { position: relative; top:-36px; left:6px; display:flex; gap:6px; flex-wrap:wrap; }
  .hud .badge { background:rgba(16,20,27,.88); color:#e8f1ff; border:1px solid var(--border); padding:4px 8px; border-radius:10px; font-size:.85rem; }
  @media (prefers-color-scheme: light){ .hud .badge{ background:rgba(255,255,255,.9); color:#0f2230; } }
  .jartic-grad { height: 10px; border-radius:6px; background: linear-gradient(90deg, #3c78c8, #4ec67a, #f0d438, #e63c3c); }
  .redline-sample { height: 0; border-top:4px solid #e60000; width: 80px; border-radius:2px; }

  /* â˜… Seamless ticker (loop) */
  .ticker-wrap{ position:sticky; top:58px; z-index:9; margin:-12px -16px 10px -16px;
                background:var(--panel2); border-bottom:1px solid var(--border); overflow:hidden; }
  .ticker{ display:flex; gap:60px; white-space:nowrap; padding:6px 0;
           will-change: transform; animation: ticker-move 45s linear infinite; }
  .ticker:hover{ animation-play-state: paused; }
  .ticker-seq{ display:inline-block; }
  @keyframes ticker-move {
    0%   { transform: translateX(0); }
    100% { transform: translateX(-50%); } /* 2åˆ—åˆ†ã®ã†ã¡åŠåˆ†ã ã‘å·¦ã¸ â†’ é€”åˆ‡ã‚Œãšç„¡é™ãƒ«ãƒ¼ãƒ— */
  }
  @media (prefers-reduced-motion: reduce){
    .ticker{ animation: none; }
  }
</style>
""", unsafe_allow_html=True)

st.markdown(
    "<div class='topbar'><div class='brand'><div class='id'>ES</div>"
    f"<div><div>{APP_TITLE}</div><div class='subnote'>{SUBTITLE}</div></div>"
    "</div></div>", unsafe_allow_html=True
)

# ----------------------------------------------------------------------------
# Session state
# ----------------------------------------------------------------------------
if "map_choice" not in st.session_state:
    st.session_state.map_choice = "æ¨™æº–"
if "show_jartic_points" not in st.session_state:
    st.session_state.show_jartic_points = True
if "show_snap_lines" not in st.session_state:
    st.session_state.show_snap_lines = True
if "show_hotspots" not in st.session_state:
    st.session_state.show_hotspots = True

# ----------------------------------------------------------------------------
# SQLite cache
# ----------------------------------------------------------------------------
@st.cache_resource
def get_sqlite():
    os.makedirs("data", exist_ok=True)
    conn = sqlite3.connect("data/esp_cache.sqlite", check_same_thread=False)
    with conn:
        conn.execute("CREATE TABLE IF NOT EXISTS geocode_cache(key TEXT PRIMARY KEY, json TEXT, created_at TEXT)")
        conn.execute("CREATE TABLE IF NOT EXISTS llm_cache(key TEXT PRIMARY KEY, json TEXT, created_at TEXT)")
    return conn
_conn = get_sqlite(); _conn_lock = threading.Lock()
def cache_get(table:str, key:str) -> Optional[str]:
    with _conn_lock:
        row = _conn.execute(f"SELECT json FROM {table} WHERE key=?", (key,)).fetchone()
    return row[0] if row else None
def cache_put(table:str, key:str, payload:str):
    with _conn_lock, _conn:
        _conn.execute(f"INSERT OR REPLACE INTO {table} VALUES (?,?,datetime('now'))", (key, payload))

# ----------------------------------------------------------------------------
# çœŒè­¦é€Ÿå ±ã®å–å¾—ãƒ»è§£æ
# ----------------------------------------------------------------------------
@dataclass
class IncidentItem:
    heading: str
    body: str
    incident_date: Optional[str]

@st.cache_data(ttl=TTL_HTML)
def fetch_ehime_html() -> str:
    headers = {"User-Agent": USER_AGENT}
    for attempt in range(3):
        try:
            with httpx.Client(headers=headers, timeout=TIMEOUT) as client:
                r = client.get(EHIME_POLICE_URL)
                if r.status_code in (429, 503):
                    raise httpx.HTTPStatusError("rate limited", request=None, response=r)
                r.raise_for_status()
                enc = r.encoding or 'utf-8'
                try:
                    r.encoding = r.apparent_encoding or enc
                except Exception:
                    r.encoding = enc
                return r.text
        except Exception:
            time.sleep(0.6 * (attempt + 1))
    try:
        r = requests.get(EHIME_POLICE_URL, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        r.encoding = r.apparent_encoding or r.encoding or "utf-8"
        return r.text
    except Exception:
        st.warning("é€Ÿå ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç©ºãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¤ºã—ã¾ã™ã€‚")
        return "<html><body></body></html>"

def parse_items(html: str) -> List[IncidentItem]:
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    text = re.sub(r"ã€æ„›åª›çœŒè­¦ã‹ã‚‰ã®ãŠé¡˜ã„ï¼ã€‘[\s\S]*?(?=â– |$)", "", text)
    lines = [ln for ln in text.split("\n") if ln.strip()]
    items, cur = [], None
    for ln in lines:
        if ln.startswith("â– "):
            if cur: items.append(cur)
            cur = {"heading": ln.strip(), "body": []}
        else:
            if cur: cur["body"].append(ln.strip())
    if cur: items.append(cur)

    out: List[IncidentItem] = []
    today = datetime.now().date(); cy = today.year
    for b in items:
        h = b.get("heading", ""); body = " ".join(b.get("body", []))
        m = re.search(r"ï¼ˆ?(\d{1,2})æœˆ(\d{1,2})æ—¥", h)
        d = None
        if m:
            mm, dd = int(m.group(1)), int(m.group(2)); y = cy
            try:
                dt = datetime(y, mm, dd).date();  y -= 1 if dt > today else 0
                d = datetime(y, mm, dd).date().isoformat()
            except Exception:
                d = None
        out.append(IncidentItem(h.replace("â– ", "").strip(), body, d))
    return out

def rule_extract(it: IncidentItem) -> Dict:
    t = it.heading + " " + it.body
    cat = next((name for name, pat in CATEGORY_PATTERNS if re.search(pat, t)), "ãã®ä»–")
    muni = next((c for c in CITY_NAMES if c in t), None)
    hints = ["å°å­¦æ ¡","ä¸­å­¦æ ¡","é«˜æ ¡","å¤§å­¦","å­¦æ ¡","ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰","ä½“è‚²é¤¨","å…¬åœ’","é§…","æ¸¯","ç—…é™¢","äº¤å·®ç‚¹"]
    places: List[str] = []
    for h in hints:
        places += re.findall(rf"([\w\u3040-\u30ff\u4e00-\u9fffA-Za-z0-9]+{h})", t)[:2]
    s = re.sub(r"\s+", " ", it.body).strip()
    s = s[:120] + ("â€¦" if len(s)>120 else "")
    return {"category":cat,"municipality":muni,"place_strings":list(dict.fromkeys(places))[:3],
            "summary": s or it.heading, "date": it.incident_date}

@st.cache_resource
def load_gazetteer(path:str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        for c in ("name","type","lon","lat"):
            if c not in df.columns: return None
        df["alt_names"] = df.get("alt_names", "").fillna("")
        return df
    except Exception:
        return None

class GazetteerIndex:
    def __init__(self, df: pd.DataFrame):
        self.df = df.reset_index(drop=True)
        self.keys = (df["name"].astype(str) + " | " + df["alt_names"].astype(str)).tolist()
    def search(self, q:str, min_score:int=78) -> Optional[Tuple[float,float,str]]:
        m = self.df[(self.df["name"].str.contains(q, na=False)) | (self.df["alt_names"].str.contains(q, na=False))]
        if not m.empty:
            r = m.iloc[0]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        hit = rf_process.extractOne(q, self.keys, scorer=fuzz.token_set_ratio)
        if hit and hit[1] >= min_score:
            r = self.df.iloc[hit[2]]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        return None

def nominatim_geocode(name:str, municipality:Optional[str]) -> Optional[Tuple[float,float]]:
    try:
        q = f"{name} {municipality or ''} æ„›åª›çœŒ æ—¥æœ¬".strip()
        headers = {"User-Agent": USER_AGENT}
        params = {"q": q, "format": "jsonv2", "limit": 1}
        for i in range(3):
            r = requests.get("https://nominatim.openstreetmap.org/search", params=params, headers=headers, timeout=TIMEOUT)
            if r.status_code in (429,503):
                time.sleep(0.6 * (i+1)); continue
            r.raise_for_status()
            arr = r.json()
            if isinstance(arr, list) and arr:
                return float(arr[0]["lon"]), float(arr[0]["lat"])  # lon, lat
        return None
    except Exception:
        return None

def _read_gemini_key() -> str:
    try:
        if "gemini" in st.secrets and "API_KEY" in st.secrets["gemini"]:
            return st.secrets["gemini"]["API_KEY"]
    except Exception:
        pass
    return os.getenv("GEMINI_API_KEY", "")

def gemini_candidates(full_text: str, muni_hint: Optional[str]) -> List[Dict]:
    api_key = _read_gemini_key()
    if not (_HAS_GEMINI and api_key):
        return []
    key = hashlib.sha1(f"gem9|{muni_hint or ''}|{full_text}".encode("utf-8")).hexdigest()
    cached = cache_get("llm_cache", key)
    if cached:
        try:
            obj = json.loads(cached)
            return obj.get("candidates", []) if isinstance(obj, dict) else []
        except Exception:
            pass
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        system = ("ã‚ãªãŸã¯æ—¥æœ¬ã®ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢è£œåŠ©ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚"
                  "å…¥åŠ›ã®äº‹ä»¶ãƒ»äº‹æ•…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€åœ°åã‚„æ–½è¨­åå€™è£œã‚’æŠ½å‡ºã—ã¾ã™ã€‚"
                  "å‡ºåŠ›ã¯ {\"candidates\":[{\"name\":str,\"kind\":str,\"confidence\":0..1}]}")
        muni_line = f"å¸‚ç”ºãƒ’ãƒ³ãƒˆ: {muni_hint}\n" if muni_hint else ""
        prompt = f"{system}\n\nãƒ†ã‚­ã‚¹ãƒˆ:\n{full_text}\n\n{muni_line}JSONã®ã¿ã§å¿œç­”ã€‚"
        resp = model.generate_content(prompt)
        txt = resp.text if hasattr(resp, "text") else str(resp)
        start = txt.find("{"); end = txt.rfind("}")
        if start >=0 and end>start: txt = txt[start:end+1]
        obj = json.loads(txt)
        if isinstance(obj, dict):
            cache_put("llm_cache", key, json.dumps(obj, ensure_ascii=False))
            return obj.get("candidates", [])
    except Exception:
        return []
    return []

# ----------------------------------------------------------------------------
# H3 & helpers
# ----------------------------------------------------------------------------
def h3_cell_from_latlng(lat: float, lon: float, res: int) -> str:
    if hasattr(h3, "geo_to_h3"): return h3.geo_to_h3(lat, lon, res)
    return h3.latlng_to_cell(lat, lon, res)
def h3_latlng_from_cell(cell: str) -> Tuple[float,float]:
    if hasattr(h3, "h3_to_geo"):
        lat, lon = h3.h3_to_geo(cell); return lat, lon
    lat, lon = h3.cell_to_latlng(cell); return lat, lon
def h3_res_from_zoom(zoom_val:int) -> int:
    return {7:5,8:6,9:7,10:8,11:9,12:9,13:10,14:10}.get(zoom_val, 8)
def cluster_points(df: pd.DataFrame, zoom_val:int) -> List[Dict]:
    if df.empty: return []
    res = h3_res_from_zoom(zoom_val)
    groups: Dict[str, List[Dict]] = {}
    for _, r in df.iterrows():
        lon, lat = float(r["lon"]), float(r["lat"])
        cell = h3_cell_from_latlng(lat, lon, res)
        d = r.to_dict(); d["cell"] = cell
        groups.setdefault(cell, []).append(d)
    centers: List[Dict] = []
    for cell, rows in groups.items():
        lat, lon = h3_latlng_from_cell(cell)
        centers.append({"cell":cell, "lon":lon, "lat":lat, "count":len(rows), "rows":rows})
    return centers

def short_summary(s: str, max_len: int = 64) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    return (s[:max_len] + ("â€¦" if len(s) > max_len else "")) if s else ""

def make_prediction(category:str, muni:Optional[str]) -> str:
    return {
        "è©æ¬º":"SNSã‚„æŠ•è³‡ã®èª˜ã„ã«æ³¨æ„ã€‚é€é‡‘å‰ã«å®¶æ—ã‚„è­¦å¯Ÿã¸ç›¸è«‡ã€‚",
        "äº¤é€šäº‹æ•…":"å¤•æ–¹ã‚„é›¨å¤©ã®äº¤å·®ç‚¹ã§å¢—ãˆã‚„ã™ã„ã€‚æ¨ªæ–­ã¨å³å·¦æŠ˜ã«æ³¨æ„ã€‚",
        "çªƒç›—":"è‡ªè»¢è»Šãƒ»è»Šä¸¡ã®æ–½éŒ ã¨é˜²çŠ¯ç™»éŒ²ã€‚å¤œé–“ã®ç„¡æ–½éŒ æ”¾ç½®ã‚’é¿ã‘ã‚‹ã€‚",
        "ç«ç½":"ä¹¾ç‡¥æ™‚ã¯å±‹å¤–ç«æ°—ã«é…æ…®ã€‚é›»æºå‘¨ã‚Šãƒ»å–«ç…™ã®å§‹æœ«ã‚’å†ç¢ºèªã€‚",
        "äº‹ä»¶":"ä¸å¯©é€£çµ¡ã¯è¨˜éŒ²ã‚’æ®‹ã—é€šå ±ã€‚å­¦æ ¡ãƒ»å…¬å…±æ–½è¨­å‘¨è¾ºã§æ„è­˜ã‚’ã€‚",
        "æ­»äº¡äº‹æ¡ˆ":"è©³ç´°ã¯å‡ºå…¸ã§ç¢ºèªã€‚å‘¨è¾ºã§ã¯æ•‘æ€¥æ´»å‹•ã«é…æ…®ã€‚",
    }.get(category, "åŒç¨®äº‹æ¡ˆãŒç¶šãå¯èƒ½æ€§ã€‚å‡ºå…¸ã§æœ€æ–°ã‚’ç¢ºèªã€‚")

CAT_STYLE = {
    "äº¤é€šäº‹æ•…": {"color":[220, 60, 60, 235],   "radius":86, "icon":"â–²"},
    "ç«ç½":     {"color":[245, 130, 50, 235],  "radius":88, "icon":"ğŸ”¥"},
    "æ­»äº¡äº‹æ¡ˆ": {"color":[170, 120, 240, 235], "radius":92, "icon":"âœ–"},
    "çªƒç›—":     {"color":[70, 150, 245, 235],  "radius":78, "icon":"ğŸ”“"},
    "è©æ¬º":     {"color":[40, 180, 160, 235],  "radius":78, "icon":"âš "},
    "äº‹ä»¶":     {"color":[245, 200, 60, 235],  "radius":82, "icon":"ï¼"},
    "ãã®ä»–":   {"color":[128, 144, 160, 220], "radius":70, "icon":"ãƒ»"},
}

def make_incident_info_html(c:str, s:str, m:str, pred:str) -> str:
    parts = []
    if c: parts.append(f"<b>{c}</b>")
    if s: parts.append(s)
    if m: parts.append(m)
    if pred: parts.append(f"äºˆæ¸¬: {pred}")
    parts.append(f"<a href='{EHIME_POLICE_URL}' target='_blank'>å‡ºå…¸</a>")
    return "<div style='min-width:240px'>" + "<br/>".join(parts) + "</div>"

def make_jartic_info_html(tlabel:str, total:int, up:int, down:int) -> str:
    return (
        "<div style='min-width:240px'>"
        "<b>JARTICï¼ˆ5åˆ†å€¤ï¼‰</b><br/>"
        f"æ™‚åˆ»: {tlabel}<br/>"
        f"åˆè¨ˆ: {total} å°/5åˆ†<br/>"
        f"ä¸Šã‚Š: {up} / ä¸‹ã‚Š: {down}"
        "</div>"
    )

# ----------------------------------------------------------------------------
# JARTIC Open Traffic (5åˆ†å€¤)
# ----------------------------------------------------------------------------
JARTIC_WFS_URL = "https://api.jartic-open-traffic.org/geoserver"

def jst_now() -> datetime:
    return datetime.utcnow() + timedelta(hours=9)

def round_to_5min(d: datetime) -> datetime:
    mm = (d.minute // 5) * 5
    return d.replace(minute=mm, second=0, microsecond=0)

@st.cache_data(ttl=180)
def fetch_jartic_5min(bbox: Tuple[float,float,float,float] = EHIME_BBOX) -> Tuple[Optional[Dict], Optional[str]]:
    # ç´„20åˆ†é…å»¶ã€5åˆ†åˆ»ã¿
    t = round_to_5min(jst_now() - timedelta(minutes=20))
    tcode = t.strftime("%Y%m%d%H%M")
    tlabel = t.strftime("%Y-%m-%d %H:%M (JST)")
    cql = (
        f"é“è·¯ç¨®åˆ¥=3 AND æ™‚é–“ã‚³ãƒ¼ãƒ‰={tcode} AND "
        f"BBOX(ã‚¸ã‚ªãƒ¡ãƒˆãƒª,{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]},'EPSG:4326')"
    )
    params = {
        "service": "WFS", "version": "2.0.0", "request": "GetFeature",
        "typeNames": "t_travospublic_measure_5m",
        "srsName": "EPSG:4326", "outputFormat": "application/json",
        "exceptions": "application/json", "cql_filter": cql
    }
    try:
        r = requests.get(JARTIC_WFS_URL, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        return r.json(), tlabel
    except Exception:
        return None, None

def jartic_features_to_points(geojson: Dict, tlabel: str) -> List[Dict]:
    if not geojson or "features" not in geojson: return []
    pts: List[Dict] = []
    for f in geojson["features"]:
        g = f.get("geometry") or {}
        if g.get("type") == "MultiPoint":
            prop = f.get("properties", {}) or {}
            up = sum(filter(None, [
                prop.get("ä¸Šã‚Šãƒ»å°å‹äº¤é€šé‡"), prop.get("ä¸Šã‚Šãƒ»å¤§å‹äº¤é€šé‡"), prop.get("ä¸Šã‚Šãƒ»è»Šç¨®åˆ¤åˆ¥ä¸èƒ½äº¤é€šé‡")
            ]))
            down = sum(filter(None, [
                prop.get("ä¸‹ã‚Šãƒ»å°å‹äº¤é€šé‡"), prop.get("ä¸‹ã‚Šãƒ»å¤§å‹äº¤é€šé‡"), prop.get("ä¸‹ã‚Šãƒ»è»Šç¨®åˆ¤åˆ¥ä¸èƒ½äº¤é€šé‡")
            ]))
            up = int(up or 0); down = int(down or 0); total = up + down
            for lon, lat in g.get("coordinates", []):
                pts.append({
                    "position":[float(lon), float(lat)],
                    "up": up, "down": down, "total": total,
                    "jt_time": tlabel, "jt_total": total, "jt_up": up, "jt_down": down,
                })
    return pts

def color_from_total(total:int) -> List[int]:
    # é’â†’ç·‘â†’é»„â†’èµ¤
    t = min(1.0, max(0.0, total / 600.0))
    if t < 0.33:
        u = t/0.33; r = int(60*(1-u)+60*u); g = int(120*(1-u)+200*u); b = int(200*(1-u)+80*u)
    elif t < 0.66:
        u = (t-0.33)/0.33; r = int(60*(1-u)+240*u); g = int(200*(1-u)+220*u); b = int(80*(1-u)+20*u)
    else:
        u = (t-0.66)/0.34; r = int(240); g = int(220*(1-u)+60*u); b = 20
    return [r,g,b, 230]

def size_from_total(total:int) -> int:
    return 52 + int(min(260, total * 0.9))

# ----------------------------------------------------------------------------
# OSMï¼ˆOverpassï¼‰ï¼†æ“¬ä¼¼æ¸‹æ»ç·šï¼ˆèµ¤ï¼‰0ã€œ10kmï¼šã‚¦ã‚§ã‚¤ã«æ²¿ã£ãŸã‚µãƒ–ãƒ‘ã‚¹æŠ½å‡º
# ----------------------------------------------------------------------------
OVERPASS_URL = "https://overpass-api.de/api/interpreter"

@st.cache_data(ttl=600)
def fetch_osm_roads_overpass(bbox: Tuple[float,float,float,float] = EHIME_BBOX) -> List[Dict]:
    minLon, minLat, maxLon, maxLat = bbox
    q = f"""
    [out:json][timeout:25];
    (
      way["highway"~"^(motorway|trunk|primary|secondary|tertiary)$"]({minLat},{minLon},{maxLat},{maxLon});
    );
    out tags geom;
    """
    try:
        r = requests.post(OVERPASS_URL, data={"data": q}, timeout=TIMEOUT)
        r.raise_for_status()
        data = r.json()
        out: List[Dict] = []
        for el in data.get("elements", []):
            if el.get("type") == "way" and "geometry" in el:
                coords = [[pt["lon"], pt["lat"]] for pt in el["geometry"]]
                if len(coords) >= 2:
                    tags = el.get("tags", {}) or {}
                    oneway = str(tags.get("oneway","no")).lower() in ("yes","1","true")
                    out.append({"coords": coords, "oneway": oneway})
        return out
    except Exception:
        return []

def _meters_scale(lat: float) -> Tuple[float,float]:
    return 111320 * math.cos(math.radians(lat)), 110540

def _dist_m(a: Tuple[float,float], b: Tuple[float,float]) -> float:
    (lon1,lat1),(lon2,lat2) = a,b
    kx, ky = _meters_scale((lat1+lat2)/2)
    return math.hypot((lon2-lon1)*kx, (lat2-lat1)*ky)

def _project_to_segment(p: Tuple[float,float], a: Tuple[float,float], b: Tuple[float,float]) -> Tuple[Tuple[float,float], float, float]:
    ax, ay = a; bx, by = b; px, py = p
    kx, ky = _meters_scale((ay+by)/2)
    ax2, ay2, bx2, by2, px2, py2 = ax*kx, ay*ky, bx*kx, by*ky, px*kx, py*ky
    vx, vy = bx2-ax2, by2-ay2; wx, wy = px2-ax2, py2-ay2
    seglen2 = vx*vx + vy*vy
    if seglen2 == 0:
        return a, 0.0, math.hypot(px2-ax2, py2-ay2)
    t = max(0.0, min(1.0, (wx*vx + wy*vy) / seglen2))
    projx2, projy2 = ax2 + t*vx, ay2 + t*vy
    proj = (projx2 / kx, projy2 / ky)
    dist_m = math.hypot(px2-projx2, py2-projy2)
    return proj, t, dist_m

def _nearest_point_on_way(p: Tuple[float,float], way_coords: List[List[float]]) -> Tuple[int, float, Tuple[float,float], float]:
    best = (0, 0.0, tuple(way_coords[0]), float("inf"))
    for i in range(len(way_coords)-1):
        a = tuple(way_coords[i]); b = tuple(way_coords[i+1])
        proj, t, d = _project_to_segment(p, a, b)
        if d < best[3]:
            best = (i, t, proj, d)
    return best

def _subpath_centered_on(way_coords: List[List[float]], seg_idx: int, t: float, length_m: float) -> List[List[float]]:
    if length_m <= 0:
        length_m = 1.0
    half = length_m / 2.0
    a = way_coords[seg_idx]; b = way_coords[seg_idx+1]
    ax, ay = a; bx, by = b
    proj = [ax + (bx-ax)*t, ay + (by-ay)*t]

    back_pts: List[List[float]] = [proj]
    remain = half; i = seg_idx; cur = proj
    while i >= 0 and remain > 0:
        prev = way_coords[i]
        d = _dist_m(tuple(cur), tuple(prev))
        if d >= remain:
            ratio = remain / d if d > 0 else 0
            x = cur[0] + (prev[0]-cur[0]) * ratio
            y = cur[1] + (prev[1]-cur[1]) * ratio
            back_pts.append([x,y]); remain = 0; break
        else:
            back_pts.append(prev); remain -= d; cur = prev; i -= 1

    fwd_pts: List[List[float]] = [proj]
    remain = half; i = seg_idx + 1; cur = proj
    while i < len(way_coords) and remain > 0:
        nxt = way_coords[i]
        d = _dist_m(tuple(cur), tuple(nxt))
        if d >= remain:
            ratio = remain / d if d > 0 else 0
            x = cur[0] + (nxt[0]-cur[0]) * ratio
            y = cur[1] + (nxt[1]-cur[1]) * ratio
            fwd_pts.append([x,y]); remain = 0; break
        else:
            fwd_pts.append(nxt); remain -= d; cur = nxt; i += 1

    back_pts = back_pts[::-1]
    path = back_pts + fwd_pts[1:]
    if len(path) < 2:
        path.append(path[0][:])
    return path

@st.cache_data(ttl=180)
def build_snap_lines(j_points: List[Dict], ways: List[Dict],
                     base_min: int = 0, base_max: int = 10_000, thresh_m: int = 220) -> List[Dict]:
    out: List[Dict] = []
    if not j_points: return out
    RED = [230, 0, 0, 235]
    for jp in j_points:
        lon, lat = jp["position"]; p = (lon, lat)
        total = int(jp.get("total", jp.get("jt_total", 0)))
        length_m = max(base_min, min(base_max, total * 20.0))  # 0ã€œ10km

        best = None
        for wi, way in enumerate(ways):
            coords = way["coords"]
            seg_idx, t, proj, d = _nearest_point_on_way(p, coords)
            if (best is None) or (d < best[0]):
                best = (d, wi, seg_idx, t, proj)

        if best and best[0] <= thresh_m:
            _, wi, seg_idx, t, _ = best
            coords = ways[wi]["coords"]
            path = _subpath_centered_on(coords, seg_idx, t, length_m)
        else:
            kx, _ = _meters_scale(lat)
            half = length_m / 2.0
            dx_deg = half / max(kx, 1e-6)
            p1 = [lon - dx_deg, lat]; p2 = [lon + dx_deg, lat]
            path = [p1, p2]

        width_px = 5 + min(14, (total // 100))
        out.append({
            "path": path, "color": RED, "width": width_px,
            "info_html": make_jartic_info_html(
                jp.get("jt_time"), jp.get("jt_total", 0), jp.get("jt_up", 0), jp.get("jt_down", 0)
            ),
        })
    return out

# ----------------------------------------------------------------------------
# Pipeline: çœŒè­¦é€Ÿå ±â†’ä½ç½®æ¨å®š
# ----------------------------------------------------------------------------
with st.spinner("é€Ÿå ±ã‚’å–å¾—ä¸­â€¦"):
    html = fetch_ehime_html()
raw_items = parse_items(html)
extracted: List[Dict] = []
for it in raw_items:
    ex = rule_extract(it); ex["heading"] = it.heading; ex["full_text"] = (it.heading + " " + it.body).strip()
    extracted.append(ex)

gdf = load_gazetteer("data/gazetteer_ehime.csv")
idx = GazetteerIndex(gdf) if gdf is not None else None
def try_gazetteer(name:str, min_score:int=78) -> Optional[Tuple[float,float,str]]:
    return None if not idx else idx.search(name, min_score)

def resolve_loc(ex: Dict) -> Dict:
    muni = ex.get("municipality"); places = ex.get("place_strings") or []
    full_text = ex.get("full_text", "")
    llm_cands = gemini_candidates(full_text, muni)
    cand_names = [c.get("name","") for c in llm_cands if isinstance(c, dict)]
    queries = [q for q in cand_names if q] + places + ([muni] if muni else [])
    for q in queries:
        hit = try_gazetteer(q, 78)
        if hit:
            lon, lat, typ = hit; return {"lon":lon, "lat":lat, "type":typ or "facility"}
    if muni and muni in MUNI_CENTERS:
        lon, lat = MUNI_CENTERS[muni]; return {"lon":lon, "lat":lat, "type":"city"}
    for q in queries:
        ll = nominatim_geocode(q, muni)
        if ll: return {"lon":ll[0], "lat":ll[1], "type":"facility"}
    return {"lon":EHIME_PREF_LON, "lat":EHIME_PREF_LAT, "type":"pref"}

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exctr:
    results = list(exctr.map(resolve_loc, extracted))

rows: List[Dict] = []
for ex, loc in zip(extracted, results):
    typ = loc.get("type","city")
    base_radius = {"facility":300,"intersection":250,"town":600,"chome":600,"oaza":900,"aza":900,"city":2000,"pref":2500}.get(typ, 1200)
    radius = int(base_radius * FUTURE_BUFFER_SCALE)
    rows.append({
        "lon": float(loc["lon"]), "lat": float(loc["lat"]),
        "category": ex["category"], "summary": short_summary(ex["summary"], 60),
        "municipality": ex.get("municipality") or "",
        "radius_m": radius,
        "pred": make_prediction(ex["category"], ex.get("municipality")),
        "src": EHIME_POLICE_URL,
    })
df = pd.DataFrame(rows)

# å¤šç™ºäº¤å·®ç‚¹ï¼ˆå†…è”µï¼‰
CSV_TEXT = """åœ°ç‚¹å,ç·¯åº¦,çµŒåº¦,å¹´é–“æœ€å¤šäº‹æ•…ä»¶æ•°,è£œè¶³
å¤©å±±äº¤å·®ç‚¹,33.8223,132.7758,6,æ¾å±±å¸‚å¤©å±±ç”ºä»˜è¿‘ï¼ˆ2023å¹´ã«6ä»¶äº‹æ•…ï¼‰
å’Œæ³‰äº¤å·®ç‚¹,33.8216,132.7554,5,æ¾å±±å¸‚å’Œæ³‰ç”ºä»˜è¿‘ï¼ˆ2023å¹´ã«5ä»¶äº‹æ•…ï¼‰
å°å‚äº¤å·®ç‚¹,33.8266,132.7833,5,æ¾å±±å¸‚ææ¾åœ°åŒºï¼ˆ2023å¹´ã«5ä»¶äº‹æ•…ï¼‰
æœ¬ç”º5ä¸ç›®äº¤å·®ç‚¹,33.8530,132.7588,4,æ¾å±±å¸‚ä¸­å¿ƒéƒ¨ï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
å±±è¶Šäº¤å·®ç‚¹,33.8565,132.7592,4,æ¾å±±å¸‚å±±è¶Šï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
æ¶ˆé˜²å±€å‰äº¤å·®ç‚¹,33.8527,132.7588,4,æ¾å±±å¸‚æœ¬ç”º6ä¸ç›®ï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
å¤§å·æ©‹äº¤å·®ç‚¹,33.8739,132.7521,4,æ¾å±±å¸‚é´¨å·ç”ºï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
ä¹…ç±³äº¤å·®ç‚¹,33.8143,132.7957,4,æ¾å±±å¸‚ä¹…ç±³åœ°åŒºï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
"""
hot_df = pd.read_csv(StringIO(CSV_TEXT))
hot_df.rename(columns={"åœ°ç‚¹å":"name","ç·¯åº¦":"lat","çµŒåº¦":"lon","å¹´é–“æœ€å¤šäº‹æ•…ä»¶æ•°":"count","è£œè¶³":"note"}, inplace=True)
hot_df["lat"] = hot_df["lat"].astype(float); hot_df["lon"] = hot_df["lon"].astype(float)
hot_df["count"] = hot_df["count"].astype(int)
max_count = int(hot_df["count"].max()) if not hot_df.empty else 1
hot_df["position"] = hot_df.apply(lambda r: [float(r["lon"]), float(r["lat"])], axis=1)
hot_df["weight"] = hot_df["count"].astype(float)
def _cfc(c:int) -> List[int]:
    t = max(0.0, min(1.0, (c-1) / max(1, max_count-1)))
    if t < 0.5:
        u = t/0.5; r = int(255*(1-u)+255*u); g = int(212*(1-u)+107*u); b = int(212*(1-u)+107*u)
    else:
        u = (t-0.5)/0.5; r = int(255*(1-u)+217*u); g = int(107*(1-u)+4*u); b = int(107*(1-u)+41*u)
    return [r,g,b, 190 if c>=5 else 180]
hot_df["rgba"] = hot_df["count"].apply(_cfc)
hot_df["elev"] = hot_df["count"].apply(lambda c: 300 + (c-1)*220)

# ----------------------------------------------------------------------------
# JARTICãƒ‡ãƒ¼ã‚¿ï¼ˆprefetchï¼‰â†’ ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã«ã‚‚åˆ©ç”¨
# ----------------------------------------------------------------------------
gj_prefetch, tlabel_prefetch = fetch_jartic_5min(EHIME_BBOX)
j_points_prefetch: List[Dict] = (jartic_features_to_points(gj_prefetch, tlabel_prefetch)
                                 if (gj_prefetch and tlabel_prefetch) else [])

def build_ticker_items(incidents_df: pd.DataFrame, jpts: List[Dict], tlabel: Optional[str]) -> List[str]:
    items: List[str] = []
    for _, r in incidents_df.head(12).iterrows():
        cat = r.get("category",""); muni = r.get("municipality","") or ""; s = r.get("summary","")
        if s: items.append(f"{muni} {s}")
    if jpts and tlabel:
        top = sorted(jpts, key=lambda p: p.get("total",0), reverse=True)[:8]
        for p in top:
            total, up, down = p.get("total",0), p.get("up",0), p.get("down",0)
            items.append(f"ã€äº¤é€šé‡ {tlabel}ã€‘åˆè¨ˆ{total}å°/5åˆ†ï¼ˆâ†‘{up} / â†“{down}ï¼‰")
    return items

def render_ticker_html(lines: List[str]) -> None:
    if not lines: return
    # åŒã˜ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’2å›ä¸¦ã¹ã¦ã€translateX(-50%) ã®ç„¡é™ãƒ«ãƒ¼ãƒ—ã§åˆ‡ã‚Œç›®ãªãæµã™
    text = "ã€€ï½œã€€".join(lines)
    html = (
        "<div class='ticker-wrap'>"
        "  <div class='ticker'>"
        f"    <span class='ticker-seq'>{text}</span>"
        f"    <span class='ticker-seq' aria-hidden='true'>{text}</span>"
        "  </div>"
        "</div>"
    )
    st.markdown(html, unsafe_allow_html=True)

# â˜… ãƒ†ã‚£ãƒƒã‚«ãƒ¼æç”»ï¼ˆæœ€ä¸Šéƒ¨ï¼‰
ticker_lines = build_ticker_items(df, j_points_prefetch, tlabel_prefetch)
render_ticker_html(ticker_lines)
# è‡ªå‹•æ›´æ–°ï¼ˆ5åˆ†ã”ã¨ï¼‰
st_autorefresh(interval=5*60*1000, key="ticker_refresh")

# ----------------------------------------------------------------------------
# Sidebar
# ----------------------------------------------------------------------------
with st.sidebar:
    st.markdown("<div class='panel'>", unsafe_allow_html=True)

    st.markdown("#### è¡¨ç¤ºæœŸé–“")
    period = st.select_slider("è¡¨ç¤ºæœŸé–“ã‚’é¸æŠ",
                              ["å½“æ—¥","éå»3æ—¥","éå»7æ—¥","éå»30æ—¥"],
                              value="éå»7æ—¥", label_visibility="collapsed")

    st.markdown("#### è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰")
    mode_3d = st.radio("2D / 3D", ["2D","3D"], horizontal=True, index=0)
    init_zoom = st.slider("åˆæœŸã‚ºãƒ¼ãƒ ", 8, 17, 11)

    st.markdown("#### JARTIC äº¤é€šé‡ (5åˆ†)")
    st.session_state.show_jartic_points = st.checkbox("JARTIC 5åˆ†å€¤ï¼ˆç‚¹ï¼‰",
                                                      value=bool(st.session_state.show_jartic_points))
    st.session_state.show_snap_lines   = st.checkbox("JARTIC 5åˆ†å€¤ï¼ˆç·šï¼šOSMã‚¹ãƒŠãƒƒãƒ—ï¼‰",
                                                      value=bool(st.session_state.show_snap_lines))
    st.caption("å…¬é–‹API / ç´„20åˆ†é…å»¶ã€‚ç‚¹=æ–­é¢äº¤é€šé‡ã€ç·š=OSMã‚¦ã‚§ã‚¤ã«æ²¿ã£ãŸã‚µãƒ–ãƒ‘ã‚¹ï¼ˆèµ¤ï¼š0ã€œ10kmï¼‰ã€‚")

    st.markdown("#### å±ãªã„äº¤å·®ç‚¹")
    st.session_state.show_hotspots = st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— / 3DæŸ± ã‚’è¡¨ç¤º",
                                                 value=bool(st.session_state.show_hotspots))

    st.markdown("â€ƒâ€ƒ")
    st.markdown("#### åœ°å›³ã‚¿ã‚¤ãƒ«")
    map_choice = st.selectbox("åœ°å›³ã‚¹ã‚¿ã‚¤ãƒ«", list(TILESETS.keys()),
                              index=list(TILESETS.keys()).index(st.session_state.map_choice))
    st.session_state.map_choice = map_choice
    custom_tile = st.text_input("ä»»æ„ã‚¿ã‚¤ãƒ«URLï¼ˆé€éPNGæ¨å¥¨, {z}/{x}/{y})", "")

    st.markdown("</div>", unsafe_allow_html=True)

# ----------------------------------------------------------------------------
# Columns
# ----------------------------------------------------------------------------
col_map, col_feed = st.columns([7,5], gap="large")

with col_map:
    is_3d = (mode_3d == "3D")

    def build_base_layers(map_choice:str, custom_tile:str) -> List[pdk.Layer]:
        tile = TILESETS.get(map_choice, TILESETS["æ¨™æº–"])
        layers: List[pdk.Layer] = [pdk.Layer(
            "TileLayer", id=f"base-{map_choice}", data=tile["url"],
            min_zoom=0, max_zoom=tile.get("max_zoom",18), tile_size=256, opacity=1.0, refinement="no-overlap"
        )]
        if custom_tile.strip():
            layers.append(pdk.Layer("TileLayer", id="custom-overlay", data=custom_tile.strip(),
                                    min_zoom=0, max_zoom=22, tile_size=256, opacity=0.6, refinement="no-overlap"))
        return layers

    def build_intersection_layers(hot_df: pd.DataFrame, is_3d: bool) -> List[pdk.Layer]:
        if hot_df.empty or not st.session_state.show_hotspots: return []
        if is_3d:
            return [pdk.Layer("ColumnLayer", id="hot-columns", data=hot_df,
                              get_position="position", get_elevation="elev", elevation_scale=1.0,
                              radius=65, get_fill_color="rgba", pickable=True, extruded=True)]
        return [pdk.Layer("HeatmapLayer", id="hot-heatmap", data=hot_df,
                          get_position="position", get_weight="weight",
                          radius_pixels=60, intensity=0.85, threshold=0.05, opacity=0.35)]

    def build_congestion_grid(df: pd.DataFrame, is_3d: bool) -> List[pdk.Layer]:
        if is_3d or df.empty: return []
        acc = df[df["category"]=="äº¤é€šäº‹æ•…"].copy()
        if acc.empty: return []
        acc["position"] = acc.apply(lambda r: [float(r["lon"]), float(r["lat"])], axis=1)
        acc["weight"] = 1.0
        return [pdk.Layer("GridLayer", id="congestion-grid", data=acc,
                          get_position="position", get_weight="weight",
                          cell_size=200, extruded=False, opacity=0.18, pickable=False,
                          colorRange=[
                              [255,180,180,60],[255,140,140,90],[255,100,100,120],
                              [230,60,60,150],[200,40,40,190],[180,20,20,220],
                          ])]

    def circle_coords(lon: float, lat: float, radius_m: int = 300, n: int = 64) -> List[List[float]]:
        out: List[List[float]] = []
        r_earth = 6378137.0
        dlat = radius_m / r_earth
        dlon = radius_m / (r_earth * math.cos(math.radians(lat)))
        for i in range(n):
            ang = 2 * math.pi * i / n
            lat_i = lat + math.degrees(dlat * math.sin(ang))
            lon_i = lon + math.degrees(dlon * math.cos(math.radians(lat)))
            out.append([lon_i, lat_i])
        out.append(out[0]); return out

    def spiderfy(clon: float, clat: float, n: int, base_px: int = 16, gap_px: int = 8) -> List[Tuple[float,float]]:
        out = []; rpx = base_px
        for k in range(n):
            ang = math.radians(137.5 * k)
            dx = rpx*math.cos(ang); dy = rpx*math.sin(ang)
            dlon = dx / (111320 * math.cos(math.radians(clat))); dlat = dy / 110540
            out.append((clon + dlon, clat + dlat)); rpx += gap_px
        return out

    def build_points_labels_buffers(df: pd.DataFrame) -> Tuple[List[Dict], List[Dict], List[Dict], List[Dict], Dict]:
        centers = cluster_points(df, ZOOM_LIKE)
        points: List[Dict] = []; icon_fg: List[Dict] = []; mini_fg: List[Dict] = []; mini_bg: List[Dict] = []
        features: List[Dict] = []
        for c in centers:
            cnt = c["count"]; clat, clon = c["lat"], c["lon"]
            if cnt <= FANOUT_THRESHOLD:
                offs = spiderfy(clon, clat, cnt)
                for (lon, lat), row in zip(offs, c["rows"]):
                    sty = CAT_STYLE.get(row["category"], CAT_STYLE["ãã®ä»–"])
                    p = {
                        "position":[lon,lat], "color":sty["color"], "radius":sty["radius"],
                        "info_html": make_incident_info_html(
                            row["category"], row.get("summary",""), row.get("municipality",""), row.get("pred","")
                        ),
                        "r":int(row.get("radius_m",600)),
                    }
                    points.append(p)
                    icon_fg.append({"position":[lon,lat], "label":sty["icon"], "tcolor":[255,255,255,235], "offset":[0,-2]})
                    if len(mini_fg) < MAX_LABELS:
                        vtxt = "\n".join(list(row.get("summary","")[:4]))
                        offset_px = int(-14*LABEL_SCALE)
                        mini_bg.append({"position":[lon,lat],"label":vtxt,"tcolor":[0,0,0,220],"offset":[0,offset_px]})
                        mini_fg.append({"position":[lon,lat],"label":vtxt,"tcolor":[255,255,255,235],"offset":[0,offset_px]})
            else:
                points.append({
                    "position":[clon,clat],"color":[100,100,100,210],"radius":70,
                    "info_html": make_incident_info_html("é›†ä¸­","å‘¨è¾ºã«å¤šæ•°","", ""), "r":0
                })
                icon_fg.append({"position":[clon,clat], "label":"â—", "tcolor":[255,255,255,230], "offset":[0,-2]})
        for p in points:
            if p.get("r",0) > 0:
                lon, lat = p["position"]
                features.append({"type":"Feature","geometry":{"type":"Polygon","coordinates":[circle_coords(lon, lat, int(p["r"]))]},"properties":{}})
        geojson = {"type":"FeatureCollection","features": features}
        return points, icon_fg, mini_fg, mini_bg, geojson

    layers: List[pdk.Layer] = []
    layers += build_base_layers(st.session_state.map_choice, custom_tile)
    layers += build_intersection_layers(hot_df, is_3d)
    layers += build_congestion_grid(df, is_3d)

    points, icon_labels, mini_fg, mini_bg, geojson = build_points_labels_buffers(df)
    layers += [
        pdk.Layer("GeoJsonLayer", id="approx-buffers", data=geojson, pickable=False, stroked=True, filled=True,
                  get_line_width=2, get_line_color=[0,160,220], get_fill_color=[0,160,220,40], auto_highlight=False),
        pdk.Layer("ScatterplotLayer", id="incident-points", data=points, get_position="position", get_fill_color="color", get_radius="radius",
                  pickable=True, radius_min_pixels=3, radius_max_pixels=60),
        pdk.Layer("TextLayer", id="icon-labels", data=icon_labels, get_position="position", get_text="label", get_color="tcolor",
                  get_size=14, get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
        pdk.Layer("TextLayer", id="mini-labels-bg", data=mini_bg, get_position="position", get_text="label", get_color="tcolor",
                  get_size=int(12*LABEL_SCALE), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
        pdk.Layer("TextLayer", id="mini-labels-fg", data=mini_fg, get_position="position", get_text="label", get_color="tcolor",
                  get_size=int(12*LABEL_SCALE), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
    ]

    # --- JARTICï¼ˆprefetchæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ï¼‰ ---
    j_points = j_points_prefetch
    if st.session_state.show_jartic_points and j_points:
        pts_vis = [{
            "position": p["position"],
            "color": color_from_total(p["total"]),
            "radius": size_from_total(p["total"]),
            "info_html": make_jartic_info_html(p["jt_time"], p["jt_total"], p["jt_up"], p["jt_down"]),
        } for p in j_points]
        layers.append(
            pdk.Layer(
                "ScatterplotLayer", id="jartic-5min-points", data=pts_vis,
                get_position="position", get_fill_color="color", get_radius="radius",
                radius_min_pixels=4, radius_max_pixels=200, opacity=0.60, pickable=True
            )
        )

    if st.session_state.show_snap_lines and j_points:
        ways = fetch_osm_roads_overpass(EHIME_BBOX)
        lines = build_snap_lines(j_points, ways)
        if lines:
            layers.append(
                pdk.Layer(
                    "PathLayer", id="jartic-5min-snaplines", data=lines,
                    get_path="path", get_color="color", get_width="width",
                    width_scale=1, width_min_pixels=3, width_units="pixels",
                    opacity=0.98, pickable=True
                )
            )

    deck = pdk.Deck(
        layers=layers,
        initial_view_state=pdk.ViewState(latitude=EHIME_PREF_LAT, longitude=EHIME_PREF_LON,
                                         zoom=init_zoom, pitch=(45 if is_3d else 0), bearing=0),
        tooltip={"html": "{info_html}",
                 "style": {"backgroundColor":"rgba(10,15,20,.96)","color":"#e6f1ff",
                           "maxWidth":"380px","whiteSpace":"normal","wordBreak":"break-word",
                           "lineHeight":1.4,"fontSize":"12px","padding":"10px 12px",
                           "borderRadius":"12px","border":"1px solid var(--border)"}},
        map_provider=None, map_style=None
    )
    st.pydeck_chart(deck, use_container_width=True, height=640)

    st.markdown(
        "<div class='hud'><div class='hud-inner'>"
        f"<div class='badge'>Zoom: {init_zoom}ï¼ˆåˆæœŸï¼‰</div>"
        "</div></div>", unsafe_allow_html=True
    )

    # å‡¡ä¾‹
    legend_items = []
    for k, v in CAT_STYLE.items():
        rgba = f"rgba({v['color'][0]}, {v['color'][1]}, {v['color'][2]}, {v['color'][3]/255:.9f})"
        legend_items.append(f"<span class='item'><span class='dot' style='background:{rgba}'></span>{k}</span>")
    jartic_legend = (
        "<div style='margin-top:10px'>"
        "<div style='font-weight:600;margin-bottom:6px'>JARTIC äº¤é€šé‡ï¼ˆç‚¹ï¼‰</div>"
        "<div class='jartic-grad'></div>"
        "<div class='risklbl'><span>ä½</span><span>ä¸­</span><span>é«˜</span><span>éå¸¸ã«é«˜</span></div>"
        "<div style='height:8px'></div>"
        "<div style='font-weight:600;margin-bottom:6px'>JARTIC ç·šï¼ˆæ“¬ä¼¼ï¼‰</div>"
        "<div class='redline-sample'></div>"
        "<div class='risklbl'><span>OSMã‚¦ã‚§ã‚¤ã«æ²¿ã£ãŸä¸­å¿ƒã‚µãƒ–ãƒ‘ã‚¹ï¼ˆèµ¤ï¼‰: 0ã€œ10km</span><span></span></div>"
        "</div>"
    )
    st.markdown(
        f"<div class='legend'>{''.join(legend_items)}"
        f"<div style='margin-top:10px'><div class='riskbar'></div>"
        f"<div class='risklbl'><span>äº¤å·®ç‚¹ãƒªã‚¹ã‚¯ï¼ˆä½ï¼‰</span><span>é«˜ï¼šæœ€å¤§ {int(hot_df['count'].max()) if not hot_df.empty else 0}ä»¶/å¹´</span></div></div>"
        f"{jartic_legend}"
        f"</div>",
        unsafe_allow_html=True,
    )

with col_feed:
    st.markdown("<div class='panel'>", unsafe_allow_html=True)
    st.markdown("#### é€Ÿå ±ãƒ•ã‚£ãƒ¼ãƒ‰")
    q = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¸‚ç”ºåã‚„è¦ç‚¹ï¼‰")
    feed = df.copy()
    if q:
        feed = feed[feed.apply(lambda r: q in (r.get("summary") or "") or q in (r.get("municipality") or ""), axis=1)]
    total = len(feed)
    page_size = st.slider("ãƒšãƒ¼ã‚¸å½“ãŸã‚Šä»¶æ•°", 10, 50, 30, 5)
    pages = max(1, (total + page_size - 1)//page_size)
    page = st.number_input("ãƒšãƒ¼ã‚¸", 1, pages, 1)
    view = feed.iloc[(page-1)*page_size : page*page_size]

    html_list = ["<div class='feed-scroll'>"]
    for _, r in view.iterrows():
        cat = r.get('category','')
        html_list.append("<div class='feed-card'>")
        html_list.append(f"<div><b>{cat}</b></div>")
        html_list.append(f"<div>{r.get('summary','')}</div>")
        muni = r.get('municipality') or ''
        if muni: html_list.append(f"<div style='color:var(--muted);font-size:.9rem'>{muni}</div>")
        html_list.append(f"<div style='color:#00b894;font-size:.9rem'>äºˆæ¸¬: {r.get('pred','')}</div>")
        html_list.append(f"<div style='color:var(--muted);font-size:.9rem'><a href='{r.get('src')}' target='_blank'>å‡ºå…¸</a></div>")
        html_list.append("</div>")
    html_list.append("</div>")
    st.markdown("\n".join(html_list), unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

st.caption("åœ°å›³: OSM/GSI/OpenTopoMap/HOT/GSIèˆªç©ºå†™çœŸ | JARTIC 5åˆ†å€¤ï¼ˆç‚¹=é‡ã‚°ãƒ©ãƒ‡/ç·š=OSMã‚¦ã‚§ã‚¤ã«æ²¿ã† 0ã€œ10kmï¼‰ | äº¤å·®ç‚¹ãƒ’ãƒ¼ãƒˆ/æŸ± ON/OFF | çœŒè­¦é€Ÿå ±ï¼‹äº¤å·®ç‚¹CSVã€‚ç·Šæ€¥æ™‚ã¯110ãƒ»119ã¸ã€‚")
