# ============================================================
# Ehime Safety Platform (ESP) â€“ v4.0.1
# ä¿®æ­£: Python h3 ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® v3/v4 API å·®ç•°ã§è½ã¡ã‚‹ä¸å…·åˆã‚’è§£æ¶ˆ
#  - h3.geo_to_h3 / h3.h3_to_geo ãŒå­˜åœ¨ã—ãªã„ç’°å¢ƒï¼ˆv4ï¼‰ã§ã‚‚å‹•ä½œ
#  - äº’æ›ãƒ˜ãƒ«ãƒ‘: latlng_to_cell / cell_to_latlng ã‚’è‡ªå‹•åˆ‡æ›¿
# ãã®ã»ã‹ã®ä»•æ§˜ã¯ v4.0 ã¨åŒã˜ï¼ˆH3ã‚¯ãƒ©ã‚¹ã‚¿ + ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆç­‰ï¼‰
# ============================================================

import os
import re
import json
import math
import time
import hashlib
import sqlite3
import threading
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

import requests
import pandas as pd
import streamlit as st
import pydeck as pdk
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process as rf_process
import h3

APP_TITLE = "æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆEhime Safety Platform / ESPï¼‰"
EHIME_POLICE_URL = "https://www.police.pref.ehime.jp/sokuho/sokuho.htm"
USER_AGENT = "ESP/4.0.1 (civic); contact: localgov"
REQUEST_TIMEOUT = 12
FETCH_TTL_SEC = 300

GEMINI_MODEL = "gemini-2.5-flash"
GEMINI_MAX_TOKENS = 512
GEMINI_TEMPERATURE = 0.2

EHIME_PREF_LAT = 33.8390
EHIME_PREF_LON = 132.7650

SLEEP_RANGE = (0.6, 1.0)

st.set_page_config(page_title="Ehime Safety Platform", layout="wide")
st.markdown(
    """
    <style>
      .big-title {font-size:1.5rem; font-weight:700;}
      .subtle {color:#5f6368}
      .legend {font-size:0.95rem; background:#fafafa; border:1px solid #eee; border-radius:12px; padding:10px 12px;}
      .legend .item {display:inline-flex; align-items:center; margin-right:14px; margin-bottom:6px}
      .dot {width:12px; height:12px; border-radius:50%; display:inline-block; margin-right:6px; border:1px solid #00000022}
      .feed-card {background:#ffffff; padding:12px 14px; border-radius:14px; border:1px solid #e8e8e8; margin-bottom:10px; box-shadow:0 1px 2px rgba(0,0,0,.04)}
      .feed-scroll {max-height:700px; overflow-y:auto; padding-right:6px}
      .stButton>button {border-radius:999px;}
      /* Quick FABs */
      .fab-bar {position:sticky; bottom:12px; display:flex; gap:8px; justify-content:center; padding:6px 8px;}
      .fab {border-radius:999px; border:1px solid #e3e3e3; padding:6px 10px; background:white; box-shadow:0 2px 8px rgba(0,0,0,.08); font-size:0.9rem;}
      .fab.active {box-shadow:0 0 0 2px rgba(0,0,0,.10) inset;}
      @media (max-width: 640px){ .big-title{font-size:1.2rem} .feed-scroll{max-height:50vh} .legend{font-size:.9rem} }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(f"<div class='big-title'>ğŸ—ºï¸ {APP_TITLE}</div>", unsafe_allow_html=True)

# ------------------ Sidebar ------------------
st.sidebar.header("æœŸé–“")
period = st.sidebar.select_slider("è¡¨ç¤ºæœŸé–“", ["å½“æ—¥","éå»3æ—¥","éå»7æ—¥","éå»30æ—¥"], value="éå»7æ—¥")
period_days = {"å½“æ—¥":1, "éå»3æ—¥":3, "éå»7æ—¥":7, "éå»30æ—¥":30}[period]

st.sidebar.header("å¯†åº¦ãƒ»é‡ãªã‚Šåˆ¶å¾¡")
zoom_like = st.sidebar.slider("è¡¨ç¤ºå¯†åº¦ï¼ˆã‚ºãƒ¼ãƒ ç›¸å½“ï¼‰", 7, 14, 10)
fanout_threshold = st.sidebar.slider("ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆä»¶æ•°é–¾å€¤", 2, 8, 4)
label_scale = st.sidebar.slider("ãƒ©ãƒ™ãƒ«å€ç‡", 0.7, 1.6, 1.0, 0.1)

st.sidebar.header("è§£æãƒ¢ãƒ¼ãƒ‰")
use_llm = st.sidebar.checkbox("Geminiè§£æã‚’æœ‰åŠ¹åŒ–", True)

st.sidebar.header("ãƒ‡ãƒ¼ã‚¿å–å¾—/è¨­å®š")
if st.sidebar.button("çœŒè­¦é€Ÿå ±ã‚’å†å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡è¦–ï¼‰"):
    st.session_state.pop("_fetch_meta", None)
    st.session_state.pop("_incidents_cache", None)
    st.session_state.pop("_analyzed_cache", None)

gazetteer_path = st.sidebar.text_input("ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢CSVãƒ‘ã‚¹", "data/gazetteer_ehime.csv")
use_fuzzy = st.sidebar.checkbox("ã‚†ã‚‰ãå¯¾å¿œï¼ˆãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒï¼‰", True)
min_fuzzy_score = st.sidebar.slider("æœ€å°ã‚¹ã‚³ã‚¢", 60, 95, 78)

# ------------------ Utils ------------------

def jst_now_iso() -> str:
    return datetime.now().astimezone().isoformat(timespec="seconds")

def content_fingerprint(text: str) -> str:
    return hashlib.blake2s(text.encode("utf-8")).hexdigest()

# ------------------ SQLite Cache ------------------
@st.cache_resource
def get_sqlite():
    os.makedirs("data", exist_ok=True)
    conn = sqlite3.connect("data/esp_cache.sqlite", check_same_thread=False)
    with conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS nlp_cache(
                id TEXT PRIMARY KEY,
                payload TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
        """)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS geocode_cache(
                key TEXT PRIMARY KEY,
                lon REAL, lat REAL, type TEXT,
                created_at TEXT NOT NULL
            )
        """)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS fetch_meta(
                url TEXT PRIMARY KEY,
                etag TEXT, last_modified TEXT,
                content_hash TEXT,
                fetched_at TEXT NOT NULL
            )
        """)
    return conn

conn = get_sqlite()
conn_lock = threading.Lock()

# ------------------ Cache helpers ------------------

def cache_get_nlp(hid: str) -> Optional[Dict]:
    with conn_lock:
        cur = conn.execute("SELECT payload FROM nlp_cache WHERE id=?", (hid,))
        r = cur.fetchone()
    return json.loads(r[0]) if r else None

def cache_put_nlp(hid: str, payload: Dict) -> None:
    with conn_lock, conn:
        conn.execute(
            "INSERT OR REPLACE INTO nlp_cache(id,payload,created_at) VALUES(?,?,datetime('now'))",
            (hid, json.dumps(payload, ensure_ascii=False)),
        )

def fetch_meta_get(url: str) -> Tuple[Optional[str], Optional[str]]:
    with conn_lock:
        cur = conn.execute("SELECT etag,last_modified FROM fetch_meta WHERE url=?", (url,))
        r = cur.fetchone()
    if r:
        return r[0], r[1]
    return None, None

def fetch_meta_put(url: str, etag: Optional[str], last_modified: Optional[str], content_hash: str) -> None:
    with conn_lock, conn:
        conn.execute(
            "INSERT OR REPLACE INTO fetch_meta(url,etag,last_modified,content_hash,fetched_at) VALUES(?,?,?,?,datetime('now'))",
            (url, etag, last_modified, content_hash),
        )

# ------------------ Fetch & Parse ------------------
@dataclass
class IncidentItem:
    id: str
    source_url: str
    heading: str
    station: Optional[str]
    incident_date: Optional[str]
    body: str
    fetched_at: str

def http_get(url: str) -> str:
    r = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()
    enc = r.apparent_encoding or r.encoding or "utf-8"
    r.encoding = enc
    return r.text

def http_get_conditional(url: str) -> Optional[str]:
    etag, last_mod = fetch_meta_get(url)
    headers = {"User-Agent": USER_AGENT}
    if etag: headers["If-None-Match"] = etag
    if last_mod: headers["If-Modified-Since"] = last_mod
    r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
    if r.status_code == 304: return None
    r.raise_for_status()
    enc = r.apparent_encoding or r.encoding or "utf-8"; r.encoding = enc
    txt = r.text
    fetch_meta_put(url, r.headers.get("ETag"), r.headers.get("Last-Modified"), content_fingerprint(txt))
    return txt

def parse_ehime_police_page(html: str) -> List[IncidentItem]:
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    text = re.sub(r"ã€æ„›åª›çœŒè­¦ã‹ã‚‰ã®ãŠé¡˜ã„ï¼ã€‘[\s\S]*?(?=â– |$)", "", text)
    lines = [ln for ln in text.split("\n") if ln.strip()]
    blocks, current = [], None
    for ln in lines:
        if ln.startswith("â– "):
            if current: blocks.append(current)
            current = {"heading": ln.strip(), "body": []}
        else:
            if current: current["body"].append(ln.strip())
    if current: blocks.append(current)

    out: List[IncidentItem] = []
    today = datetime.now().date(); cy = today.year
    for b in blocks:
        heading = b.get("heading", ""); body = " ".join(b.get("body", []))[:1600]
        m_date = re.search(r"ï¼ˆ?(\d{1,2})æœˆ(\d{1,2})æ—¥", heading)
        m_station = re.search(r"ï¼ˆ\d{1,2}æœˆ\d{1,2}æ—¥\s*([^\sï¼‰]+)ï¼‰", heading)
        incident_date = None
        if m_date:
            m, d = int(m_date.group(1)), int(m_date.group(2)); y = cy
            cand = datetime(y, m, d).date()
            if cand > today: y -= 1
            incident_date = datetime(y, m, d).date().isoformat()
        station = m_station.group(1) if m_station else None
        rid = content_fingerprint((heading + " " + body)[:800])
        out.append(IncidentItem(rid, EHIME_POLICE_URL, heading, station, incident_date, body, jst_now_iso()))
    return out

# ------------------ Gemini / Rule-based ------------------
@st.cache_resource
def gemini_client():
    import google.generativeai as genai
    key = os.getenv("GEMINI_API_KEY")
    if not key: return None
    genai.configure(api_key=key)
    return genai.GenerativeModel(GEMINI_MODEL)

from concurrent.futures import ThreadPoolExecutor, as_completed

CITY_NAMES = ["æ¾å±±å¸‚","ä»Šæ²»å¸‚","æ–°å±…æµœå¸‚","è¥¿æ¡å¸‚","å¤§æ´²å¸‚","ä¼Šäºˆå¸‚","å››å›½ä¸­å¤®å¸‚","è¥¿äºˆå¸‚","æ±æ¸©å¸‚","ä¸Šå³¶ç”º","ä¹…ä¸‡é«˜åŸç”º","æ¾å‰ç”º","ç ¥éƒ¨ç”º","å†…å­ç”º","ä¼Šæ–¹ç”º","æ¾é‡ç”º","é¬¼åŒ—ç”º","æ„›å—ç”º"]
CATEGORY_PATTERNS = [("äº¤é€šäº‹æ•…", r"äº¤é€š.*äº‹æ•…|è‡ªè»¢è»Š|ãƒã‚¹|äºŒè¼ª|ä¹—ç”¨|è¡çª|äº¤å·®ç‚¹|å›½é“|çœŒé“|äººèº«äº‹æ•…"),("ç«ç½", r"ç«ç½|å‡ºç«|å…¨ç„¼|åŠç„¼|å»¶ç„¼"),("æ­»äº¡äº‹æ¡ˆ", r"æ­»äº¡|æ­»äº¡äº‹æ¡ˆ"),("çªƒç›—", r"çªƒç›—|ä¸‡å¼•|ç›—"),("è©æ¬º", r"è©æ¬º|é‚„ä»˜é‡‘|æŠ•è³‡è©æ¬º|ç‰¹æ®Šè©æ¬º"),("äº‹ä»¶", r"å¨åŠ›æ¥­å‹™å¦¨å®³|æ¡ä¾‹é•å|æš´è¡Œ|å‚·å®³|è„…è¿«|å™¨ç‰©æå£Š|é’å°‘å¹´ä¿è­·")]
FACILITY_HINT = ["å­¦æ ¡","å°å­¦æ ¡","ä¸­å­¦æ ¡","é«˜æ ¡","å¤§å­¦","ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰","ä½“è‚²é¤¨","å…¬åœ’","é§…","æ¸¯","ç—…é™¢","äº¤å·®ç‚¹"]

def rule_based_extract(it: IncidentItem) -> Dict:
    text = it.heading + " " + it.body
    category = "ãã®ä»–"
    for name, pat in CATEGORY_PATTERNS:
        if re.search(pat, text): category = name; break
    muni = None
    for c in CITY_NAMES:
        if c in text: muni = c; break
    places = []
    for hint in FACILITY_HINT:
        m = re.findall(rf"([\w\u3040-\u30ff\u4e00-\u9fffA-Za-z0-9]+{hint})", text)
        places.extend(m[:2])
    s = re.sub(r"\s+", " ", it.body)[:140]
    return {"category":category,"municipality":muni,"place_strings":list(dict.fromkeys(places))[:3],"road_refs":[],"occurred_date":it.incident_date,"occurred_time_text":None,"summary_ja": s if s else it.heading.replace("â– "," ").strip(),"confidence":0.3,"raw_heading":it.heading,"raw_snippet":it.body[:200],"source_url":it.source_url,"fetched_at":it.fetched_at,"id":it.id}

def gemini_analyze_many(items: List[IncidentItem]) -> List[Dict]:
    model = gemini_client()
    if model is None: return []
    SYS = "äº‹å®Ÿã®ã¿ã‚’ application/json ã§å‡ºåŠ›ã€‚æ¬ æ¸¬ã¯ nullã€‚è¦ç´„ã¯åŸæ–‡æº–æ‹ ã§æ†¶æ¸¬ç¦æ­¢ã€‚"
    def one(it: IncidentItem) -> Dict:
        cached = cache_get_nlp(it.id)
        if cached: return cached
        user = (f"category, municipality, place_strings, road_refs, occurred_date, occurred_time_text, summary_ja, confidence, raw_heading, raw_snippet\n" f"è¦‹å‡ºã—:{it.heading}\næœ¬æ–‡:{it.body}\næ¨å®šæ—¥:{it.incident_date} ç½²:{it.station}")
        try:
            import google.generativeai as genai
            gen_cfg = {"temperature":GEMINI_TEMPERATURE, "max_output_tokens":GEMINI_MAX_TOKENS, "response_mime_type":"application/json"}
            resp = model.generate_content([{ "role":"system","parts":[SYS]},{"role":"user","parts":[user]}], generation_config=gen_cfg)
            txt = (getattr(resp, "text", None) or "").strip(); data = json.loads(txt) if txt else {}
        except Exception: data = {}
        base = rule_based_extract(it)
        merged = {"category": data.get("category") or base["category"],"municipality": data.get("municipality") or base["municipality"],"place_strings": data.get("place_strings") or base["place_strings"],"road_refs": data.get("road_refs") or base["road_refs"],"occurred_date": data.get("occurred_date") or base["occurred_date"],"occurred_time_text": data.get("occurred_time_text") or base["occurred_time_text"],"summary_ja": data.get("summary_ja") or base["summary_ja"],"confidence": data.get("confidence", 0.6 if data else 0.3),"raw_heading": it.heading,"raw_snippet": it.body[:200],"source_url": it.source_url,"fetched_at": it.fetched_at,"id": it.id}
        cache_put_nlp(it.id, merged); time.sleep(SLEEP_RANGE[0]); return merged
    out: List[Dict] = []
    with ThreadPoolExecutor(max_workers=6) as ex:
        futs = [ex.submit(one, it) for it in items]
        for f in as_completed(futs): out.append(f.result())
    return out

# ------------------ Gazetteer / Geocoding ------------------
@st.cache_data(show_spinner=False)
def load_gazetteer(csv_path: str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(csv_path)
        for col in ["name","type","lon","lat"]:
            if col not in df.columns: st.warning("ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢ã«å¿…é ˆåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: name,type,lon,lat"); return None
        df["alt_names"] = df.get("alt_names", "").fillna(""); return df
    except Exception: return None

class GazetteerIndex:
    def __init__(self, df: pd.DataFrame):
        self.df = df.reset_index(drop=True)
        self.keys = (df["name"].astype(str) + " | " + df["alt_names"].fillna("").astype(str)).tolist()
    def search(self, q: str, min_score: int = 78) -> Optional[Tuple[float, float, str]]:
        m = self.df[(self.df["name"].str.contains(q, na=False)) | (self.df["alt_names"].str.contains(q, na=False))]
        if not m.empty:
            r = m.iloc[0]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        hit = rf_process.extractOne(q, self.keys, scorer=fuzz.token_set_ratio)
        if hit and hit[1] >= min_score:
            r = self.df.iloc[hit[2]]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        return None

FACILITY_HINT = ["å­¦æ ¡","å°å­¦æ ¡","ä¸­å­¦æ ¡","é«˜æ ¡","å¤§å­¦","ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰","ä½“è‚²é¤¨","å…¬åœ’","é§…","æ¸¯","ç—…é™¢","äº¤å·®ç‚¹"]

def decide_radius_m(match_type: str) -> int:
    if match_type == "facility": return 300
    if match_type == "intersection": return 250
    if match_type in ("town","chome"): return 600
    if match_type in ("oaza","aza"): return 900
    if match_type in ("city","municipality"): return 2000
    return 1200

def nominatim_geocode(name: str, municipality: Optional[str]) -> Optional[Tuple[float, float]]:
    try:
        q = f"{name} {municipality or ''} æ„›åª›çœŒ æ—¥æœ¬".strip(); url = "https://nominatim.openstreetmap.org/search"
        params = {"q": q, "format": "jsonv2", "limit": 1}
        r = requests.get(url, params=params, timeout=10, headers={"User-Agent": USER_AGENT}); r.raise_for_status()
        arr = r.json();
        if isinstance(arr, list) and arr:
            lat = float(arr[0]["lat"]); lon = float(arr[0]["lon"]); return lon, lat
    except Exception: return None
    return None

def geocode_with_cache(idx: Optional[GazetteerIndex], key: str, resolver) -> Optional[Tuple[float, float, str]]:
    with conn_lock:
        cur = conn.execute("SELECT lon,lat,type FROM geocode_cache WHERE key=?", (key,)); r = cur.fetchone()
    if r: return float(r[0]), float(r[1]), str(r[2])
    val = resolver()
    if val:
        lon, lat, typ = val
        with conn_lock, conn:
            conn.execute("INSERT OR REPLACE INTO geocode_cache(key,lon,lat,type,created_at) VALUES(?,?,?,?,datetime('now'))", (key, lon, lat, typ))
        return lon, lat, typ
    return None

# ------------------ Helper: ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ ------------------

def verticalize(text: str, max_lines: int = 4) -> str:
    t = re.sub(r"\s+", "", text or "");
    if not t: return ""
    t = t[:max_lines]; return "\n".join(list(t))

def short_summary(s: str, max_len: int = 60) -> str:
    s = re.sub(r"\s+", " ", s or "").strip();
    return (s[:max_len] + ("â€¦" if len(s) > max_len else "")) if s else ""

# ------------------ H3 äº’æ›ãƒ¬ã‚¤ãƒ¤ ------------------

def h3_cell_from_latlng(lat: float, lon: float, res: int) -> str:
    # v3: geo_to_h3, v4: latlng_to_cell
    if hasattr(h3, "geo_to_h3"):
        return h3.geo_to_h3(lat, lon, res)  # type: ignore
    return h3.latlng_to_cell(lat, lon, res)  # type: ignore


def h3_latlng_from_cell(cell: str) -> Tuple[float, float]:
    # v3: h3_to_geo -> (lat,lon), v4: cell_to_latlng -> (lat,lon)
    if hasattr(h3, "h3_to_geo"):
        lat, lon = h3.h3_to_geo(cell)  # type: ignore
        return lat, lon
    lat, lon = h3.cell_to_latlng(cell)  # type: ignore
    return lat, lon


def h3_res_from_zoom(zoom_like_val:int) -> int:
    mapping = {7:5, 8:6, 9:7, 10:8, 11:9, 12:9, 13:10, 14:10}
    return mapping.get(zoom_like_val, 8)


def cluster_points(df: pd.DataFrame, zoom_like_val:int) -> List[Dict]:
    res = h3_res_from_zoom(zoom_like_val)
    groups: Dict[str, List[Dict]] = {}
    for _, r in df.iterrows():
        lon, lat = float(r["lon"]), float(r["lat"])
        cell = h3_cell_from_latlng(lat, lon, res)
        d = r.to_dict(); d["cell"] = cell
        groups.setdefault(cell, []).append(d)
    centers: List[Dict] = []
    for cell, rows in groups.items():
        lat, lon = h3_latlng_from_cell(cell)
        centers.append({"cell":cell, "lon":lon, "lat":lat, "count":len(rows), "rows":rows})
    return centers


def spiderfy(center_lon: float, center_lat: float, n: int, base_px: int = 16, gap_px: int = 8) -> List[Tuple[float,float]]:
    out: List[Tuple[float,float]] = []; rpx = base_px
    for k in range(n):
        ang = math.radians(137.5 * k); dx = rpx*math.cos(ang); dy = rpx*math.sin(ang)
        dlon = dx / (111320 * math.cos(math.radians(center_lat))); dlat = dy / 110540
        out.append((center_lon + dlon, center_lat + dlat)); rpx += gap_px
    return out

# ------------------ Pipeline ------------------
@st.cache_data(ttl=FETCH_TTL_SEC)
def load_police_items() -> List[IncidentItem]:
    txt = http_get_conditional(EHIME_POLICE_URL)
    if txt is None: txt = http_get(EHIME_POLICE_URL)
    return parse_ehime_police_page(txt)

@st.cache_data(ttl=FETCH_TTL_SEC)
def analyze_items(items: List[IncidentItem], enable_llm: bool) -> pd.DataFrame:
    rows: List[Dict] = []
    if enable_llm:
        new_items = [it for it in items if cache_get_nlp(it.id) is None]
        if new_items: _ = gemini_analyze_many(new_items)
        rows = [cache_get_nlp(it.id) or {} for it in items]
    final_rows: List[Dict] = []
    for it, base in zip(items, rows if rows else [{}]*len(items)):
        rb = rule_based_extract(it)
        if not base: final_rows.append(rb); continue
        merged = {"category": base.get("category") or rb["category"],"municipality": base.get("municipality") or rb["municipality"],"place_strings": base.get("place_strings") or rb["place_strings"],"road_refs": base.get("road_refs") or rb["road_refs"],"occurred_date": base.get("occurred_date") or rb["occurred_date"],"occurred_time_text": base.get("occurred_time_text") or rb["occurred_time_text"],"summary_ja": base.get("summary_ja") or rb["summary_ja"],"confidence": base.get("confidence", 0.6),"raw_heading": it.heading,"raw_snippet": it.body[:200],"source_url": it.source_url,"fetched_at": it.fetched_at,"id": it.id}
        final_rows.append(merged)
    return pd.DataFrame(final_rows)

# ------------------ Run ------------------
with st.spinner("çœŒè­¦é€Ÿå ±ã®å–å¾—ãƒ»è§£æä¸­..."):
    items = load_police_items(); an_df = analyze_items(items, enable_llm=use_llm)

REQUIRED_COLS = {"category":"ãã®ä»–","municipality":None,"place_strings":[],"road_refs":[],"occurred_date":None,"occurred_time_text":None,"summary_ja":None,"confidence":0.4,"raw_heading":None,"raw_snippet":None,"source_url":EHIME_POLICE_URL,"fetched_at":jst_now_iso(),"id":None}
if an_df is None or an_df.empty:
    an_df = pd.DataFrame([{k:v for k,v in REQUIRED_COLS.items()}]).iloc[0:0].copy()
else:
    for col, default in REQUIRED_COLS.items():
        if col not in an_df.columns: an_df[col] = default

# æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿
now = datetime.now(); cutoff = (now - timedelta(days=period_days)).date()
if an_df["occurred_date"].notna().any():
    mask = pd.to_datetime(an_df["occurred_date"], errors="coerce").dt.date >= cutoff
else:
    mask = pd.to_datetime(an_df["fetched_at"], errors="coerce").dt.date >= cutoff
an_df = an_df[mask]

# Basemapï¼ˆæ—¥æœ¬èª GSIï¼‰
TILES = {"GSI æ·¡è‰² (å›½åœŸåœ°ç†é™¢)": {"url": "https://cyberjapandata.gsi.go.jp/xyz/pale/{z}/{x}/{y}.png","attribution": "åœ°ç†é™¢ã‚¿ã‚¤ãƒ«ï¼ˆæ·¡è‰²ï¼‰","max_zoom": 18},"GSI æ¨™æº– (å›½åœŸåœ°ç†é™¢)": {"url": "https://cyberjapandata.gsi.go.jp/xyz/std/{z}/{x}/{y}.png","attribution": "åœ°ç†é™¢ã‚¿ã‚¤ãƒ«ï¼ˆæ¨™æº–ï¼‰","max_zoom": 18}}
BASEMAP = "GSI æ·¡è‰² (å›½åœŸåœ°ç†é™¢)"; TILE = TILES[BASEMAP]

# Gazetteer
@st.cache_data(show_spinner=False)
def load_gaz(csv_path:str):
    try:
        df = pd.read_csv(csv_path); df["alt_names"] = df.get("alt_names", "").fillna(""); return df
    except Exception: return None

gaz_df = load_gaz(gazetteer_path) if gazetteer_path else None
idx = GazetteerIndex(gaz_df) if gaz_df is not None else None

# ã‚«ãƒ†ã‚´ãƒªé…è‰²
CAT_STYLE = {"äº¤é€šäº‹æ•…": {"color": [235, 87, 87, 220],   "radius": 86},"ç«ç½": {"color": [255, 112, 67, 220],  "radius": 88},"æ­»äº¡äº‹æ¡ˆ": {"color": [171, 71, 188, 220],  "radius": 92},"çªƒç›—": {"color": [66, 165, 245, 220],  "radius": 78},"è©æ¬º": {"color": [38, 166, 154, 220],  "radius": 78},"äº‹ä»¶": {"color": [255, 202, 40, 220],  "radius": 82},"ãã®ä»–": {"color": [120, 144, 156, 210],  "radius": 70}}

# ä½ç½®æ±ºå®šï¼ˆå¿…ãšè¡¨ç¤ºï¼‰
rows_geo: List[Dict] = []
for _, row in an_df.iterrows():
    cat = row.get("category") or "ãã®ä»–"; municipality = row.get("municipality"); places: List[str] = row.get("place_strings") or []
    lonlat_typ: Optional[Tuple[float, float, str]] = None
    if idx is not None:
        for ptxt in places:
            key = f"gaz|{municipality}|{ptxt}"
            def _resolve():
                hit = idx.search(ptxt, min_fuzzy_score if use_fuzzy else 101)
                if hit: lon, lat, typ = hit; return lon, lat, typ
                return None
            lonlat_typ = geocode_with_cache(idx, key, _resolve)
            if lonlat_typ: break
        if not lonlat_typ and municipality:
            key = f"gaz|{municipality}"
            def _resolve_city():
                hit = idx.search(municipality, min_fuzzy_score if use_fuzzy else 101)
                if hit: lon, lat, typ = hit; return lon, lat, typ
                return None
            lonlat_typ = geocode_with_cache(idx, key, _resolve_city)
    if not lonlat_typ and municipality:
        for ptxt in places:
            key = f"osm|{municipality}|{ptxt}"
            def _resolve_osm():
                ll = nominatim_geocode(ptxt, municipality)
                if ll: return ll[0], ll[1], "facility"
                return None
            lonlat_typ = geocode_with_cache(idx, key, _resolve_osm)
            if lonlat_typ: break
        if not lonlat_typ:
            key = f"osm|{municipality}"
            def _resolve_osm_city():
                ll = nominatim_geocode(municipality, None)
                if ll: return ll[0], ll[1], "city"
                return None
            lonlat_typ = geocode_with_cache(idx, key, _resolve_osm_city)
    if not lonlat_typ: lonlat_typ = (EHIME_PREF_LON, EHIME_PREF_LAT, "pref")
    lon, lat, match_type = lonlat_typ
    rows_geo.append({
        "lon": float(lon), "lat": float(lat), "category": cat,
        "summary": short_summary(row.get("summary_ja") or row.get("raw_heading") or "", 60),
        "municipality": municipality, "confidence": float(row.get("confidence", 0.4) or 0.4),
        "radius_m": decide_radius_m(match_type), "source_url": row.get("source_url") or EHIME_POLICE_URL,
    })

geo_df = pd.DataFrame(rows_geo)

# --- ã‚¯ã‚¤ãƒƒã‚¯ãƒˆã‚°ãƒ«ï¼ˆFABï¼‰
active_cats = st.session_state.get("active_cats") or {k: True for k in CAT_STYLE.keys()}
bar_html = ["<div class='fab-bar'>"]
for cname in CAT_STYLE.keys():
    on = active_cats.get(cname, True)
    cls = "fab active" if on else "fab"
    color = f"rgba({CAT_STYLE[cname]['color'][0]}, {CAT_STYLE[cname]['color'][1]}, {CAT_STYLE[cname]['color'][2]}, .9)"
    fg = "#222" if cname not in ("äº¤é€šäº‹æ•…","ç«ç½","æ­»äº¡äº‹æ¡ˆ") else "#fff"
    bar_html.append(f"<button class='{cls}' style='background:{color}; color:{fg}' onclick=\"window.parent.postMessage({{'type':'toggle','cat':'{cname}'}},'*')\">{cname}</button>")
bar_html.append("</div>")
st.markdown("".join(bar_html), unsafe_allow_html=True)

st.components.v1.html("""
<script>
window.addEventListener('message', (e)=>{
  if(!e.data || e.data.type!=='toggle') return;
  const cat = e.data.cat; const url = new URL(window.location);
  const key = 'cat_'+encodeURIComponent(cat); const cur = url.searchParams.get(key);
  url.searchParams.set(key, cur==='0' ? '1' : '0'); window.location.replace(url.toString());
});
</script>
""", height=0)

qs = st.query_params
for cname in list(CAT_STYLE.keys()):
    key = "cat_"+cname
    if key in qs: active_cats[cname] = (qs[key] != "0")
st.session_state["active_cats"] = active_cats
geo_df = geo_df[geo_df["category"].apply(lambda c: active_cats.get(str(c), True))]

# ---- H3ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ----
centers = cluster_points(geo_df, zoom_like)

# ---- ãƒ¬ã‚¤ãƒ¤ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰ ----
hex_points = [{"position": [c["lon"], c["lat"]], "count": c["count"]} for c in centers]
points: List[Dict] = []; labels_fg: List[Dict] = []; labels_bg: List[Dict] = []; polys: List[Dict] = []
for c in centers:
    cnt = c["count"]; clat, clon = c["lat"], c["lon"]
    if cnt <= fanout_threshold:
        offs = spiderfy(clon, clat, cnt, base_px=16, gap_px=8)
        for (lon, lat), row in zip(offs, c["rows"]):
            style = CAT_STYLE.get(row["category"], CAT_STYLE["ãã®ä»–"])
            points.append({"position":[lon,lat],"color":style["color"],"radius":style["radius"],"c":row["category"],"s":row["summary"],"m":row.get("municipality"),"f":round(float(row.get("confidence",0.4)),2),"r":int(row.get("radius_m",600))})
            vtxt = verticalize(row["summary"], 4); offset_px = int(-12*label_scale)
            labels_bg.append({"position":[lon,lat],"label":vtxt,"tcolor":[0,0,0,220],"offset":[0,offset_px]})
            labels_fg.append({"position":[lon,lat],"label":vtxt,"tcolor":[255,255,255,235],"offset":[0,offset_px]})
            polys.append({"type":"Feature","geometry":{"type":"Polygon","coordinates":[[[lon,lat]]] },"properties":{"_fill":[255,140,0,0]}})  # dummy; circleã¯ä¸‹ã§ã¾ã¨ã‚ã‚‹
    else:
        points.append({"position":[clon,clat],"color":[90,90,90,200],"radius":70,"c":f"{cnt}ä»¶","s":"å‘¨è¾ºã«å¤šæ•°ã®äº‹æ¡ˆ","m":"","f":0.0,"r":0})
        labels_bg.append({"position":[clon,clat],"label":str(cnt),"tcolor":[0,0,0,220],"offset":[0,-10]})
        labels_fg.append({"position":[clon,clat],"label":str(cnt),"tcolor":[255,255,255,235],"offset":[0,-10]})

# è¿‘ä¼¼å††ï¼ˆåˆ¥é€”ä½œæˆï¼‰
from math import radians, degrees, cos, sin, pi

def circle_coords(lon: float, lat: float, radius_m: int = 300, n: int = 64) -> List[List[float]]:
    coords: List[List[float]] = []
    r_earth = 6378137.0
    dlat = radius_m / r_earth
    dlon = radius_m / (r_earth * math.cos(math.radians(lat)))
    for i in range(n):
        ang = 2 * math.pi * i / n
        lat_i = lat + math.degrees(dlat * math.sin(ang))
        lon_i = lon + math.degrees(dlon * math.cos(math.radians(lat)))
        coords.append([lon_i, lat_i])
    coords.append(coords[0])
    return coords

poly_features = []
for p in points:
    if p.get("r",0) > 0:
        poly_features.append({"type":"Feature","geometry":{"type":"Polygon","coordinates":[circle_coords(p["position"][0], p["position"][1], int(p["r"]))]},"properties":{"_fill":[255,140,0,60],"c":p["c"],"s":p["s"],"m":p.get("m"),"r":p.get("r"),"f":p.get("f")}})
geojson = {"type":"FeatureCollection","features": poly_features}

# ------------------ Render ------------------
col_map, col_feed = st.columns([7,5], gap="large")
with col_map:
    tile_layer = pdk.Layer("TileLayer", data=TILE["url"], min_zoom=0, max_zoom=TILE.get("max_zoom",18), tile_size=256, opacity=1.0)
    hex_layer = pdk.Layer("HexagonLayer", data=hex_points, get_position="position", get_elevation_weight="count", elevation_scale=6, elevation_range=[0,1000], extruded=False, radius=500, coverage=0.9, opacity=0.25, pickable=True)
    circle_layer = pdk.Layer("GeoJsonLayer", data=geojson, pickable=True, stroked=True, filled=True, get_line_width=2, get_line_color=[230, 90, 20], get_fill_color="properties._fill", auto_highlight=True)
    marker_layer = pdk.Layer("ScatterplotLayer", data=points, get_position="position", get_fill_color="color", get_radius="radius", pickable=True, radius_min_pixels=3, radius_max_pixels=60)
    text_bg = pdk.Layer("TextLayer", data=labels_bg, get_position="position", get_text="label", get_color="tcolor", get_size=int(12*label_scale), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle")
    text_fg = pdk.Layer("TextLayer", data=labels_fg, get_position="position", get_text="label", get_color="tcolor", get_size=int(12*label_scale), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle")

    layers = [tile_layer, hex_layer, circle_layer, marker_layer, text_bg, text_fg]
    tooltip = {"html": "<b>{c}</b><br/>{s}<br/><span class='subtle'>{m}</span><br/>åŠå¾„:{r}m / conf:{f}", "style": {"backgroundColor":"#111","color":"white","maxWidth":"280px","whiteSpace":"normal","wordBreak":"break-word","lineHeight":1.35,"fontSize":"13px","padding":"8px 10px","borderRadius":"10px","boxShadow":"0 2px 10px rgba(0,0,0,.25)"}}
    deck = pdk.Deck(layers=layers, initial_view_state=pdk.ViewState(latitude=EHIME_PREF_LAT, longitude=EHIME_PREF_LON, zoom=9), tooltip=tooltip, map_provider=None, map_style=None)
    st.pydeck_chart(deck, use_container_width=True, height=520)

    # å‡¡ä¾‹
    legend_items = []
    for k, v in CAT_STYLE.items():
        col = f"rgba({v['color'][0]}, {v['color'][1]}, {v['color'][2]}, {v['color'][3]/255:.9f})"
        legend_items.append(f"<span class='item'><span class='dot' style='background:{col}'></span>{k}</span>")
    st.markdown(f"<div class='legend'>{''.join(legend_items)}<div class='subtle' style='margin-top:6px'>å…­è§’ã¯å¯†åº¦ã€æ”¾å°„ç‚¹ã¯è¿‘æ¥å±•é–‹ã€‚å††ã¯è¿‘ä¼¼ç¯„å›²ã€‚</div></div>", unsafe_allow_html=True)

with col_feed:
    st.subheader("é€Ÿå ±ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆæœŸé–“ãƒ»ã‚«ãƒ†ã‚´ãƒªé€£å‹•ï¼‰")
    cats_on = [c for c, on in st.session_state.get("active_cats", {}).items() if on]
    feed = an_df[an_df["category"].isin(cats_on)] if cats_on else an_df.iloc[0:0]
    q = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆè¦ç´„/åŸæ–‡/è¦‹å‡ºã—ï¼‰")
    if q:
        feed = feed[feed.apply(lambda r: (q in (r.get("summary_ja") or "")) or (q in (r.get("raw_snippet") or "")) or (q in (r.get("raw_heading") or "")), axis=1)]
    html = ["<div class='feed-scroll'>"]
    for _, r in feed.iterrows():
        html.append("<div class='feed-card'>")
        html.append(f"<b>{r.get('category','ãã®ä»–')}</b><br>")
        html.append(f"<div class='subtle'>{short_summary(r.get('summary_ja') or (r.get('raw_heading') or 'è¦ç´„ãªã—'), 110)}</div>")
        html.append(f"<div class='subtle'>{r.get('municipality') or 'å¸‚ç”ºæ‘ä¸æ˜'} / å–å¾—: {r.get('fetched_at')} / conf: {r.get('confidence')}</div>")
        src = r.get("source_url") or EHIME_POLICE_URL
        html.append(f"<a href='{src}' target='_blank'>å‡ºå…¸ã‚’é–‹ã</a>")
        html.append("</div>")
    html.append("</div>"); st.markdown("\n".join(html), unsafe_allow_html=True)

st.markdown("---")
st.caption("å‡ºå…¸: æ„›åª›çœŒè­¦ äº‹ä»¶äº‹æ•…é€Ÿå ± / åœ°å›³: å›½åœŸåœ°ç†é™¢ã‚¿ã‚¤ãƒ«ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ã¯H3ã€å°‘æ•°ã¯ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆã§é‡ãªã‚Šè§£æ¶ˆã€‚è¦ç´„ã¯åŸæ–‡æŠœç²‹ã§æ†¶æ¸¬ãªã—ã€‚ç·Šæ€¥æ™‚ã¯110/119ã¸ã€‚")

# requirements å‚è€ƒ
# streamlit
# pandas
# pydeck
# requests
# beautifulsoup4
# rapidfuzz
# h3
# google-generativeai>=0.8.0  # ä»»æ„
