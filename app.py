# -*- coding: utf-8 -*-
# æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  / Ehime Safety Platform  v8.0
# - è¿½åŠ : Gemini 2.5 Flash ã«ã‚ˆã‚‹äº‹æ¡ˆãƒ†ã‚­ã‚¹ãƒˆâ†’åœ°åå€™è£œæŠ½å‡ºï¼ˆç„¡æ–™ã‚¿ã‚¤ãƒ«/ã‚­ãƒ¼ç„¡ã—ã§ã‚‚å‹•ä½œã€ã‚­ãƒ¼ãŒã‚ã‚Œã°ç²¾åº¦å‘ä¸Šï¼‰
# - å¼·åŒ–: ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢ï¼ˆå¤–éƒ¨CSV + å†…è”µå¸‚ç”ºä¸­å¿ƒï¼‰ã‚’ç”¨ã„ãŸã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å …ç‰¢åŒ–
# - è¸è¥²: ç„¡æ–™åœ°å›³é¸æŠï¼ˆGSI/OSM/HOT/OpenTopoï¼‰ãƒ»2D/3D åˆ‡æ›¿ãƒ»å°†æ¥ãƒãƒƒãƒ•ã‚¡æ‹¡å¤§ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿/å‡¡ä¾‹/ãƒ•ã‚£ãƒ¼ãƒ‰

import os, re, math, time, json, sqlite3, threading, unicodedata, hashlib
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor

import httpx
import requests
import pandas as pd
import streamlit as st
import pydeck as pdk
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process as rf_process
import h3

# === Gemini (ä»»æ„) ===
try:
    import google.generativeai as genai  # requirements: google-generativeai
    _HAS_GEMINI = True
except Exception:
    _HAS_GEMINI = False

APP_TITLE = "æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ "
EHIME_POLICE_URL = "https://www.police.pref.ehime.jp/sokuho/sokuho.htm"
USER_AGENT = "ESP/8.0 (gemini+gazetteer)"
TIMEOUT = 12
TTL_HTML = 600
MAX_WORKERS = 6

EHIME_PREF_LAT = 33.8390
EHIME_PREF_LON = 132.7650

# å°†æ¥å½±éŸ¿ã‚’è€ƒæ…®ã—ãŸæ¦‚ä½ç½®ãƒãƒƒãƒ•ã‚¡å€ç‡
FUTURE_BUFFER_SCALE = 1.8

# å†…éƒ¨æ—¢å®š
ZOOM_LIKE = 10
FANOUT_THRESHOLD = 4
LABEL_SCALE = 1.0
MAX_LABELS = 400

CITY_NAMES = [
    "æ¾å±±å¸‚","ä»Šæ²»å¸‚","æ–°å±…æµœå¸‚","è¥¿æ¡å¸‚","å¤§æ´²å¸‚","ä¼Šäºˆå¸‚","å››å›½ä¸­å¤®å¸‚",
    "è¥¿äºˆå¸‚","æ±æ¸©å¸‚","ä¸Šå³¶ç”º","ä¹…ä¸‡é«˜åŸç”º","æ¾å‰ç”º","ç ¥éƒ¨ç”º","å†…å­ç”º",
    "ä¼Šæ–¹ç”º","æ¾é‡ç”º","é¬¼åŒ—ç”º","æ„›å—ç”º","å®‡å’Œå³¶å¸‚","å…«å¹¡æµœå¸‚"
]

# å†…è”µå¸‚ç”ºä¸­å¿ƒï¼ˆç¢ºå®Ÿã«å¸‚ç”ºã¸è½ã¨ã™ï¼‰
MUNI_CENTERS = {
    "æ¾å±±å¸‚":(132.7650,33.8390),"ä»Šæ²»å¸‚":(133.0000,34.0660),"æ–°å±…æµœå¸‚":(133.2830,33.9600),
    "è¥¿æ¡å¸‚":(133.1830,33.9180),"å¤§æ´²å¸‚":(132.5500,33.5000),"ä¼Šäºˆå¸‚":(132.7010,33.7550),
    "å››å›½ä¸­å¤®å¸‚":(133.5500,33.9800),"è¥¿äºˆå¸‚":(132.5000,33.3660),"æ±æ¸©å¸‚":(132.8710,33.7930),
    "ä¸Šå³¶ç”º":(133.2000,34.2600),"ä¹…ä¸‡é«˜åŸç”º":(132.9040,33.5380),"æ¾å‰ç”º":(132.7110,33.7870),
    "ç ¥éƒ¨ç”º":(132.7870,33.7350),"å†…å­ç”º":(132.6580,33.5360),"ä¼Šæ–¹ç”º":(132.3560,33.4880),
    "æ¾é‡ç”º":(132.7570,33.2260),"é¬¼åŒ—ç”º":(132.8800,33.2280),"æ„›å—ç”º":(132.5660,33.0000),
    "å®‡å’Œå³¶å¸‚":(132.5600,33.2230),"å…«å¹¡æµœå¸‚":(132.4230,33.4620),
}

CATEGORY_PATTERNS = [
    ("äº¤é€šäº‹æ•…", r"äº¤é€š.*äº‹æ•…|è‡ªè»¢è»Š|ãƒã‚¹|äºŒè¼ª|ä¹—ç”¨|è¡çª|äº¤å·®ç‚¹|å›½é“|çœŒé“|äººèº«äº‹æ•…"),
    ("ç«ç½",     r"ç«ç½|å‡ºç«|å…¨ç„¼|åŠç„¼|å»¶ç„¼"),
    ("æ­»äº¡äº‹æ¡ˆ", r"æ­»äº¡|æ­»äº¡äº‹æ¡ˆ"),
    ("çªƒç›—",     r"çªƒç›—|ä¸‡å¼•|ç›—"),
    ("è©æ¬º",     r"è©æ¬º|é‚„ä»˜é‡‘|æŠ•è³‡è©æ¬º|ç‰¹æ®Šè©æ¬º"),
    ("äº‹ä»¶",     r"å¨åŠ›æ¥­å‹™å¦¨å®³|æ¡ä¾‹é•å|æš´è¡Œ|å‚·å®³|è„…è¿«|å™¨ç‰©æå£Š|é’å°‘å¹´ä¿è­·"),
]

# ===== UIãƒ†ãƒ¼ãƒ =====
st.set_page_config(page_title="Ehime Safety Platform", layout="wide")
st.markdown(
    """
    <style>
      :root{ --bg:#0b0f14; --panel:#0f141b; --panel2:#121924; --text:#e8f1ff; --muted:#8aa0b6; --border:#2b3a4d; --a:#007aff; --b:#00b894; }
      @media (prefers-color-scheme: light){ :root{ --bg:#f7fafc; --panel:#ffffff; --panel2:#f1f5f9; --text:#0f2230; --muted:#586b7a; --border:#dfe7ef; --a:#005acb; --b:#009a7a; } }
      html, body, .stApp { background: var(--bg); color: var(--text); }
      .topbar{ position: sticky; top:0; z-index:10; padding:14px 16px; margin:-16px -16px 14px -16px; border-bottom:1px solid var(--border); background:var(--panel); }
      .brand{ display:flex; align-items:center; gap:10px; font-weight:800; font-size:1.05rem; }
      .brand .id{ width:28px; height:28px; border-radius:8px; display:grid; place-items:center; background: linear-gradient(135deg,var(--a),var(--b)); color:#00131a; font-weight:900; }
      .subnote{ color: var(--muted); font-size:.85rem; margin-top:4px}
      .panel { background: var(--panel); border:1px solid var(--border); border-radius: 14px; padding: 10px 12px; }
      .legend { font-size:.95rem; background:var(--panel); border:1px solid var(--border); border-radius:12px; padding:10px 12px;}
      .legend .item { display:inline-flex; align-items:center; margin-right:14px; margin-bottom:6px}
      .dot { width:12px; height:12px; border-radius:50%; display:inline-block; margin-right:6px; border:1px solid #0003}
      .feed-card {background:var(--panel); padding:12px 14px; border-radius:14px; border:1px solid var(--border); margin-bottom:10px;}
      .feed-scroll {max-height:62vh; overflow-y:auto; padding-right:6px}
      @media (max-width: 640px){ .feed-scroll{max-height:48vh} }
      a { color: var(--a); }
    </style>
    """,
    unsafe_allow_html=True,
)
st.markdown(
    f"""
    <div class="topbar">
      <div class="brand">
        <div class="id">ES</div>
        <div>
          <div>{APP_TITLE}</div>
          <div class="subnote">ä»Šã‚’çŸ¥ã‚Šãƒ»å±é™ºã®å…ˆã‚’èª­ã‚€å±é™ºç¢ºèªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ </div>
        </div>
      </div>
    </div>
    """, unsafe_allow_html=True
)

# ===== Sidebar =====
with st.sidebar:
    st.markdown("<div class='panel'>", unsafe_allow_html=True)
    st.markdown("#### è¡¨ç¤ºæœŸé–“")
    period = st.select_slider("è¡¨ç¤ºæœŸé–“ã‚’é¸æŠ", ["å½“æ—¥","éå»3æ—¥","éå»7æ—¥","éå»30æ—¥"], value="éå»7æ—¥", label_visibility="collapsed")
    period_days = {"å½“æ—¥":1, "éå»3æ—¥":3, "éå»7æ—¥":7, "éå»30æ—¥":30}[period]

    st.markdown("#### åœ°å›³ã‚¿ã‚¤ãƒ«")
    map_choice = st.selectbox(
        "åœ°å›³ã‚¹ã‚¿ã‚¤ãƒ«",
        ["GSI æ·¡è‰²","GSI æ¨™æº–","OpenStreetMap Standard","OSM Humanitarian","OpenTopoMap"],
        index=0
    )

    st.markdown("#### è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰")
    mode_3d = st.radio("2D / 3D", ["2D","3D"], horizontal=True, index=0)
    st.markdown("</div>", unsafe_allow_html=True)

# ===== SQLite cache =====
@st.cache_resource
def get_sqlite():
    os.makedirs("data", exist_ok=True)
    conn = sqlite3.connect("data/esp_cache.sqlite", check_same_thread=False)
    with conn:
        conn.execute("CREATE TABLE IF NOT EXISTS geocode_cache(key TEXT PRIMARY KEY, lon REAL, lat REAL, type TEXT, created_at TEXT)")
        conn.execute("CREATE TABLE IF NOT EXISTS llm_cache(key TEXT PRIMARY KEY, json TEXT, created_at TEXT)")
    return conn
conn = get_sqlite(); conn_lock = threading.Lock()

def cache_get(table:str, key:str) -> Optional[str]:
    with conn_lock:
        row = conn.execute(f"SELECT json FROM {table} WHERE key=?", (key,)).fetchone()
    return row[0] if row else None

def cache_put(table:str, key:str, payload:str):
    with conn_lock, conn:
        conn.execute(f"INSERT OR REPLACE INTO {table} VALUES (?,?,datetime('now'))", (key, payload))

# ===== Helpers =====
def _norm(s: str) -> str:
    s = unicodedata.normalize('NFKC', s or '').strip()
    s = re.sub(r"\s+", " ", s)
    return s

@dataclass
class IncidentItem:
    heading: str
    body: str
    incident_date: Optional[str]

# ===== Fetch & Parse =====
@st.cache_data(ttl=TTL_HTML)
def fetch_ehime_html() -> str:
    headers = {"User-Agent": USER_AGENT}
    for attempt in range(3):
        try:
            with httpx.Client(headers=headers, timeout=TIMEOUT) as client:
                r = client.get(EHIME_POLICE_URL)
                if r.status_code in (429, 503):
                    raise httpx.HTTPStatusError("rate limited", request=None, response=r)
                r.raise_for_status()
                enc = r.encoding or 'utf-8'
                try:
                    r.encoding = r.apparent_encoding or enc
                except Exception:
                    r.encoding = enc
                return r.text
        except Exception:
            time.sleep(0.6 * (attempt + 1))
    try:
        r = requests.get(EHIME_POLICE_URL, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        r.encoding = r.apparent_encoding or r.encoding or "utf-8"
        return r.text
    except Exception:
        st.warning("é€Ÿå ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç©ºãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¤ºã—ã¾ã™ã€‚")
        return "<html><body></body></html>"

def parse_items(html: str) -> List[IncidentItem]:
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    text = re.sub(r"ã€æ„›åª›çœŒè­¦ã‹ã‚‰ã®ãŠé¡˜ã„ï¼ã€‘[\s\S]*?(?=â– |$)", "", text)
    lines = [ln for ln in text.split("\n") if ln.strip()]
    items, cur = [], None
    for ln in lines:
        if ln.startswith("â– "):
            if cur: items.append(cur)
            cur = {"heading": ln.strip(), "body": []}
        else:
            if cur: cur["body"].append(ln.strip())
    if cur: items.append(cur)

    out: List[IncidentItem] = []
    today = datetime.now().date(); cy = today.year
    for b in items:
        h = b.get("heading", ""); body = " ".join(b.get("body", []))
        m = re.search(r"ï¼ˆ?(\d{1,2})æœˆ(\d{1,2})æ—¥", h)
        d = None
        if m:
            mm, dd = int(m.group(1)), int(m.group(2)); y = cy
            try:
                dt = datetime(y, mm, dd).date();  y -= 1 if dt > today else 0
                d = datetime(y, mm, dd).date().isoformat()
            except: d = None
        out.append(IncidentItem(h.replace("â– ", "").strip(), body, d))
    return out

# ===== è»½é‡ãƒ«ãƒ¼ãƒ«æŠ½å‡º =====
def rule_extract(it: IncidentItem) -> Dict:
    t = it.heading + " " + it.body
    cat = "ãã®ä»–"
    for name, pat in CATEGORY_PATTERNS:
        if re.search(pat, t): cat = name; break
    muni = None
    for c in CITY_NAMES:
        if c in t: muni = c; break
    places = []
    for hint in ["å°å­¦æ ¡","ä¸­å­¦æ ¡","é«˜æ ¡","å¤§å­¦","å­¦æ ¡","ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰","ä½“è‚²é¤¨","å…¬åœ’","é§…","æ¸¯","ç—…é™¢","äº¤å·®ç‚¹"]:
        places += re.findall(rf"([\w\u3040-\u30ff\u4e00-\u9fffA-Za-z0-9]+{hint})", t)[:2]
    s = re.sub(r"\s+", " ", it.body).strip()
    s = s[:120] + ("â€¦" if len(s)>120 else "")
    return {"category":cat,"municipality":muni,"place_strings":list(dict.fromkeys(places))[:3],
            "summary": s or it.heading, "date": it.incident_date}

# ===== Gazetteerï¼ˆå¤–éƒ¨CSVã¯ä»»æ„ï¼‰ =====
@st.cache_resource
def load_gazetteer(path:str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        for c in ("name","type","lon","lat"):
            if c not in df.columns: return None
        df["alt_names"] = df.get("alt_names", "").fillna("")
        return df
    except Exception:
        return None

class GazetteerIndex:
    def __init__(self, df: pd.DataFrame):
        self.df = df.reset_index(drop=True)
        self.keys = (df["name"].astype(str) + " | " + df["alt_names"].astype(str)).tolist()
    def search(self, q:str, min_score:int=78) -> Optional[Tuple[float,float,str]]:
        m = self.df[(self.df["name"].str.contains(q, na=False)) | (self.df["alt_names"].str.contains(q, na=False))]
        if not m.empty:
            r = m.iloc[0]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        hit = rf_process.extractOne(q, self.keys, scorer=fuzz.token_set_ratio)
        if hit and hit[1] >= min_score:
            r = self.df.iloc[hit[2]]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        return None

# ===== Nominatim =====
def nominatim_geocode(name:str, municipality:Optional[str]) -> Optional[Tuple[float,float]]:
    try:
        q = f"{name} {municipality or ''} æ„›åª›çœŒ æ—¥æœ¬".strip()
        headers = {"User-Agent": USER_AGENT}
        params = {"q": q, "format": "jsonv2", "limit": 1}
        for i in range(3):
            r = requests.get("https://nominatim.openstreetmap.org/search", params=params, headers=headers, timeout=TIMEOUT)
            if r.status_code in (429,503):
                time.sleep(0.6 * (i+1)); continue
            r.raise_for_status()
            arr = r.json()
            if isinstance(arr, list) and arr:
                return float(arr[0]["lon"]), float(arr[0]["lat"])
        return None
    except Exception:
        return None

# ===== Gemini è§£æï¼ˆä»»æ„ã€APIã‚­ãƒ¼ãŒã‚ã‚Œã°ä½¿ç”¨ï¼‰ =====
def gemini_candidates(full_text: str, muni_hint: Optional[str]) -> List[Dict]:
    api_key = os.getenv("GEMINI_API_KEY", "")
    if not (_HAS_GEMINI and api_key):
        return []
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    key_src = f"gem8|{muni_hint or ''}|{full_text}"
    key = hashlib.sha1(key_src.encode("utf-8")).hexdigest()
    cached = cache_get("llm_cache", key)
    if cached:
        try:
            obj = json.loads(cached)
            return obj.get("candidates", []) if isinstance(obj, dict) else []
        except Exception:
            pass

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        system = (
            "ã‚ãªãŸã¯æ—¥æœ¬ã®ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢è£œåŠ©ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚"
            "å…¥åŠ›ã®äº‹ä»¶ãƒ»äº‹æ•…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€åœ°åã‚„æ–½è¨­åå€™è£œã‚’é«˜ç²¾åº¦ã§æŠ½å‡ºã—ã¾ã™ã€‚"
            "å‡ºåŠ›ã¯æ¬¡ã®JSONã®ã¿: {\"candidates\":[{\"name\":str,\"kind\":str,\"confidence\":0..1}]}"
            "kindã¯ facility/street/intersection/town/city/unknown ã®ã„ãšã‚Œã‹ã€‚"
            "æ–‡ç« å†…ã®å›ºæœ‰ã®åœ°ç‰©ã«é™å®šã—ã€ã‚ã„ã¾ã„ãªã‚‰å¸‚ç”ºã§ã‚ˆã„ã€‚æœ€å¤§5ä»¶ã¾ã§ã€‚"
        )
        muni_line = f"å¸‚ç”ºãƒ’ãƒ³ãƒˆ: {muni_hint}\n" if muni_hint else ""
        prompt = (
            f"{system}\n\nãƒ†ã‚­ã‚¹ãƒˆ:\n{full_text}\n\n{muni_line}"
            "JSONã®ã¿ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚è¿½åŠ èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"
        )
        resp = model.generate_content(prompt)
        txt = resp.text if hasattr(resp, "text") else str(resp)
        # JSONæŠ½å‡º
        start = txt.find("{"); end = txt.rfind("}")
        if start >=0 and end>start:
            txt = txt[start:end+1]
        obj = json.loads(txt)
        if isinstance(obj, dict):
            cache_put("llm_cache", key, json.dumps(obj, ensure_ascii=False))
            return obj.get("candidates", [])
    except Exception:
        return []
    return []

# ===== H3/Cluster =====
def h3_cell_from_latlng(lat: float, lon: float, res: int) -> str:
    if hasattr(h3, "geo_to_h3"): return h3.geo_to_h3(lat, lon, res)  # v3
    return h3.latlng_to_cell(lat, lon, res)  # v4

def h3_latlng_from_cell(cell: str) -> Tuple[float,float]:
    if hasattr(h3, "h3_to_geo"): lat, lon = h3.h3_to_geo(cell); return lat, lon  # v3
    lat, lon = h3.cell_to_latlng(cell); return lat, lon  # v4

def h3_res_from_zoom(zoom_val:int) -> int:
    return {7:5,8:6,9:7,10:8,11:9,12:9,13:10,14:10}.get(zoom_val, 8)

def cluster_points(df: pd.DataFrame, zoom_val:int) -> List[Dict]:
    res = h3_res_from_zoom(zoom_val)
    groups: Dict[str, List[Dict]] = {}
    for _, r in df.iterrows():
        lon, lat = float(r["lon"]), float(r["lat"])
        cell = h3_cell_from_latlng(lat, lon, res)
        d = r.to_dict(); d["cell"] = cell
        groups.setdefault(cell, []).append(d)
    centers: List[Dict] = []
    for cell, rows in groups.items():
        lat, lon = h3_latlng_from_cell(cell)
        centers.append({"cell":cell, "lon":lon, "lat":lat, "count":len(rows), "rows":rows})
    return centers

# ===== è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ =====
def short_summary(s: str, max_len: int = 64) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    return (s[:max_len] + ("â€¦" if len(s) > max_len else "")) if s else ""

def make_prediction(category:str, muni:Optional[str]) -> str:
    if category == "è©æ¬º":       return "SNSã‚„æŠ•è³‡ã®èª˜ã„ã«æ³¨æ„ã€‚é€é‡‘å‰ã«å®¶æ—ã‚„è­¦å¯Ÿã¸ç›¸è«‡ã€‚"
    if category == "äº¤é€šäº‹æ•…":   return "å¤•æ–¹ã‚„é›¨å¤©ã®äº¤å·®ç‚¹ã§å¢—ãˆã‚„ã™ã„ã€‚æ¨ªæ–­ã¨å³å·¦æŠ˜ã«æ³¨æ„ã€‚"
    if category == "çªƒç›—":       return "è‡ªè»¢è»Šãƒ»è»Šä¸¡ã®æ–½éŒ ã¨é˜²çŠ¯ç™»éŒ²ã€‚å¤œé–“ã®ç„¡æ–½éŒ æ”¾ç½®ã‚’é¿ã‘ã‚‹ã€‚"
    if category == "ç«ç½":       return "ä¹¾ç‡¥æ™‚ã¯å±‹å¤–ç«æ°—ã«é…æ…®ã€‚é›»æºå‘¨ã‚Šãƒ»å–«ç…™ã®å§‹æœ«ã‚’å†ç¢ºèªã€‚"
    if category == "äº‹ä»¶":       return "ä¸å¯©é€£çµ¡ã¯è¨˜éŒ²ã‚’æ®‹ã—é€šå ±ã€‚å­¦æ ¡ãƒ»å…¬å…±æ–½è¨­å‘¨è¾ºã§æ„è­˜ã‚’ã€‚"
    if category == "æ­»äº¡äº‹æ¡ˆ":   return "è©³ç´°ã¯å‡ºå…¸ã§ç¢ºèªã€‚å‘¨è¾ºã§ã¯æ•‘æ€¥æ´»å‹•ã«é…æ…®ã€‚"
    return "åŒç¨®äº‹æ¡ˆãŒç¶šãå¯èƒ½æ€§ã€‚å‡ºå…¸ã§æœ€æ–°ã‚’ç¢ºèªã€‚"

# ===== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ =====
with st.spinner("é€Ÿå ±ã‚’å–å¾—ä¸­â€¦"):
    html = fetch_ehime_html()
raw_items = parse_items(html)

extracted: List[Dict] = []
for it in raw_items:
    ex = rule_extract(it)
    ex["heading"] = it.heading
    ex["full_text"] = (it.heading + " " + it.body).strip()
    extracted.append(ex)

# å¤–éƒ¨ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢ï¼ˆä»»æ„ï¼‰
gdf = load_gazetteer("data/gazetteer_ehime.csv")
idx = GazetteerIndex(gdf) if gdf is not None else None

# ---- åº§æ¨™æ±ºå®šé †åºï¼ˆGemini + ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢ + å†…è”µ + Nominatim + çœŒåºï¼‰ ----
# 1) GeminiãŒææ¡ˆã™ã‚‹å€™è£œï¼ˆæ–½è¨­/äº¤å·®ç‚¹/ç”ºåï¼‰
# 2) å¤–éƒ¨ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢ï¼ˆå€™è£œåâ†’ä¸€è‡´/ãƒ•ã‚¡ã‚¸ãƒ¼ï¼‰
# 3) å†…è”µå¸‚ç”ºä¸­å¿ƒ
# 4) Nominatimï¼ˆæ–½è¨­â†’å¸‚ç”ºï¼‰
# 5) çœŒåº

def try_gazetteer(name:str, min_score:int=78) -> Optional[Tuple[float,float,str]]:
    if not idx: return None
    hit = idx.search(name, min_score)
    return hit

def resolve_one(ex: Dict) -> Dict:
    muni = ex.get("municipality"); places = ex.get("place_strings") or []
    full_text = ex.get("full_text", "")

    # 1) Gemini å€™è£œæŠ½å‡º
    llm_cands = gemini_candidates(full_text, muni)
    cand_names = [c.get("name","") for c in llm_cands if isinstance(c, dict)]

    # å„ªå…ˆé †: Geminiå€™è£œ â†’ ãƒ«ãƒ¼ãƒ«æŠ½å‡ºã®place_strings â†’ å¸‚ç”º
    queries = [q for q in cand_names if q] + places + ([muni] if muni else [])

    # 2) å¤–éƒ¨ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢
    for q in queries:
        hit = try_gazetteer(q, 78)
        if hit:
            lon, lat, typ = hit
            return {"lon":lon, "lat":lat, "type":typ or "facility"}

    # 3) å†…è”µå¸‚ç”ºä¸­å¿ƒ
    if muni and muni in MUNI_CENTERS:
        lon, lat = MUNI_CENTERS[muni]
        return {"lon":lon, "lat":lat, "type":"city"}

    # 4) Nominatim
    for q in queries:
        ll = nominatim_geocode(q, muni)
        if ll:
            return {"lon":ll[0], "lat":ll[1], "type":"facility"}

    # 5) çœŒåº
    return {"lon":EHIME_PREF_LON, "lat":EHIME_PREF_LAT, "type":"pref"}

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exctr:
    results = list(exctr.map(resolve_one, extracted))

rows: List[Dict] = []
for ex, loc in zip(extracted, results):
    typ = loc.get("type","city")
    base_radius = {"facility":300,"intersection":250,"town":600,"chome":600,"oaza":900,"aza":900,"city":2000,"pref":2500}.get(typ, 1200)
    radius = int(base_radius * FUTURE_BUFFER_SCALE)
    rows.append({
        "lon": float(loc["lon"]), "lat": float(loc["lat"]),
        "category": ex["category"],
        "summary": short_summary(ex["summary"], 60),
        "municipality": ex.get("municipality") or "",
        "radius_m": radius,
        "pred": make_prediction(ex["category"], ex.get("municipality")),
        "src": EHIME_POLICE_URL,
    })

df = pd.DataFrame(rows)

# ===== ã‚«ãƒ†ã‚´ãƒªè‰²ï¼ˆãƒ©ã‚¤ãƒˆ/ãƒ€ãƒ¼ã‚¯ä¸¡å¯¾å¿œï¼‰ =====
CAT_STYLE = {
    "äº¤é€šäº‹æ•…": {"color":[220, 60, 60, 235],   "radius":86, "icon":"â–²"},
    "ç«ç½":     {"color":[245, 130, 50, 235],  "radius":88, "icon":"ğŸ”¥"},
    "æ­»äº¡äº‹æ¡ˆ": {"color":[170, 120, 240, 235], "radius":92, "icon":"âœ–"},
    "çªƒç›—":     {"color":[70, 150, 245, 235],  "radius":78, "icon":"ğŸ”“"},
    "è©æ¬º":     {"color":[40, 180, 160, 235],  "radius":78, "icon":"âš "},
    "äº‹ä»¶":     {"color":[245, 200, 60, 235],  "radius":82, "icon":"ï¼"},
    "ãã®ä»–":   {"color":[128, 144, 160, 220], "radius":70, "icon":"ãƒ»"},
}

# ===== å¯è¦–ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ =====
vis_df = df
centers = cluster_points(vis_df, ZOOM_LIKE) if not vis_df.empty else []

# ===== ãƒ¬ã‚¤ãƒ¤ãƒ‡ãƒ¼ã‚¿ä½œæˆ =====
def spiderfy(clon: float, clat: float, n: int, base_px: int = 16, gap_px: int = 8):
    out = []; rpx = base_px
    for k in range(n):
        ang = math.radians(137.5 * k)
        dx = rpx*math.cos(ang); dy = rpx*math.sin(ang)
        dlon = dx / (111320 * math.cos(math.radians(clat))); dlat = dy / 110540
        out.append((clon + dlon, clat + dlat)); rpx += gap_px
    return out

hex_points = [{"position":[c["lon"],c["lat"]],"count":c["count"]} for c in centers]
points: List[Dict] = []
icon_labels: List[Dict] = []
mini_labels_fg: List[Dict] = []
mini_labels_bg: List[Dict] = []
polys: List[Dict] = []

for c in centers:
    cnt = c["count"]; clat, clon = c["lat"], c["lon"]
    if cnt <= FANOUT_THRESHOLD:
        offs = spiderfy(clon, clat, cnt, base_px=16, gap_px=8)
        for (lon, lat), row in zip(offs, c["rows"]):
            sty = CAT_STYLE.get(row["category"], CAT_STYLE["ãã®ä»–"])
            points.append({
                "position":[lon,lat], "color":sty["color"], "radius":sty["radius"],
                "c":row["category"], "s":row["summary"], "m":row.get("municipality",""),
                "pred": row.get("pred",""), "src": row.get("src", EHIME_POLICE_URL),
                "r": int(row.get("radius_m", 600)),
                "ico": sty["icon"],
            })
            icon_labels.append({"position":[lon,lat], "label":sty["icon"], "tcolor":[255,255,255,235], "offset":[0,-2]})
            if len(mini_labels_fg) < MAX_LABELS:
                vtxt = (row["summary"] or "")[:4]; vtxt = "\n".join(list(vtxt))
                offset_px = int(-14*LABEL_SCALE)
                mini_labels_bg.append({"position":[lon,lat],"label":vtxt,"tcolor":[0,0,0,220],"offset":[0,offset_px]})
                mini_labels_fg.append({"position":[lon,lat],"label":vtxt,"tcolor":[255,255,255,235],"offset":[0,offset_px]})
            polys.append({"lon":lon,"lat":lat,"r":int(row.get("radius_m",600))})
    else:
        points.append({"position":[clon,clat],"color":[100,100,100,210],"radius":70,"c":f"{cnt}ä»¶","s":"å‘¨è¾ºã«å¤šæ•°","m":"","pred":"","src":EHIME_POLICE_URL,"r":0,"ico":"â—"})
        icon_labels.append({"position":[clon,clat], "label":"â—", "tcolor":[255,255,255,230], "offset":[0,-2]})
        if len(mini_labels_fg) < MAX_LABELS:
            mini_labels_bg.append({"position":[clon,clat],"label":str(cnt),"tcolor":[0,0,0,220],"offset":[0,-12]})
            mini_labels_fg.append({"position":[clon,clat],"label":str(cnt),"tcolor":[255,255,255,235],"offset":[0,-12]})

# è¿‘ä¼¼å††

def circle_coords(lon: float, lat: float, radius_m: int = 300, n: int = 64):
    coords = []
    r_earth = 6378137.0
    dlat = radius_m / r_earth
    dlon = radius_m / (r_earth * math.cos(math.radians(lat)))
    for i in range(n):
        ang = 2 * math.pi * i / n
        lat_i = lat + math.degrees(dlat * math.sin(ang))
        lon_i = lon + math.degrees(dlon * math.cos(math.radians(lat)))
        coords.append([lon_i, lat_i])
    coords.append(coords[0]); return coords

geo_features = []
for p in points:
    if p.get("r",0) > 0:
        geo_features.append({"type":"Feature","geometry":{"type":"Polygon","coordinates":[circle_coords(p["position"][0], p["position"][1], int(p["r"]))]},"properties":{}})
geojson = {"type":"FeatureCollection","features": geo_features}

# ===== ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ =====
col_map, col_feed = st.columns([7,5], gap="large")

with col_map:
    TILESETS = {
        "GSI æ·¡è‰²": {"url": "https://cyberjapandata.gsi.go.jp/xyz/pale/{z}/{x}/{y}.png", "max_zoom": 18},
        "GSI æ¨™æº–": {"url": "https://cyberjapandata.gsi.go.jp/xyz/std/{z}/{x}/{y}.png", "max_zoom": 18},
        "OpenStreetMap Standard": {"url": "https://tile.openstreetmap.org/{z}/{x}/{y}.png", "max_zoom": 19},
        "OSM Humanitarian": {"url": "https://tile-a.openstreetmap.fr/hot/{z}/{x}/{y}.png", "max_zoom": 19},
        "OpenTopoMap": {"url": "https://a.tile.opentopomap.org/{z}/{x}/{y}.png", "max_zoom": 17},
    }
    TILE = TILESETS.get(map_choice, TILESETS["GSI æ·¡è‰²"])

    is_3d = (mode_3d == "3D")
    hex_layer = pdk.Layer(
        "HexagonLayer",
        data=[{"position":x["position"],"count":x["count"]} for x in hex_points],
        get_position="position",
        get_elevation_weight="count",
        elevation_scale=10 if is_3d else 5,
        elevation_range=[0,1200 if is_3d else 800],
        extruded=is_3d,
        radius=500,
        coverage=0.9,
        opacity=0.25 if is_3d else 0.22,
        pickable=False
    )

    layers = [
        pdk.Layer("TileLayer", data=TILE["url"], min_zoom=0, max_zoom=TILE.get("max_zoom",18), tile_size=256, opacity=1.0),
        hex_layer,
        pdk.Layer("GeoJsonLayer", data=geojson, pickable=False, stroked=True, filled=True,
                  get_line_width=2, get_line_color=[0,160,220], get_fill_color=[0,160,220,40], auto_highlight=False),
        pdk.Layer("ScatterplotLayer", data=points, get_position="position", get_fill_color="color", get_radius="radius",
                  pickable=True, radius_min_pixels=3, radius_max_pixels=60),
        pdk.Layer("TextLayer", data=icon_labels, get_position="position", get_text="label", get_color="tcolor",
                  get_size=14, get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
        pdk.Layer("TextLayer", data=mini_labels_bg, get_position="position", get_text="label", get_color="tcolor",
                  get_size=int(12*LABEL_SCALE), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
        pdk.Layer("TextLayer", data=mini_labels_fg, get_position="position", get_text="label", get_color="tcolor",
                  get_size=int(12*LABEL_SCALE), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
    ]

    tooltip = {
        "html": "<b>{c}</b><br/>{s}<br/>{m}<br/>äºˆæ¸¬: {pred}<br/><a href='{src}' target='_blank'>å‡ºå…¸</a>",
        "style": {"backgroundColor":"rgba(10,15,20,.96)","color":"#e6f1ff","maxWidth":"280px","whiteSpace":"normal",
                  "wordBreak":"break-word","lineHeight":1.4,"fontSize":"12px","padding":"10px 12px",
                  "borderRadius":"12px","border":"1px solid var(--border)"}
    }

    initial_view = pdk.ViewState(latitude=EHIME_PREF_LAT, longitude=EHIME_PREF_LON, zoom=9, pitch=(45 if is_3d else 0), bearing=0)
    deck = pdk.Deck(layers=layers, initial_view_state=initial_view, tooltip=tooltip, map_provider=None, map_style=None)
    st.pydeck_chart(deck, use_container_width=True, height=560)

    legend_items = []
    for k, v in CAT_STYLE.items():
        rgba = f"rgba({v['color'][0]}, {v['color'][1]}, {v['color'][2]}, {v['color'][3]/255:.9f})"
        legend_items.append(f"<span class='item'><span class='dot' style='background:{rgba}'></span>{k}</span>")
    st.markdown(f"<div class='legend'>{''.join(legend_items)}<div style='margin-top:6px;color:var(--muted);font-size:.85rem'>å††ã¯æ¦‚ã­ã®ç¯„å›²ã€‚è©³ç´°ã¯å‡ºå…¸ã‚’ç¢ºèªã€‚</div></div>", unsafe_allow_html=True)

with col_feed:
    st.markdown("<div class='panel'>", unsafe_allow_html=True)
    st.markdown("#### é€Ÿå ±ãƒ•ã‚£ãƒ¼ãƒ‰")
    q = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¸‚ç”ºåã‚„è¦ç‚¹ï¼‰")
    feed = df.copy()
    if q:
        feed = feed[feed.apply(lambda r: q in (r.get("summary") or "") or q in (r.get("municipality") or ""), axis=1)]
    total = len(feed)
    page_size = st.slider("ãƒšãƒ¼ã‚¸å½“ãŸã‚Šä»¶æ•°", 10, 50, 30, 5)
    pages = max(1, (total + page_size - 1)//page_size)
    page = st.number_input("ãƒšãƒ¼ã‚¸", 1, pages, 1)
    view = feed.iloc[(page-1)*page_size : page*page_size]

    html_list = ["<div class='feed-scroll'>"]
    for _, r in view.iterrows():
        cat = r.get('category','')
        html_list.append("<div class='feed-card'>")
        html_list.append(f"<div><b>{cat}</b></div>")
        html_list.append(f"<div>{r.get('summary','')}</div>")
        muni = r.get('municipality') or ''
        if muni: html_list.append(f"<div style='color:var(--muted);font-size:.9rem'>{muni}</div>")
        html_list.append(f"<div style='color:#00b894;font-size:.9rem'>äºˆæ¸¬: {r.get('pred','')}</div>")
        html_list.append(f"<div style='color:var(--muted);font-size:.9rem'><a href='{r.get('src')}' target='_blank'>å‡ºå…¸</a></div>")
        html_list.append("</div>")
    html_list.append("</div>")
    st.markdown("\n".join(html_list), unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

st.caption("åœ°å›³: GSI / OSM / OpenTopoMapï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰ | æƒ…å ±: çœŒè­¦é€Ÿå ±ã€‚ç·Šæ€¥æ™‚ã¯110ãƒ»119ã¸ã€‚")
