# -*- coding: utf-8 -*-
# æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  v9.9-A4
# - ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã‚’ {info_html} ã®ã¿ã«çµ±ä¸€ï¼ˆæœªå±•é–‹ã® {c}{s}{m}{pred} ã‚’æ’é™¤ï¼‰
# - JARTIC ç‚¹ãƒ»ç·šãƒ›ãƒãƒ¼ï¼šæ™‚åˆ»ï¼ˆJSTï¼‰ãƒ»åˆè¨ˆãƒ»ä¸Šã‚Šãƒ»ä¸‹ã‚Š
# - æ“¬ä¼¼æ¸‹æ»ç·šã®é•·ã•ã‚’ 300mã€œ1000m ã«å‹•çš„æ‹¡å¼µï¼ˆäº¤é€šé‡ã«å¿œã˜ã¦ï¼‰
# - æ—¢å­˜æ©Ÿèƒ½ç¶­æŒï¼šé€Ÿå ±â†’ä½ç½®æ¨å®šã€äº¤å·®ç‚¹ãƒ’ãƒ¼ãƒˆ/3DæŸ± ON/OFFã€JARTIC ç‚¹ã€æ“¬ä¼¼ç·šã€ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ã€ãƒ•ã‚£ãƒ¼ãƒ‰ ç­‰

import os, re, math, time, json, sqlite3, threading, unicodedata, hashlib
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from io import StringIO

import httpx
import requests
import pandas as pd
import streamlit as st
import pydeck as pdk
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process as rf_process
import h3

# === Gemini (optional) ===
try:
    import google.generativeai as genai
    _HAS_GEMINI = True
except Exception:
    _HAS_GEMINI = False

APP_TITLE = "æ„›åª›ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ "
EHIME_POLICE_URL = "https://www.police.pref.ehime.jp/sokuho/sokuho.htm"
USER_AGENT = "ESP/9.9-A4 (info_html+line300-1000)"
TIMEOUT = 15
TTL_HTML = 600
MAX_WORKERS = 6

EHIME_PREF_LAT = 33.8390
EHIME_PREF_LON = 132.7650
EHIME_BBOX = (132.2, 33.0, 133.7, 34.2)  # (minLon, minLat, maxLon, maxLat)

FUTURE_BUFFER_SCALE = 1.8
ZOOM_LIKE = 10
FANOUT_THRESHOLD = 4
LABEL_SCALE = 1.0
MAX_LABELS = 400

CATEGORY_PATTERNS = [
    ("äº¤é€šäº‹æ•…", r"äº¤é€š.*äº‹æ•…|è‡ªè»¢è»Š|ãƒã‚¹|äºŒè¼ª|ä¹—ç”¨|è¡çª|äº¤å·®ç‚¹|å›½é“|çœŒé“|äººèº«äº‹æ•…"),
    ("ç«ç½",     r"ç«ç½|å‡ºç«|å…¨ç„¼|åŠç„¼|å»¶ç„¼"),
    ("æ­»äº¡äº‹æ¡ˆ", r"æ­»äº¡|æ­»äº¡äº‹æ¡ˆ"),
    ("çªƒç›—",     r"çªƒç›—|ä¸‡å¼•|ç›—"),
    ("è©æ¬º",     r"è©æ¬º|é‚„ä»˜é‡‘|æŠ•è³‡è©æ¬º|ç‰¹æ®Šè©æ¬º"),
    ("äº‹ä»¶",     r"å¨åŠ›æ¥­å‹™å¦¨å®³|æ¡ä¾‹é•å|æš´è¡Œ|å‚·å®³|è„…è¿«|å™¨ç‰©æå£Š|é’å°‘å¹´ä¿è­·"),
]
CITY_NAMES = [
    "æ¾å±±å¸‚","ä»Šæ²»å¸‚","æ–°å±…æµœå¸‚","è¥¿æ¡å¸‚","å¤§æ´²å¸‚","ä¼Šäºˆå¸‚","å››å›½ä¸­å¤®å¸‚",
    "è¥¿äºˆå¸‚","æ±æ¸©å¸‚","ä¸Šå³¶ç”º","ä¹…ä¸‡é«˜åŸç”º","æ¾å‰ç”º","ç ¥éƒ¨ç”º","å†…å­ç”º",
    "ä¼Šæ–¹ç”º","æ¾é‡ç”º","é¬¼åŒ—ç”º","æ„›å—ç”º","å®‡å’Œå³¶å¸‚","å…«å¹¡æµœå¸‚"
]
MUNI_CENTERS = {
    "æ¾å±±å¸‚":(132.7650,33.8390),"ä»Šæ²»å¸‚":(133.0000,34.0660),"æ–°å±…æµœå¸‚":(133.2830,33.9600),
    "è¥¿æ¡å¸‚":(133.1830,33.9180),"å¤§æ´²å¸‚":(132.5500,33.5000),"ä¼Šäºˆå¸‚":(132.7010,33.7550),
    "å››å›½ä¸­å¤®å¸‚":(133.5500,33.9800),"è¥¿äºˆå¸‚":(132.5000,33.3660),"æ±æ¸©å¸‚":(132.8710,33.7930),
    "ä¸Šå³¶ç”º":(133.2000,34.2600),"ä¹…ä¸‡é«˜åŸç”º":(132.9040,33.5380),"æ¾å‰ç”º":(132.7110,33.7870),
    "ç ¥éƒ¨ç”º":(132.7870,33.7350),"å†…å­ç”º":(132.6580,33.5360),"ä¼Šæ–¹ç”º":(132.3560,33.4880),
    "æ¾é‡ç”º":(132.7570,33.2260),"é¬¼åŒ—ç”º":(132.8800,33.2280),"æ„›å—ç”º":(132.5660,33.0000),
    "å®‡å’Œå³¶å¸‚":(132.5600,33.2230),"å…«å¹¡æµœå¸‚":(132.4230,33.4620),
}

TILESETS: Dict[str, Dict] = {
    "æ¨™æº–":     {"url":"https://tile.openstreetmap.org/{z}/{x}/{y}.png", "max_zoom":19},
    "æ·¡è‰²":     {"url":"https://cyberjapandata.gsi.go.jp/xyz/pale/{z}/{x}/{y}.png", "max_zoom":18},
    "åœ°å½¢":     {"url":"https://a.tile.opentopomap.org/{z}/{x}/{y}.png", "max_zoom":17},
    "äººé“æ”¯æ´": {"url":"https://tile-a.openstreetmap.fr/hot/{z}/{x}/{y}.png", "max_zoom":19},
    "èˆªç©ºå†™çœŸ": {"url":"https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg", "max_zoom":18},
}

# ----------------------------------------------------------------------------
# UI / CSS
# ----------------------------------------------------------------------------
st.set_page_config(page_title="Ehime Safety Platform", layout="wide")
st.markdown("""
<style>
  :root{ --bg:#0b0f14; --panel:#0f141b; --panel2:#121924; --text:#e8f1ff; --muted:#8aa0b6; --border:#2b3a4d; --a:#007aff; --b:#00b894; }
  @media (prefers-color-scheme: light){ :root{ --bg:#f7fafc; --panel:#ffffff; --panel2:#f1f5f9; --text:#0f2230; --muted:#586b7a; --border:#dfe7ef; --a:#005acb; --b:#009a7a; } }
  html, body, .stApp { background: var(--bg); color: var(--text); }
  .topbar{ position: sticky; top:0; z-index:10; padding:14px 16px; margin:-16px -16px 14px -16px; border-bottom:1px solid var(--border); background:var(--panel); }
  .brand{ display:flex; align-items:center; gap:10px; font-weight:800; font-size:1.05rem; }
  .brand .id{ width:28px; height:28px; border-radius:8px; display:grid; place-items:center; background: linear-gradient(135deg,var(--a),var(--b)); color:#00131a; font-weight:900; }
  .subnote{ color: var(--muted); font-size:.85rem; margin-top:4px}
  .panel { background: var(--panel); border:1px solid var(--border); border-radius: 14px; padding: 10px 12px; }
  .legend { font-size:.95rem; background:var(--panel); border:1px solid var(--border); border-radius:12px; padding:10px 12px;}
  .legend .item { display:inline-flex; align-items:center; margin-right:14px; margin-bottom:6px}
  .dot { width:12px; height:12px; border-radius:50%; display:inline-block; margin-right:6px; border:1px solid #0003}
  .feed-card {background:var(--panel); padding:12px 14px; border-radius:14px; border:1px solid var(--border); margin-bottom:10px;}
  .feed-scroll {max-height:62vh; overflow-y:auto; padding-right:6px}
  @media (max-width: 640px){ .feed-scroll{max-height:48vh} }
  a { color: var(--a); }
  .riskbar{height:10px; border-radius:6px; background:linear-gradient(90deg,#ffd4d4,#ff6b6b,#d90429);}
  .risklbl{display:flex; justify-content:space-between; font-size:.85rem; color: var(--muted); margin-top:4px}
  .hud { position: relative; height:0; }
  .hud-inner { position: relative; top:-36px; left:6px; display:flex; gap:6px; flex-wrap:wrap; }
  .hud .badge { background:rgba(16,20,27,.88); color:#e8f1ff; border:1px solid var(--border); padding:4px 8px; border-radius:10px; font-size:.85rem; }
  @media (prefers-color-scheme: light){ .hud .badge{ background:rgba(255,255,255,.9); color:#0f2230; } }
  .jartic-grad { height: 10px; border-radius:6px; background: linear-gradient(90deg, #3c78c8, #4ec67a, #f0d438, #e63c3c); }
  .redline-sample { height: 0; border-top:4px solid #e60000; width: 80px; border-radius:2px; }
</style>
""", unsafe_allow_html=True)

st.markdown(
    "<div class='topbar'><div class='brand'><div class='id'>ES</div>"
    f"<div><div>{APP_TITLE}</div><div class='subnote'>ä»Šã«å¼·ã„ãƒ»å…ˆã‚’èª­ã‚€ã€‚åœ°å›³ã§ä¸€ç›®ã€è¦ç‚¹ã¯ç°¡æ½”ã€‚</div></div>"
    "</div></div>", unsafe_allow_html=True
)

# ----------------------------------------------------------------------------
# Session state
# ----------------------------------------------------------------------------
if "map_choice" not in st.session_state:
    st.session_state.map_choice = "æ¨™æº–"
if "show_jartic_points" not in st.session_state:
    st.session_state.show_jartic_points = True
if "show_snap_lines" not in st.session_state:
    st.session_state.show_snap_lines = True
if "show_hotspots" not in st.session_state:
    st.session_state.show_hotspots = True

# ----------------------------------------------------------------------------
# SQLite cache
# ----------------------------------------------------------------------------
@st.cache_resource
def get_sqlite():
    os.makedirs("data", exist_ok=True)
    conn = sqlite3.connect("data/esp_cache.sqlite", check_same_thread=False)
    with conn:
        conn.execute("CREATE TABLE IF NOT EXISTS geocode_cache(key TEXT PRIMARY KEY, json TEXT, created_at TEXT)")
        conn.execute("CREATE TABLE IF NOT EXISTS llm_cache(key TEXT PRIMARY KEY, json TEXT, created_at TEXT)")
    return conn
_conn = get_sqlite(); _conn_lock = threading.Lock()
def cache_get(table:str, key:str) -> Optional[str]:
    with _conn_lock:
        row = _conn.execute(f"SELECT json FROM {table} WHERE key=?", (key,)).fetchone()
    return row[0] if row else None
def cache_put(table:str, key:str, payload:str):
    with _conn_lock, _conn:
        _conn.execute(f"INSERT OR REPLACE INTO {table} VALUES (?,?,datetime('now'))", (key, payload))

# ----------------------------------------------------------------------------
# çœŒè­¦é€Ÿå ±ã®å–å¾—ãƒ»è§£æ
# ----------------------------------------------------------------------------
@dataclass
class IncidentItem:
    heading: str
    body: str
    incident_date: Optional[str]

EHIME_POLICE_URL_TXT = EHIME_POLICE_URL

@st.cache_data(ttl=TTL_HTML)
def fetch_ehime_html() -> str:
    headers = {"User-Agent": USER_AGENT}
    for attempt in range(3):
        try:
            with httpx.Client(headers=headers, timeout=TIMEOUT) as client:
                r = client.get(EHIME_POLICE_URL)
                if r.status_code in (429, 503):
                    raise httpx.HTTPStatusError("rate limited", request=None, response=r)
                r.raise_for_status()
                enc = r.encoding or 'utf-8'
                try:
                    r.encoding = r.apparent_encoding or enc
                except Exception:
                    r.encoding = enc
                return r.text
        except Exception:
            time.sleep(0.6 * (attempt + 1))
    try:
        r = requests.get(EHIME_POLICE_URL, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        r.encoding = r.apparent_encoding or r.encoding or "utf-8"
        return r.text
    except Exception:
        st.warning("é€Ÿå ±ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç©ºãƒ‡ãƒ¼ã‚¿ã§è¡¨ç¤ºã—ã¾ã™ã€‚")
        return "<html><body></body></html>"

def parse_items(html: str) -> List[IncidentItem]:
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    text = re.sub(r"ã€æ„›åª›çœŒè­¦ã‹ã‚‰ã®ãŠé¡˜ã„ï¼ã€‘[\s\S]*?(?=â– |$)", "", text)
    lines = [ln for ln in text.split("\n") if ln.strip()]
    items, cur = [], None
    for ln in lines:
        if ln.startswith("â– "):
            if cur: items.append(cur)
            cur = {"heading": ln.strip(), "body": []}
        else:
            if cur: cur["body"].append(ln.strip())
    if cur: items.append(cur)

    out: List[IncidentItem] = []
    today = datetime.now().date(); cy = today.year
    for b in items:
        h = b.get("heading", ""); body = " ".join(b.get("body", []))
        m = re.search(r"ï¼ˆ?(\d{1,2})æœˆ(\d{1,2})æ—¥", h)
        d = None
        if m:
            mm, dd = int(m.group(1)), int(m.group(2)); y = cy
            try:
                dt = datetime(y, mm, dd).date();  y -= 1 if dt > today else 0
                d = datetime(y, mm, dd).date().isoformat()
            except Exception:
                d = None
        out.append(IncidentItem(h.replace("â– ", "").strip(), body, d))
    return out

def rule_extract(it: IncidentItem) -> Dict:
    t = it.heading + " " + it.body
    cat = next((name for name, pat in CATEGORY_PATTERNS if re.search(pat, t)), "ãã®ä»–")
    muni = next((c for c in CITY_NAMES if c in t), None)
    hints = ["å°å­¦æ ¡","ä¸­å­¦æ ¡","é«˜æ ¡","å¤§å­¦","å­¦æ ¡","ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰","ä½“è‚²é¤¨","å…¬åœ’","é§…","æ¸¯","ç—…é™¢","äº¤å·®ç‚¹"]
    places: List[str] = []
    for h in hints:
        places += re.findall(rf"([\w\u3040-\u30ff\u4e00-\u9fffA-Za-z0-9]+{h})", t)[:2]
    s = re.sub(r"\s+", " ", it.body).strip()
    s = s[:120] + ("â€¦" if len(s)>120 else "")
    return {"category":cat,"municipality":muni,"place_strings":list(dict.fromkeys(places))[:3],
            "summary": s or it.heading, "date": it.incident_date}

@st.cache_resource
def load_gazetteer(path:str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(path)
        for c in ("name","type","lon","lat"):
            if c not in df.columns: return None
        df["alt_names"] = df.get("alt_names", "").fillna("")
        return df
    except Exception:
        return None

class GazetteerIndex:
    def __init__(self, df: pd.DataFrame):
        self.df = df.reset_index(drop=True)
        self.keys = (df["name"].astype(str) + " | " + df["alt_names"].astype(str)).tolist()
    def search(self, q:str, min_score:int=78) -> Optional[Tuple[float,float,str]]:
        m = self.df[(self.df["name"].str.contains(q, na=False)) | (self.df["alt_names"].str.contains(q, na=False))]
        if not m.empty:
            r = m.iloc[0]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        hit = rf_process.extractOne(q, self.keys, scorer=fuzz.token_set_ratio)
        if hit and hit[1] >= min_score:
            r = self.df.iloc[hit[2]]; return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore
        return None

def nominatim_geocode(name:str, municipality:Optional[str]) -> Optional[Tuple[float,float]]:
    try:
        q = f"{name} {municipality or ''} æ„›åª›çœŒ æ—¥æœ¬".strip()
        headers = {"User-Agent": USER_AGENT}
        params = {"q": q, "format": "jsonv2", "limit": 1}
        for i in range(3):
            r = requests.get("https://nominatim.openstreetmap.org/search", params=params, headers=headers, timeout=TIMEOUT)
            if r.status_code in (429,503):
                time.sleep(0.6 * (i+1)); continue
            r.raise_for_status()
            arr = r.json()
            if isinstance(arr, list) and arr:
                return float(arr[0]["lon"]), float(arr[0]["lat"])  # lon, lat
        return None
    except Exception:
        return None

def _read_gemini_key() -> str:
    try:
        if "gemini" in st.secrets and "API_KEY" in st.secrets["gemini"]:
            return st.secrets["gemini"]["API_KEY"]
    except Exception:
        pass
    return os.getenv("GEMINI_API_KEY", "")

def gemini_candidates(full_text: str, muni_hint: Optional[str]) -> List[Dict]:
    api_key = _read_gemini_key()
    if not (_HAS_GEMINI and api_key):
        return []
    key = hashlib.sha1(f"gem9|{muni_hint or ''}|{full_text}".encode("utf-8")).hexdigest()
    cached = cache_get("llm_cache", key)
    if cached:
        try:
            obj = json.loads(cached)
            return obj.get("candidates", []) if isinstance(obj, dict) else []
        except Exception:
            pass
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        system = ("ã‚ãªãŸã¯æ—¥æœ¬ã®ã‚¬ã‚¼ãƒƒãƒ†ã‚£ã‚¢è£œåŠ©ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚"
                  "å…¥åŠ›ã®äº‹ä»¶ãƒ»äº‹æ•…ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€åœ°åã‚„æ–½è¨­åå€™è£œã‚’æŠ½å‡ºã—ã¾ã™ã€‚"
                  "å‡ºåŠ›ã¯ {\"candidates\":[{\"name\":str,\"kind\":str,\"confidence\":0..1}]}")
        muni_line = f"å¸‚ç”ºãƒ’ãƒ³ãƒˆ: {muni_hint}\n" if muni_hint else ""
        prompt = f"{system}\n\nãƒ†ã‚­ã‚¹ãƒˆ:\n{full_text}\n\n{muni_line}JSONã®ã¿ã§å¿œç­”ã€‚"
        resp = model.generate_content(prompt)
        txt = resp.text if hasattr(resp, "text") else str(resp)
        start = txt.find("{"); end = txt.rfind("}")
        if start >=0 and end>start: txt = txt[start:end+1]
        obj = json.loads(txt)
        if isinstance(obj, dict):
            cache_put("llm_cache", key, json.dumps(obj, ensure_ascii=False))
            return obj.get("candidates", [])
    except Exception:
        return []
    return []

# ----------------------------------------------------------------------------
# H3 helpers & presentation helpers
# ----------------------------------------------------------------------------
def h3_cell_from_latlng(lat: float, lon: float, res: int) -> str:
    if hasattr(h3, "geo_to_h3"): return h3.geo_to_h3(lat, lon, res)
    return h3.latlng_to_cell(lat, lon, res)
def h3_latlng_from_cell(cell: str) -> Tuple[float,float]:
    if hasattr(h3, "h3_to_geo"):
        lat, lon = h3.h3_to_geo(cell); return lat, lon
    lat, lon = h3.cell_to_latlng(cell); return lat, lon
def h3_res_from_zoom(zoom_val:int) -> int:
    return {7:5,8:6,9:7,10:8,11:9,12:9,13:10,14:10}.get(zoom_val, 8)
def cluster_points(df: pd.DataFrame, zoom_val:int) -> List[Dict]:
    if df.empty: return []
    res = h3_res_from_zoom(zoom_val)
    groups: Dict[str, List[Dict]] = {}
    for _, r in df.iterrows():
        lon, lat = float(r["lon"]), float(r["lat"])
        cell = h3_cell_from_latlng(lat, lon, res)
        d = r.to_dict(); d["cell"] = cell
        groups.setdefault(cell, []).append(d)
    centers: List[Dict] = []
    for cell, rows in groups.items():
        lat, lon = h3_latlng_from_cell(cell)
        centers.append({"cell":cell, "lon":lon, "lat":lat, "count":len(rows), "rows":rows})
    return centers

def short_summary(s: str, max_len: int = 64) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    return (s[:max_len] + ("â€¦" if len(s) > max_len else "")) if s else ""

def make_prediction(category:str, muni:Optional[str]) -> str:
    return {
        "è©æ¬º":"SNSã‚„æŠ•è³‡ã®èª˜ã„ã«æ³¨æ„ã€‚é€é‡‘å‰ã«å®¶æ—ã‚„è­¦å¯Ÿã¸ç›¸è«‡ã€‚",
        "äº¤é€šäº‹æ•…":"å¤•æ–¹ã‚„é›¨å¤©ã®äº¤å·®ç‚¹ã§å¢—ãˆã‚„ã™ã„ã€‚æ¨ªæ–­ã¨å³å·¦æŠ˜ã«æ³¨æ„ã€‚",
        "çªƒç›—":"è‡ªè»¢è»Šãƒ»è»Šä¸¡ã®æ–½éŒ ã¨é˜²çŠ¯ç™»éŒ²ã€‚å¤œé–“ã®ç„¡æ–½éŒ æ”¾ç½®ã‚’é¿ã‘ã‚‹ã€‚",
        "ç«ç½":"ä¹¾ç‡¥æ™‚ã¯å±‹å¤–ç«æ°—ã«é…æ…®ã€‚é›»æºå‘¨ã‚Šãƒ»å–«ç…™ã®å§‹æœ«ã‚’å†ç¢ºèªã€‚",
        "äº‹ä»¶":"ä¸å¯©é€£çµ¡ã¯è¨˜éŒ²ã‚’æ®‹ã—é€šå ±ã€‚å­¦æ ¡ãƒ»å…¬å…±æ–½è¨­å‘¨è¾ºã§æ„è­˜ã‚’ã€‚",
        "æ­»äº¡äº‹æ¡ˆ":"è©³ç´°ã¯å‡ºå…¸ã§ç¢ºèªã€‚å‘¨è¾ºã§ã¯æ•‘æ€¥æ´»å‹•ã«é…æ…®ã€‚",
    }.get(category, "åŒç¨®äº‹æ¡ˆãŒç¶šãå¯èƒ½æ€§ã€‚å‡ºå…¸ã§æœ€æ–°ã‚’ç¢ºèªã€‚")

CAT_STYLE = {
    "äº¤é€šäº‹æ•…": {"color":[220, 60, 60, 235],   "radius":86, "icon":"â–²"},
    "ç«ç½":     {"color":[245, 130, 50, 235],  "radius":88, "icon":"ğŸ”¥"},
    "æ­»äº¡äº‹æ¡ˆ": {"color":[170, 120, 240, 235], "radius":92, "icon":"âœ–"},
    "çªƒç›—":     {"color":[70, 150, 245, 235],  "radius":78, "icon":"ğŸ”“"},
    "è©æ¬º":     {"color":[40, 180, 160, 235],  "radius":78, "icon":"âš "},
    "äº‹ä»¶":     {"color":[245, 200, 60, 235],  "radius":82, "icon":"ï¼"},
    "ãã®ä»–":   {"color":[128, 144, 160, 220], "radius":70, "icon":"ãƒ»"},
}

# ---- tooltip html makers ----
def make_incident_info_html(c:str, s:str, m:str, pred:str) -> str:
    parts = []
    if c: parts.append(f"<b>{c}</b>")
    if s: parts.append(s)
    if m: parts.append(m)
    if pred: parts.append(f"äºˆæ¸¬: {pred}")
    parts.append(f"<a href='{EHIME_POLICE_URL_TXT}' target='_blank'>å‡ºå…¸</a>")
    return "<div style='min-width:240px'>" + "<br/>".join(parts) + "</div>"

def make_jartic_info_html(tlabel:str, total:int, up:int, down:int) -> str:
    return (
        "<div style='min-width:240px'>"
        "<b>JARTICï¼ˆ5åˆ†å€¤ï¼‰</b><br/>"
        f"æ™‚åˆ»: {tlabel}<br/>"
        f"åˆè¨ˆ: {total} å°/5åˆ†<br/>"
        f"ä¸Šã‚Š: {up} / ä¸‹ã‚Š: {down}"
        "</div>"
    )

# ----------------------------------------------------------------------------
# JARTIC Open Traffic (5åˆ†å€¤)
# ----------------------------------------------------------------------------
JARTIC_WFS_URL = "https://api.jartic-open-traffic.org/geoserver"

def jst_now() -> datetime:
    return datetime.utcnow() + timedelta(hours=9)

def round_to_5min(d: datetime) -> datetime:
    mm = (d.minute // 5) * 5
    return d.replace(minute=mm, second=0, microsecond=0)

@st.cache_data(ttl=180)
def fetch_jartic_5min(bbox: Tuple[float,float,float,float] = EHIME_BBOX) -> Tuple[Optional[Dict], Optional[str]]:
    """
    Returns: (geojson, tlabel) where tlabel like 'YYYY-MM-DD HH:MM (JST)'
    """
    t = round_to_5min(jst_now() - timedelta(minutes=20))
    tcode = t.strftime("%Y%m%d%H%M")
    tlabel = t.strftime("%Y-%m-%d %H:%M (JST)")
    cql = (
        f"é“è·¯ç¨®åˆ¥=3 AND æ™‚é–“ã‚³ãƒ¼ãƒ‰={tcode} AND "
        f"BBOX(ã‚¸ã‚ªãƒ¡ãƒˆãƒª,{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]},'EPSG:4326')"
    )
    params = {
        "service": "WFS", "version": "2.0.0", "request": "GetFeature",
        "typeNames": "t_travospublic_measure_5m",
        "srsName": "EPSG:4326", "outputFormat": "application/json",
        "exceptions": "application/json", "cql_filter": cql
    }
    try:
        r = requests.get(JARTIC_WFS_URL, params=params, timeout=TIMEOUT)
        r.raise_for_status()
        return r.json(), tlabel
    except Exception:
        return None, None

def jartic_features_to_points(geojson: Dict, tlabel: str) -> List[Dict]:
    if not geojson or "features" not in geojson: return []
    pts: List[Dict] = []
    for f in geojson["features"]:
        g = f.get("geometry") or {}
        if g.get("type") == "MultiPoint":
            prop = f.get("properties", {}) or {}
            up = sum(filter(None, [
                prop.get("ä¸Šã‚Šãƒ»å°å‹äº¤é€šé‡"), prop.get("ä¸Šã‚Šãƒ»å¤§å‹äº¤é€šé‡"), prop.get("ä¸Šã‚Šãƒ»è»Šç¨®åˆ¤åˆ¥ä¸èƒ½äº¤é€šé‡")
            ]))
            down = sum(filter(None, [
                prop.get("ä¸‹ã‚Šãƒ»å°å‹äº¤é€šé‡"), prop.get("ä¸‹ã‚Šãƒ»å¤§å‹äº¤é€šé‡"), prop.get("ä¸‹ã‚Šãƒ»è»Šç¨®åˆ¤åˆ¥ä¸èƒ½äº¤é€šé‡")
            ]))
            up = int(up or 0); down = int(down or 0); total = up + down
            for lon, lat in g.get("coordinates", []):
                pts.append({
                    "position":[float(lon), float(lat)],
                    "up": up, "down": down, "total": total,
                    "jt_time": tlabel, "jt_total": total, "jt_up": up, "jt_down": down,
                })
    return pts

def color_from_total(total:int) -> List[int]:
    # é’â†’ç·‘â†’é»„â†’èµ¤
    t = min(1.0, max(0.0, total / 600.0))
    if t < 0.33:
        u = t/0.33; r = int(60*(1-u)+60*u); g = int(120*(1-u)+200*u); b = int(200*(1-u)+80*u)
    elif t < 0.66:
        u = (t-0.33)/0.33; r = int(60*(1-u)+240*u); g = int(200*(1-u)+220*u); b = int(80*(1-u)+20*u)
    else:
        u = (t-0.66)/0.34; r = int(240); g = int(220*(1-u)+60*u); b = 20
    return [r,g,b, 230]

def size_from_total(total:int) -> int:
    # ç‚¹ã‚’å¤§ãã‚ã«
    return 48 + int(min(260, total * 0.9))

# ----------------------------------------------------------------------------
# OSMï¼ˆOverpassï¼‰ï¼†æ“¬ä¼¼æ¸‹æ»ç·šï¼ˆèµ¤ï¼‰300mã€œ1000m
# ----------------------------------------------------------------------------
OVERPASS_URL = "https://overpass-api.de/api/interpreter"

@st.cache_data(ttl=600)
def fetch_osm_roads_overpass(bbox: Tuple[float,float,float,float] = EHIME_BBOX) -> List[List[List[float]]]:
    minLon, minLat, maxLon, maxLat = bbox
    q = f"""
    [out:json][timeout:25];
    (
      way["highway"~"^(motorway|trunk|primary|secondary|tertiary)$"]({minLat},{minLon},{maxLat},{maxLon});
    );
    out geom;
    """
    try:
        r = requests.post(OVERPASS_URL, data={"data": q}, timeout=TIMEOUT)
        r.raise_for_status()
        data = r.json()
        lines: List[List[List[float]]] = []
        for el in data.get("elements", []):
            if el.get("type") == "way" and "geometry" in el:
                coords = [[pt["lon"], pt["lat"]] for pt in el["geometry"]]
                if len(coords) >= 2:
                    lines.append(coords)
        return lines
    except Exception:
        return []

def _nearest_on_segment(p: Tuple[float,float], a: Tuple[float,float], b: Tuple[float,float]) -> Tuple[Tuple[float,float], Tuple[float,float], float]:
    ax, ay = a; bx, by = b; px, py = p
    kx = 111320 * math.cos(math.radians((ay+by)/2)); ky = 110540
    ax2, ay2, bx2, by2, px2, py2 = ax*kx, ay*ky, bx*kx, by*ky, px*kx, py*ky
    vx, vy = bx2-ax2, by2-ay2; wx, wy = px2-ax2, py2-ay2
    seglen2 = vx*vx + vy*vy
    if seglen2 == 0: return (a, (0.0,0.0), math.hypot(px2-ax2, py2-ay2))
    t = max(0.0, min(1.0, (wx*vx + wy*vy) / seglen2))
    projx2, projy2 = ax2 + t*vx, ay2 + t*vy
    proj = (projx2 / kx, projy2 / ky)
    dir_vec = (bx - ax, by - ay)
    dist_m = math.hypot(px2-projx2, py2-projy2)
    return (proj, dir_vec, dist_m)

@st.cache_data(ttl=180)
def build_snap_lines(j_points: List[Dict], roads: List[List[List[float]]],
                     base_min: int = 30, base_max: int = 100000, thresh_m: int = 220) -> List[Dict]:
    """
    äº¤é€šé‡ã«å¿œã˜ã¦ç·šé•·ã‚’ 300mã€œ1000m ã«è‡ªå‹•èª¿æ•´ã€‚
      length_m = base_min + min(base_max-base_min, jt_total*2)  # total>=350ã§ç´„1km
    é“è·¯ãŒé ã„/ç„¡ã„å ´åˆã¯æ±è¥¿çŸ­ç·šï¼ˆåŒã˜é•·ã•ï¼‰ã§ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ã€‚
    """
    out: List[Dict] = []
    if not j_points: return out
    RED = [230, 0, 0, 235]

    for jp in j_points:
        lon, lat = jp["position"]; p = (lon, lat)
        total = int(jp.get("total", jp.get("jt_total", 0)))
        # å¯å¤‰é•·ï¼ˆä¸Šé™1kmï¼‰
        length_m = base_min + min(base_max - base_min, total * 2)

        best = None
        if roads:
            for line in roads:
                for i in range(len(line)-1):
                    a = tuple(line[i]); b = tuple(line[i+1])
                    proj, dv, d = _nearest_on_segment(p, a, b)
                    if (best is None) or (d < best[2]):
                        best = (proj, dv, d)

        if best and best[2] <= thresh_m:
            proj, dv, _ = best
            vx, vy = dv; mag = math.hypot(vx, vy) or 1.0
            ux, uy = vx/mag, vy/mag
            kx = 111320 * math.cos(math.radians(proj[1])); ky = 110540
            half = length_m / 2.0
            dx_deg = (ux * half) / kx; dy_deg = (uy * half) / ky
            p1 = [proj[0] - dx_deg, proj[1] - dy_deg]
            p2 = [proj[0] + dx_deg, proj[1] + dy_deg]
        else:
            # ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ï¼šæ±è¥¿ç·š
            kx = 111320 * math.cos(math.radians(lat))
            half = length_m / 2.0
            dx_deg = half / kx
            p1 = [lon - dx_deg, lat]
            p2 = [lon + dx_deg, lat]

        width_px = 5 + min(12, (total // 100))
        out.append({
            "path": [p1, p2], "color": RED, "width": width_px,
            "info_html": make_jartic_info_html(
                jp.get("jt_time"), jp.get("jt_total", 0), jp.get("jt_up", 0), jp.get("jt_down", 0)
            ),
        })
    return out

# ----------------------------------------------------------------------------
# ãƒ“ãƒ«ãƒ€ãƒ¼ï¼ˆåœ°å›³ãƒ»äº‹æ•…ãƒã‚¤ãƒ³ãƒˆç­‰ï¼‰
# ----------------------------------------------------------------------------
def build_base_layers(map_choice:str, custom_tile:str) -> List[pdk.Layer]:
    tile = TILESETS.get(map_choice, TILESETS["æ¨™æº–"])
    layers: List[pdk.Layer] = [pdk.Layer(
        "TileLayer", id=f"base-{map_choice}", data=tile["url"],
        min_zoom=0, max_zoom=tile.get("max_zoom",18), tile_size=256, opacity=1.0, refinement="no-overlap"
    )]
    if custom_tile.strip():
        layers.append(pdk.Layer("TileLayer", id="custom-overlay", data=custom_tile.strip(),
                                min_zoom=0, max_zoom=22, tile_size=256, opacity=0.6, refinement="no-overlap"))
    return layers

def grid_color_range() -> List[List[int]]:
    return [
        [255, 180, 180, 60], [255, 140, 140, 90], [255, 100, 100, 120],
        [230, 60, 60, 150], [200, 40, 40, 190], [180, 20, 20, 220],
    ]

def color_from_count_factory(max_count:int):
    def _fn(c:int) -> List[int]:
        t = max(0.0, min(1.0, (c-1) / max(1, max_count-1)))
        if t < 0.5:
            u = t/0.5; r = int(255*(1-u)+255*u); g = int(212*(1-u)+107*u); b = int(212*(1-u)+107*u)
        else:
            u = (t-0.5)/0.5; r = int(255*(1-u)+217*u); g = int(107*(1-u)+4*u); b = int(107*(1-u)+41*u)
        return [r,g,b, 190 if c>=5 else 180]
    return _fn

def circle_coords(lon: float, lat: float, radius_m: int = 300, n: int = 64) -> List[List[float]]:
    out: List[List[float]] = []
    r_earth = 6378137.0
    dlat = radius_m / r_earth
    dlon = radius_m / (r_earth * math.cos(math.radians(lat)))
    for i in range(n):
        ang = 2 * math.pi * i / n
        lat_i = lat + math.degrees(dlat * math.sin(ang))
        lon_i = lon + math.degrees(dlon * math.cos(math.radians(lat)))
        out.append([lon_i, lat_i])
    out.append(out[0])
    return out

def build_intersection_layers(hot_df: pd.DataFrame, is_3d: bool) -> List[pdk.Layer]:
    if hot_df.empty: return []
    if is_3d:
        return [pdk.Layer("ColumnLayer", id="hot-columns", data=hot_df,
                          get_position="position", get_elevation="elev", elevation_scale=1.0,
                          radius=65, get_fill_color="rgba", pickable=True, extruded=True)]
    return [pdk.Layer("HeatmapLayer", id="hot-heatmap", data=hot_df,
                      get_position="position", get_weight="weight", radius_pixels=60,
                      intensity=0.85, threshold=0.05, opacity=0.35, pickable=False)]

def build_congestion_grid(df: pd.DataFrame, is_3d: bool) -> List[pdk.Layer]:
    if is_3d or df.empty: return []
    acc = df[df["category"]=="äº¤é€šäº‹æ•…"].copy()
    if acc.empty: return []
    acc["position"] = acc.apply(lambda r: [float(r["lon"]), float(r["lat"])], axis=1)
    acc["weight"] = 1.0
    return [pdk.Layer("GridLayer", id="congestion-grid", data=acc,
                      get_position="position", get_weight="weight",
                      cell_size=200, extruded=False, opacity=0.18, pickable=False, aggregation="SUM",
                      colorRange=grid_color_range())]

def spiderfy(clon: float, clat: float, n: int, base_px: int = 16, gap_px: int = 8) -> List[Tuple[float,float]]:
    out = []; rpx = base_px
    for k in range(n):
        ang = math.radians(137.5 * k)
        dx = rpx*math.cos(ang); dy = rpx*math.sin(ang)
        dlon = dx / (111320 * math.cos(math.radians(clat))); dlat = dy / 110540
        out.append((clon + dlon, clat + dlat)); rpx += gap_px
    return out

def build_points_labels_buffers(df: pd.DataFrame) -> Tuple[List[Dict], List[Dict], List[Dict], List[Dict], Dict]:
    centers = cluster_points(df, ZOOM_LIKE)
    points: List[Dict] = []; icon_fg: List[Dict] = []; mini_fg: List[Dict] = []; mini_bg: List[Dict] = []
    features: List[Dict] = []
    for c in centers:
        cnt = c["count"]; clat, clon = c["lat"], c["lon"]
        if cnt <= FANOUT_THRESHOLD:
            offs = spiderfy(clon, clat, cnt)
            for (lon, lat), row in zip(offs, c["rows"]):
                sty = CAT_STYLE.get(row["category"], CAT_STYLE["ãã®ä»–"])
                p = {
                    "position":[lon,lat], "color":sty["color"], "radius":sty["radius"],
                    "c":row["category"], "s":row.get("summary",""), "m":row.get("municipality",""),
                    "pred":row.get("pred",""), "src":row.get("src", EHIME_POLICE_URL),
                    "r":int(row.get("radius_m",600)), "ico":sty["icon"],
                    "info_html": make_incident_info_html(
                        row["category"], row.get("summary",""), row.get("municipality",""), row.get("pred","")
                    ),
                }
                points.append(p)
                icon_fg.append({"position":[lon,lat], "label":sty["icon"], "tcolor":[255,255,255,235], "offset":[0,-2]})
                if len(mini_fg) < MAX_LABELS:
                    vtxt = (row.get("summary","")[:4])
                    vtxt = "\n".join(list(vtxt))
                    offset_px = int(-14*LABEL_SCALE)
                    mini_bg.append({"position":[lon,lat],"label":vtxt,"tcolor":[0,0,0,220],"offset":[0,offset_px]})
                    mini_fg.append({"position":[lon,lat],"label":vtxt,"tcolor":[255,255,255,235],"offset":[0,offset_px]})
        else:
            points.append({"position":[clon,clat],"color":[100,100,100,210],"radius":70,"c":"é›†ä¸­","s":"å‘¨è¾ºã«å¤šæ•°","m":"","pred":"","src":EHIME_POLICE_URL,"r":0,"ico":"â—",
                           "info_html": make_incident_info_html("é›†ä¸­","å‘¨è¾ºã«å¤šæ•°","", "")})
            icon_fg.append({"position":[clon,clat], "label":"â—", "tcolor":[255,255,255,230], "offset":[0,-2]})
    for p in points:
        if p.get("r",0) > 0:
            lon, lat = p["position"]
            features.append({"type":"Feature","geometry":{"type":"Polygon","coordinates":[circle_coords(lon, lat, int(p["r"]))]},"properties":{}})
    geojson = {"type":"FeatureCollection","features": features}
    return points, icon_fg, mini_fg, mini_bg, geojson

# ----------------------------------------------------------------------------
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆé€Ÿå ±â†’ä½ç½®æ¨å®šï¼‰
# ----------------------------------------------------------------------------
with st.spinner("é€Ÿå ±ã‚’å–å¾—ä¸­â€¦"):
    html = fetch_ehime_html()
raw_items = parse_items(html)
extracted: List[Dict] = []
for it in raw_items:
    ex = rule_extract(it); ex["heading"] = it.heading; ex["full_text"] = (it.heading + " " + it.body).strip()
    extracted.append(ex)

gdf = load_gazetteer("data/gazetteer_ehime.csv")
idx = GazetteerIndex(gdf) if gdf is not None else None
def try_gazetteer(name:str, min_score:int=78) -> Optional[Tuple[float,float,str]]:
    return None if not idx else idx.search(name, min_score)

def resolve_loc(ex: Dict) -> Dict:
    muni = ex.get("municipality"); places = ex.get("place_strings") or []
    full_text = ex.get("full_text", "")
    llm_cands = gemini_candidates(full_text, muni)
    cand_names = [c.get("name","") for c in llm_cands if isinstance(c, dict)]
    queries = [q for q in cand_names if q] + places + ([muni] if muni else [])
    for q in queries:
        hit = try_gazetteer(q, 78)
        if hit:
            lon, lat, typ = hit; return {"lon":lon, "lat":lat, "type":typ or "facility"}
    if muni and muni in MUNI_CENTERS:
        lon, lat = MUNI_CENTERS[muni]; return {"lon":lon, "lat":lat, "type":"city"}
    for q in queries:
        ll = nominatim_geocode(q, muni)
        if ll: return {"lon":ll[0], "lat":ll[1], "type":"facility"}
    return {"lon":EHIME_PREF_LON, "lat":EHIME_PREF_LAT, "type":"pref"}

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as exctr:
    results = list(exctr.map(resolve_loc, extracted))

rows: List[Dict] = []
for ex, loc in zip(extracted, results):
    typ = loc.get("type","city")
    base_radius = {"facility":300,"intersection":250,"town":600,"chome":600,"oaza":900,"aza":900,"city":2000,"pref":2500}.get(typ, 1200)
    radius = int(base_radius * FUTURE_BUFFER_SCALE)
    rows.append({
        "lon": float(loc["lon"]), "lat": float(loc["lat"]),
        "category": ex["category"], "summary": short_summary(ex["summary"], 60),
        "municipality": ex.get("municipality") or "",
        "radius_m": radius,
        "pred": make_prediction(ex["category"], ex.get("municipality")),
        "src": EHIME_POLICE_URL,
    })
df = pd.DataFrame(rows)

# å¤šç™ºäº¤å·®ç‚¹ï¼ˆå†…è”µï¼‰
CSV_TEXT = """åœ°ç‚¹å,ç·¯åº¦,çµŒåº¦,å¹´é–“æœ€å¤šäº‹æ•…ä»¶æ•°,è£œè¶³
å¤©å±±äº¤å·®ç‚¹,33.8223,132.7758,6,æ¾å±±å¸‚å¤©å±±ç”ºä»˜è¿‘ï¼ˆ2023å¹´ã«6ä»¶äº‹æ•…ï¼‰
å’Œæ³‰äº¤å·®ç‚¹,33.8216,132.7554,5,æ¾å±±å¸‚å’Œæ³‰ç”ºä»˜è¿‘ï¼ˆ2023å¹´ã«5ä»¶äº‹æ•…ï¼‰
å°å‚äº¤å·®ç‚¹,33.8266,132.7833,5,æ¾å±±å¸‚ææ¾åœ°åŒºï¼ˆ2023å¹´ã«5ä»¶äº‹æ•…ï¼‰
æœ¬ç”º5ä¸ç›®äº¤å·®ç‚¹,33.8530,132.7588,4,æ¾å±±å¸‚ä¸­å¿ƒéƒ¨ï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
å±±è¶Šäº¤å·®ç‚¹,33.8565,132.7592,4,æ¾å±±å¸‚å±±è¶Šï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
æ¶ˆé˜²å±€å‰äº¤å·®ç‚¹,33.8527,132.7588,4,æ¾å±±å¸‚æœ¬ç”º6ä¸ç›®ï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
å¤§å·æ©‹äº¤å·®ç‚¹,33.8739,132.7521,4,æ¾å±±å¸‚é´¨å·ç”ºï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
ä¹…ç±³äº¤å·®ç‚¹,33.8143,132.7957,4,æ¾å±±å¸‚ä¹…ç±³åœ°åŒºï¼ˆ2023å¹´ã«4ä»¶äº‹æ•…ï¼‰
"""
hot_df = pd.read_csv(StringIO(CSV_TEXT))
hot_df.rename(columns={"åœ°ç‚¹å":"name","ç·¯åº¦":"lat","çµŒåº¦":"lon","å¹´é–“æœ€å¤šäº‹æ•…ä»¶æ•°":"count","è£œè¶³":"note"}, inplace=True)
hot_df["lat"] = hot_df["lat"].astype(float); hot_df["lon"] = hot_df["lon"].astype(float)
hot_df["count"] = hot_df["count"].astype(int)
max_count = int(hot_df["count"].max()) if not hot_df.empty else 1
hot_df["position"] = hot_df.apply(lambda r: [float(r["lon"]), float(r["lat"])], axis=1)
hot_df["weight"] = hot_df["count"].astype(float)
color_from_count = color_from_count_factory(max_count)
hot_df["rgba"] = hot_df["count"].apply(color_from_count)
hot_df["elev"] = hot_df["count"].apply(lambda c: 300 + (c-1)*220)

# ----------------------------------------------------------------------------
# Sidebar
# ----------------------------------------------------------------------------
with st.sidebar:
    st.markdown("<div class='panel'>", unsafe_allow_html=True)

    st.markdown("#### è¡¨ç¤ºæœŸé–“")
    period = st.select_slider("è¡¨ç¤ºæœŸé–“ã‚’é¸æŠ",
                              ["å½“æ—¥","éå»3æ—¥","éå»7æ—¥","éå»30æ—¥"],
                              value="éå»7æ—¥", label_visibility="collapsed")

    st.markdown("#### è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰")
    mode_3d = st.radio("2D / 3D", ["2D","3D"], horizontal=True, index=0)
    init_zoom = st.slider("åˆæœŸã‚ºãƒ¼ãƒ ", 8, 17, 11)

    st.markdown("#### JARTIC äº¤é€šé‡ (5åˆ†)")
    st.session_state.show_jartic_points = st.checkbox("JARTIC 5åˆ†å€¤ï¼ˆç‚¹ï¼‰",
                                                      value=bool(st.session_state.show_jartic_points))
    st.session_state.show_snap_lines   = st.checkbox("JARTIC 5åˆ†å€¤ï¼ˆç·šï¼šOSMã‚¹ãƒŠãƒƒãƒ—ï¼‰",
                                                      value=bool(st.session_state.show_snap_lines))
    st.caption("å…¬é–‹API / ç´„20åˆ†é…å»¶ã€‚ç‚¹=æ–­é¢äº¤é€šé‡ã€ç·š=è¿‘å‚é“è·¯ã¸æ“¬ä¼¼ã‚¹ãƒŠãƒƒãƒ—ï¼ˆèµ¤ã„â€œæ¸‹æ»ç·šâ€ï¼š300mã€œ1kmï¼‰ã€‚")

    st.markdown("#### å±ãªã„äº¤å·®ç‚¹")
    st.session_state.show_hotspots = st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— / 3DæŸ± ã‚’è¡¨ç¤º",
                                                 value=bool(st.session_state.show_hotspots))

    st.markdown("#### ä»»æ„ã‚¿ã‚¤ãƒ«URLï¼ˆé€éPNGæ¨å¥¨ï¼‰")
    custom_tile = st.text_input("ä¾‹: https://â€¦/{z}/{x}/{y}.png", "")

    st.markdown("</div>", unsafe_allow_html=True)

# ----------------------------------------------------------------------------
# Columns
# ----------------------------------------------------------------------------
col_map, col_feed = st.columns([7,5], gap="large")

with col_map:
    is_3d = (mode_3d == "3D")
    layers: List[pdk.Layer] = []
    layers += build_base_layers(st.session_state.map_choice, custom_tile)

    if st.session_state.show_hotspots:
        layers += build_intersection_layers(hot_df, is_3d)

    layers += build_congestion_grid(df, is_3d)

    points, icon_labels, mini_fg, mini_bg, geojson = build_points_labels_buffers(df)
    layers += [
        pdk.Layer("GeoJsonLayer", id="approx-buffers", data=geojson, pickable=False, stroked=True, filled=True,
                  get_line_width=2, get_line_color=[0,160,220], get_fill_color=[0,160,220,40], auto_highlight=False),
        pdk.Layer("ScatterplotLayer", id="incident-points", data=points, get_position="position", get_fill_color="color", get_radius="radius",
                  pickable=True, radius_min_pixels=3, radius_max_pixels=60),
        pdk.Layer("TextLayer", id="icon-labels", data=icon_labels, get_position="position", get_text="label", get_color="tcolor",
                  get_size=14, get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
        pdk.Layer("TextLayer", id="mini-labels-bg", data=mini_bg, get_position="position", get_text="label", get_color="tcolor",
                  get_size=int(12*LABEL_SCALE), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
        pdk.Layer("TextLayer", id="mini-labels-fg", data=mini_fg, get_position="position", get_text="label", get_color="tcolor",
                  get_size=int(12*LABEL_SCALE), get_pixel_offset="offset", get_alignment_baseline="bottom", get_text_anchor="middle"),
    ]

    # --- JARTIC 5åˆ†å€¤ï¼ˆç‚¹/ç·šï¼‰ ---
    j_points: List[Dict] = []; roads: List[List[List[float]]] = []
    if st.session_state.show_jartic_points or st.session_state.show_snap_lines:
        gj, tlabel = fetch_jartic_5min(EHIME_BBOX)
        if gj and tlabel:
            raw_pts = jartic_features_to_points(gj, tlabel)
            j_points = raw_pts

    if st.session_state.show_jartic_points and j_points:
        pts_vis = [{
            "position": p["position"],
            "color": color_from_total(p["total"]),
            "radius": size_from_total(p["total"]),
            "info_html": make_jartic_info_html(p["jt_time"], p["jt_total"], p["jt_up"], p["jt_down"]),
        } for p in j_points]
        layers.append(
            pdk.Layer(
                "ScatterplotLayer", id="jartic-5min-points", data=pts_vis,
                get_position="position", get_fill_color="color", get_radius="radius",
                radius_min_pixels=4, radius_max_pixels=200, opacity=0.60, pickable=True
            )
        )

    if st.session_state.show_snap_lines and j_points:
        roads = fetch_osm_roads_overpass(EHIME_BBOX)
        lines = build_snap_lines(j_points, roads)
        if lines:
            layers.append(
                pdk.Layer(
                    "PathLayer", id="jartic-5min-snaplines", data=lines,
                    get_path="path", get_color="color", get_width="width",
                    width_scale=1, width_min_pixels=3, width_units="pixels",
                    opacity=0.98, pickable=True
                )
            )

    # â–¼ ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã¯ info_html ã®ã¿
    deck = pdk.Deck(
        layers=layers,
        initial_view_state=pdk.ViewState(latitude=EHIME_PREF_LAT, longitude=EHIME_PREF_LON,
                                         zoom=init_zoom, pitch=(45 if is_3d else 0), bearing=0),
        tooltip={"html": "{info_html}",
                 "style": {"backgroundColor":"rgba(10,15,20,.96)","color":"#e6f1ff",
                           "maxWidth":"380px","whiteSpace":"normal","wordBreak":"break-word",
                           "lineHeight":1.4,"fontSize":"12px","padding":"10px 12px",
                           "borderRadius":"12px","border":"1px solid var(--border)"}},
        map_provider=None, map_style=None
    )
    st.pydeck_chart(deck, use_container_width=True, height=640)

    # HUD
    st.markdown(
        "<div class='hud'><div class='hud-inner'>"
        f"<div class='badge'>Zoom: {init_zoom}ï¼ˆåˆæœŸï¼‰</div>"
        "</div></div>", unsafe_allow_html=True
    )

    # å‡¡ä¾‹ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‹JARTICï¼‰
    legend_items = []
    for k, v in CAT_STYLE.items():
        rgba = f"rgba({v['color'][0]}, {v['color'][1]}, {v['color'][2]}, {v['color'][3]/255:.9f})"
        legend_items.append(f"<span class='item'><span class='dot' style='background:{rgba}'></span>{k}</span>")
    jartic_legend = (
        "<div style='margin-top:10px'>"
        "<div style='font-weight:600;margin-bottom:6px'>JARTIC äº¤é€šé‡ï¼ˆç‚¹ï¼‰</div>"
        "<div class='jartic-grad'></div>"
        "<div class='risklbl'><span>ä½</span><span>ä¸­</span><span>é«˜</span><span>éå¸¸ã«é«˜</span></div>"
        "<div style='height:8px'></div>"
        "<div style='font-weight:600;margin-bottom:6px'>JARTIC ç·šï¼ˆæ“¬ä¼¼ï¼‰</div>"
        "<div class='redline-sample'></div>"
        "<div class='risklbl'><span>èµ¤ã„çŸ­ç·šï¼æ¸¬å®šç®‡æ‰€ã®äº¤é€šæµæ–¹å‘ã‚’å¼·èª¿ï¼ˆé•·ã•:300mã€œ1kmï¼‰</span><span></span></div>"
        "</div>"
    )
    st.markdown(
        f"<div class='legend'>{''.join(legend_items)}"
        f"<div style='margin-top:10px'><div class='riskbar'></div>"
        f"<div class='risklbl'><span>äº¤å·®ç‚¹ãƒªã‚¹ã‚¯ï¼ˆä½ï¼‰</span><span>é«˜ï¼šæœ€å¤§ {int(hot_df['count'].max()) if not hot_df.empty else 0}ä»¶/å¹´</span></div></div>"
        f"{jartic_legend}"
        f"</div>",
        unsafe_allow_html=True,
    )

with col_feed:
    st.markdown("<div class='panel'>", unsafe_allow_html=True)
    st.markdown("#### é€Ÿå ±ãƒ•ã‚£ãƒ¼ãƒ‰")
    q = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¸‚ç”ºåã‚„è¦ç‚¹ï¼‰")
    feed = df.copy()
    if q:
        feed = feed[feed.apply(lambda r: q in (r.get("summary") or "") or q in (r.get("municipality") or ""), axis=1)]
    total = len(feed)
    page_size = st.slider("ãƒšãƒ¼ã‚¸å½“ãŸã‚Šä»¶æ•°", 10, 50, 30, 5)
    pages = max(1, (total + page_size - 1)//page_size)
    page = st.number_input("ãƒšãƒ¼ã‚¸", 1, pages, 1)
    view = feed.iloc[(page-1)*page_size : page*page_size]

    html_list = ["<div class='feed-scroll'>"]
    for _, r in view.iterrows():
        cat = r.get('category','')
        html_list.append("<div class='feed-card'>")
        html_list.append(f"<div><b>{cat}</b></div>")
        html_list.append(f"<div>{r.get('summary','')}</div>")
        muni = r.get('municipality') or ''
        if muni: html_list.append(f"<div style='color:var(--muted);font-size:.9rem'>{muni}</div>")
        html_list.append(f"<div style='color:#00b894;font-size:.9rem'>äºˆæ¸¬: {r.get('pred','')}</div>")
        html_list.append(f"<div style='color:var(--muted);font-size:.9rem'><a href='{r.get('src')}' target='_blank'>å‡ºå…¸</a></div>")
        html_list.append("</div>")
    html_list.append("</div>")
    st.markdown("\n".join(html_list), unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

st.caption("åœ°å›³: OSM/GSI/OpenTopoMap/HOT/GSIèˆªç©ºå†™çœŸ | JARTIC 5åˆ†å€¤ï¼ˆç‚¹=é‡ã‚°ãƒ©ãƒ‡/ç·š=èµ¤ã„æ“¬ä¼¼æ¸‹æ» 300mã€œ1kmï¼‰ | äº¤å·®ç‚¹ãƒ’ãƒ¼ãƒˆ/æŸ± ON/OFF | çœŒè­¦é€Ÿå ±ï¼‹äº¤å·®ç‚¹CSVã€‚ç·Šæ€¥æ™‚ã¯110ãƒ»119ã¸ã€‚")
