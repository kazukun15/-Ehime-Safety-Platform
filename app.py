# =============================================
# Ehime Incident/Disaster Map ‚Äì v2 (Gazetteer + UI polish)
# - Streamlit app
# - Data: Ehime Police "‰∫ã‰ª∂‰∫ãÊïÖÈÄüÂ†±" (scrape)
# - NLP: Google Gemini 2.5 Flash (structured JSON)
# - Gazetteer-first geocoding with fuzzy matching; fallback to Nominatim
# - Modern UI/UX: sidebar filters, chips, cards, confidence color scale
# - Initial map center: Ehime Prefectural Office (~33.8390, 132.7650)
# - GTFS intentionally removed per spec
# ---------------------------------------------
# How to run:
#   pip install -r requirements.txt
#   export GEMINI_API_KEY="..."
#   streamlit run app_v2.py
# ---------------------------------------------

import os
import re
import json
import math
import time
import random
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import requests
import pandas as pd
import streamlit as st
import pydeck as pdk
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process as rf_process

APP_TITLE = "ÊÑõÂ™õÔºö‰∫ã‰ª∂„Éª‰∫ãÊïÖ„ÉªÁÅΩÂÆ≥ „Éû„ÉÉ„Éó (v2)"
EHIME_POLICE_URL = "https://www.police.pref.ehime.jp/sokuho/sokuho.htm"
USER_AGENT = "EhimeCivic/1.0; contact: localgov"
REQUEST_TIMEOUT = 15

# Initial center (Ehime Prefectural Government Office vicinity)
EHIME_PREF_LAT = 33.8390
EHIME_PREF_LON = 132.7650

GEMINI_MODEL = "gemini-2.5-flash"
SLEEP_RANGE = (0.8, 1.5)

# -------- Streamlit page config & CSS --------
st.set_page_config(page_title=APP_TITLE, layout="wide")
st.markdown(
    """
    <style>
      :root { --card-bg:#11111110; }
      .big-title {font-size:1.6rem; font-weight:700;}
      .chip {display:inline-block; padding:4px 10px; border-radius:999px; margin:0 6px 6px 0; background:#eceff1; font-size:0.9rem;}
      .chip.on {background:#1e88e5; color:#fff}
      .subtle {color:#666}
      .legend {font-size:0.9rem;}
      .stButton>button {border-radius:999px;}
      .feed-card {background:var(--card-bg); padding:12px 14px; border-radius:12px; border:1px solid #e0e0e0;}
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(f"<div class='big-title'>üó∫Ô∏è {APP_TITLE}</div>", unsafe_allow_html=True)

# -------- Sidebar controls --------
st.sidebar.header("Ë°®Á§∫È†ÖÁõÆ")
show_accidents = st.sidebar.checkbox("‰∫ãÊïÖÊÉÖÂ†±", True)
show_crimes = st.sidebar.checkbox("ÁäØÁΩ™ÊÉÖÂ†±", True)
show_disasters = st.sidebar.checkbox("ÁÅΩÂÆ≥ÊÉÖÂ†±(Ë≠¶Â†±Á≠â)", True)

st.sidebar.header("ÂèñÂæó„Éª„Éó„É©„Ç§„Éê„Ç∑„Éº")
if st.sidebar.button("ÁúåË≠¶ÈÄüÂ†±„ÇíÂÜçÂèñÂæó"):
    st.session_state.pop("_incidents_cache", None)
    st.session_state.pop("_analysis_cache", None)

gazetteer_path = st.sidebar.text_input("„Ç¨„Çº„ÉÉ„ÉÜ„Ç£„Ç¢CSV„Éë„Çπ", "data/gazetteer_ehime.csv")
use_fuzzy = st.sidebar.checkbox("„ÇÜ„Çâ„ÅéÂØæÂøúÔºà„Éï„Ç°„Ç∏„Éº„Éû„ÉÉ„ÉÅÔºâ", True)
min_fuzzy_score = st.sidebar.slider("ÊúÄÂ∞è„Çπ„Ç≥„Ç¢", 60, 95, 78)

st.sidebar.caption("‚Äª ÂèÇËÄÉÊÉÖÂ†±„ÄÇÁ∑äÊÄ•ÊôÇ„ÅØ110/119„ÄÇÁèæÂú∞„ÅÆÊåáÁ§∫„ÇíÂÑ™ÂÖà„ÄÇÂÄã‰∫∫ÊÉÖÂ†±„ÅØË°®Á§∫„Åó„Åæ„Åõ„Çì„ÄÇ")

# -------- HTTP utils --------

def http_get(url: str) -> str:
    r = requests.get(url, timeout=REQUEST_TIMEOUT, headers={"User-Agent": USER_AGENT})
    r.raise_for_status()
    return r.text

# -------- Scraper: Ehime Police --------
@dataclass
class IncidentItem:
    source_url: str
    heading: str
    station: Optional[str]
    incident_date: Optional[str]
    body: str
    fetched_at: str


def parse_ehime_police_page(html: str) -> List[IncidentItem]:
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text("\n", strip=True)
    lines = [ln for ln in text.split("\n") if ln.strip()]

    blocks: List[Dict] = []
    current = None
    for ln in lines:
        if ln.startswith("‚ñ†"):
            if current:
                blocks.append(current)
            current = {"heading": ln.strip(), "body_lines": []}
        else:
            if current:
                current["body_lines"].append(ln.strip())
    if current:
        blocks.append(current)

    out: List[IncidentItem] = []
    today = datetime.now().date()
    cy = today.year

    for b in blocks:
        heading = b.get("heading", "")
        body = " ".join(b.get("body_lines", [])).strip()
        m_date = re.search(r"Ôºà?(\d{1,2})Êúà(\d{1,2})Êó•", heading)
        m_station = re.search(r"Ôºà\d{1,2}Êúà\d{1,2}Êó•\s*([^\sÔºâ]+)Ôºâ", heading)

        incident_date = None
        if m_date:
            m = int(m_date.group(1)); d = int(m_date.group(2))
            y = cy; cand = datetime(y, m, d).date()
            if cand > today: y -= 1
            incident_date = datetime(y, m, d).date().isoformat()

        station = m_station.group(1) if m_station else None
        out.append(IncidentItem(
            source_url=EHIME_POLICE_URL,
            heading=heading,
            station=station,
            incident_date=incident_date,
            body=body,
            fetched_at=datetime.now().astimezone().isoformat(timespec="seconds"),
        ))
    return out

# -------- Gemini (google-generativeai) --------

def gemini_client():
    import google.generativeai as genai
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        st.error("GEMINI_API_KEY „ÅåÊú™Ë®≠ÂÆö„Åß„Åô„ÄÇÁí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        st.stop()
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(GEMINI_MODEL)


def gemini_analyze_items(items: List[IncidentItem]) -> List[Dict]:
    model = gemini_client()
    sys_prompt = (
        "„ÅÇ„Å™„Åü„ÅØÊó•Êú¨„ÅÆË°åÊîøÂêë„ÅëNLP„Ç¢„Éä„É©„Ç§„Ç∂„Åß„Åô„ÄÇ‰∏é„Åà„Çâ„Çå„ÅüÈÄüÂ†±„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÂé≥ÂØÜ„Å™JSON„ÇíËøî„Åó„Åæ„Åô„ÄÇ"
        "‰∫ãÂÆü„ÅÆË£úÂÆå„ÅØÁ¶ÅÊ≠¢„ÄÇÂá∫Âäõ„ÅØ application/json „ÅÆ„Åø„ÄÇÊ¨†Ê∏¨„ÅØ null„ÄÇ"
    )
    results: List[Dict] = []
    for it in items:
        user_prompt = f"""
„ÄêÂá∫ÂäõJSON„Éï„Ç£„Éº„É´„Éâ„Äë
category: "‰∫§ÈÄö‰∫ãÊïÖ"|"ÁÅ´ÁÅΩ"|"‰∫ã‰ª∂"|"Ê≠ª‰∫°‰∫ãÊ°à"|"Á™ÉÁõó"|"Ë©êÊ¨∫"|"„Åù„ÅÆ‰ªñ"
municipality: Â∏ÇÁî∫ÊùëÂêç / ‰∏çÊòé„ÅØ null
place_strings: ÊñΩË®≠„ÉªÂú∞ÂêçÂÄôË£ú„ÅÆÈÖçÂàó
road_refs: ÈÅìË∑ØÂèÇÁÖßÈÖçÂàóÔºà‰æã: ÂõΩÈÅì/ÁúåÈÅìÔºâ
occurred_date: YYYY-MM-DD / ‰∏çÊòé„ÅØ null
occurred_time_text: "Êúù/Êòº/Â§ú/Êú™Êòé/‚óãÊôÇ‚óãÂàÜÈ†É" Á≠â / ‰∏çÊòé„ÅØ null
summary_ja: 120Â≠ó‰ª•ÂÜÖ„ÅÆË¶ÅÁ¥ÑÔºàÂéüÊñáÊ∫ñÊã†„ÉªÊÜ∂Ê∏¨Á¶ÅÊ≠¢Ôºâ
confidence: 0.0„Äú1.0
raw_heading: Ë¶ãÂá∫„Åó
raw_snippet: ÈáçË¶ÅÈÉ®ÂàÜ„ÅÆÂéüÊñáÔºà100„Äú200Â≠óÔºâ

„ÄêÊó¢Áü•„É°„Çø„ÄëÁΩ≤Âêç:{it.station} Êé®ÂÆöÊó•:{it.incident_date} Âπ¥ÁØÑÂõ≤:{datetime.now().year}-01-01„Äú{datetime.now().date().isoformat()}
„ÄêÂÖ•Âäõ„ÄëË¶ãÂá∫„Åó:{it.heading}\nÊú¨Êñá:{it.body}
"""
        gen_cfg = {
            "temperature": 0.2,
            "top_p": 0.9,
            "top_k": 40,
            "max_output_tokens": 1024,
            "response_mime_type": "application/json",
        }
        try:
            resp = model.generate_content([
                {"role":"system","parts":[sys_prompt]},
                {"role":"user","parts":[user_prompt]},
            ], generation_config=gen_cfg)
            txt = (resp.text or "").strip()
            data = json.loads(txt) if txt else {}
        except Exception:
            data = {}

        data.setdefault("category", "„Åù„ÅÆ‰ªñ")
        data.setdefault("municipality", None)
        data.setdefault("place_strings", [])
        data.setdefault("road_refs", [])
        data.setdefault("occurred_date", it.incident_date)
        data.setdefault("occurred_time_text", None)
        data.setdefault("summary_ja", None)
        data.setdefault("confidence", 0.4)
        data.setdefault("raw_heading", it.heading)
        data.setdefault("raw_snippet", it.body[:200])
        data["source_url"] = it.source_url
        data["fetched_at"] = it.fetched_at

        results.append(data)
        time.sleep(random.uniform(*SLEEP_RANGE))
    return results

# -------- Gazetteer (CSV) --------
@st.cache_data(show_spinner=False)
def load_gazetteer(csv_path: str) -> Optional[pd.DataFrame]:
    try:
        df = pd.read_csv(csv_path)
        # expected columns: name, alt_names, type, lon, lat, area_m2(optional)
        for col in ["name","type","lon","lat"]:
            if col not in df.columns:
                st.warning("„Ç¨„Çº„ÉÉ„ÉÜ„Ç£„Ç¢„ÅÆÂàó„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„Åæ„Åô: name,type,lon,lat „ÅØÂøÖÈ†à")
                return None
        df["alt_names"] = df.get("alt_names", "").fillna("")
        return df
    except Exception:
        return None


def gazetteer_lookup(place: str, gaz: pd.DataFrame, use_fuzzy: bool, min_score: int) -> Optional[Tuple[float,float,str]]:
    # 1) direct substring or exact
    m = gaz[(gaz["name"].str.contains(place, na=False)) | (gaz["alt_names"].str.contains(place, na=False))]
    if not m.empty:
        r = m.iloc[0]
        return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore

    # 2) fuzzy (against name + alt_names concatenated)
    if use_fuzzy:
        choices = (gaz["name"] + " | " + gaz["alt_names"].fillna(""))
        match = rf_process.extractOne(place, choices, scorer=fuzz.token_set_ratio)
        if match and match[1] >= min_score:
            idx = match[2]
            r = gaz.iloc[idx]
            return float(r["lon"]), float(r["lat"]), str(r["type"])  # type: ignore

    return None

# Nominatim fallback (respect rate) ‚Äî for production, replace with licensed API

def geocode_nominatim(name: str, municipality: Optional[str]) -> Optional[Tuple[float,float]]:
    try:
        q = f"{name} {municipality or ''} ÊÑõÂ™õÁúå Êó•Êú¨".strip()
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": q, "format": "jsonv2", "limit": 1}
        r = requests.get(url, params=params, timeout=15, headers={"User-Agent": USER_AGENT})
        r.raise_for_status()
        arr = r.json()
        if isinstance(arr, list) and arr:
            lat = float(arr[0]["lat"])  # type: ignore
            lon = float(arr[0]["lon"])  # type: ignore
            return lon, lat
    except Exception:
        return None
    return None

# circle polygon for pydeck

def circle_coords(lon: float, lat: float, radius_m: int = 300, n: int = 64) -> List[List[float]]:
    coords: List[List[float]] = []
    r_earth = 6378137.0
    dlat = radius_m / r_earth
    dlon = radius_m / (r_earth * math.cos(math.radians(lat)))
    for i in range(n):
        ang = 2 * math.pi * i / n
        lat_i = lat + math.degrees(dlat * math.sin(ang))
        lon_i = lon + math.degrees(dlon * math.cos(ang))
        coords.append([lon_i, lat_i])
    coords.append(coords[0])
    return coords

# radius rules

def decide_radius_m(match_type: str) -> int:
    if match_type == "facility":
        return 300
    if match_type == "intersection":
        return 250
    if match_type in ("town","chome"):
        return 600
    if match_type in ("oaza","aza"):
        return 900
    if match_type in ("city","municipality"):
        return 2000
    return 800

# -------- Optional JMA warnings (placeholder) --------

def fetch_jma_warnings_prefecture(pref_code: str = "38") -> List[Dict]:
    try:
        # Placeholder: return [] to avoid unreliable endpoints in MVP
        return []
    except Exception:
        return []

# -------- Data pipeline (cache) --------
@st.cache_data(ttl=300)
def load_incidents() -> List[IncidentItem]:
    html = http_get(EHIME_POLICE_URL)
    return parse_ehime_police_page(html)

@st.cache_data(ttl=300)
def analyze_incidents(items: List[IncidentItem]) -> pd.DataFrame:
    data = gemini_analyze_items(items)
    return pd.DataFrame(data)

# -------- Run pipeline --------
with st.spinner("ÁúåË≠¶ÈÄüÂ†±„ÅÆÂèñÂæó„ÉªËß£Êûê‰∏≠..."):
    items = load_incidents()
    an_df = analyze_incidents(items)

# Sidebar date filter
an_df["occurred_date"] = pd.to_datetime(an_df["occurred_date"], errors="coerce")
min_date = an_df["occurred_date"].min()
max_date = an_df["occurred_date"].max()
if pd.notna(min_date) and pd.notna(max_date):
    dr = st.sidebar.date_input("Áô∫ÁîüÊó•„Éï„Ç£„É´„Çø", value=(min_date.date(), max_date.date()))
    if isinstance(dr, tuple) and len(dr) == 2:
        d0, d1 = pd.to_datetime(dr[0]), pd.to_datetime(dr[1])
        an_df = an_df[(an_df["occurred_date"] >= d0) & (an_df["occurred_date"] <= d1)]

# Category chips
cats = sorted([c for c in an_df["category"].dropna().unique().tolist() if c])
st.write(" ".join([f"<span class='chip on'>{c}</span>" for c in cats]), unsafe_allow_html=True)

# Gazetteer load
gaz_df = load_gazetteer(gazetteer_path) if gazetteer_path else None

# ---- Build map features with gazetteer-first geocoding ----
features = []
for _, row in an_df.iterrows():
    if row.get("category") in ("‰∫§ÈÄö‰∫ãÊïÖ","‰∫ã‰ª∂","Á™ÉÁõó","Ë©êÊ¨∫","Ê≠ª‰∫°‰∫ãÊ°à"):
        if not (show_accidents or show_crimes):
            continue
    # ÁÅΩÂÆ≥„ÅØÂà•ÈÄîÔºàJMAÔºâ

    municipality: Optional[str] = row.get("municipality")
    places: List[str] = row.get("place_strings") or []

    lonlat: Optional[Tuple[float,float]] = None
    mtype = "unknown"

    # try gazetteer
    if gaz_df is not None:
        for ptxt in places:
            g = gazetteer_lookup(ptxt, gaz_df, use_fuzzy, min_fuzzy_score)
            if g:
                lonlat = (g[0], g[1])
                mtype = g[2]
                break
        if not lonlat and municipality:
            g = gazetteer_lookup(municipality, gaz_df, use_fuzzy, min_fuzzy_score)
            if g:
                lonlat = (g[0], g[1])
                mtype = g[2]

    # fallback to OSM if still missing
    if not lonlat:
        for ptxt in places:
            pt = geocode_nominatim(ptxt, municipality)
            if pt:
                lonlat = pt
                mtype = "facility"
                break
            time.sleep(1.0)
        if not lonlat and municipality:
            pt = geocode_nominatim(municipality, None)
            if pt:
                lonlat = pt
                mtype = "city"
            time.sleep(1.0)

    if not lonlat:
        continue

    lon, lat = lonlat
    radius_m = decide_radius_m(mtype)
    conf = float(row.get("confidence", 0.4))
    color = [255, 140, 0, int(40 + min(160, conf*160))]  # opacity by confidence

    props = {
        "title": row.get("category", "„Åù„ÅÆ‰ªñ"),
        "summary": row.get("summary_ja"),
        "municipality": municipality,
        "source": row.get("source_url"),
        "fetched_at": row.get("fetched_at"),
        "confidence": conf,
        "radius_m": radius_m,
        "match_type": mtype,
        "raw_heading": row.get("raw_heading"),
    }
    features.append({
        "type": "Feature",
        "geometry": {"type": "Polygon", "coordinates": [circle_coords(lon, lat, radius_m)]},
        "properties": {**props, "_fill": color},
    })

geojson = {"type":"FeatureCollection","features":features}

# ---- Map ----
view_state = pdk.ViewState(latitude=EHIME_PREF_LAT, longitude=EHIME_PREF_LON, zoom=9)
layer_incidents = pdk.Layer(
    "GeoJsonLayer",
    data=geojson,
    pickable=True,
    stroked=True,
    filled=True,
    get_line_width=2,
    get_line_color=[230,90,20],
    get_fill_color="properties._fill",
    auto_highlight=True,
)

layers = []
if show_accidents or show_crimes:
    layers.append(layer_incidents)

tooltip = {"html": "<b>{title}</b><br/>{summary}<br/><span class='subtle'>{municipality}</span><br/>ÂçäÂæÑ:{radius_m}m / conf:{confidence}",
           "style": {"backgroundColor":"#111","color":"white"}}

deck = pdk.Deck(layers=layers, initial_view_state=view_state, tooltip=tooltip)

col_map, col_feed = st.columns([7,5], gap="large")
with col_map:
    st.pydeck_chart(deck, use_container_width=True)
    st.markdown(
        """
        <div class='legend'>
          <span class='chip'>Âá°‰æã</span> ÂÜÜ„ÅØ„ÄåËøë‰ººÁØÑÂõ≤„Äç„Åß„ÅôÔºà„Éî„É≥„Éù„Ç§„É≥„Éà„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„ÇìÔºâ„ÄÇ
          Âá∫ÂÖ∏URL„Å®ÂèñÂæóÊôÇÂàª„ÇíÂøÖ„ÅöÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        </div>
        """,
        unsafe_allow_html=True,
    )

with col_feed:
    st.subheader("„Ç™„Éº„Éê„Éº„É¨„Ç§Ë¶ÅÁ¥ÑÔºàÈÄüÂ†±Ôºâ")
    # quick filters
    cat_filter = st.multiselect("„Ç´„ÉÜ„Ç¥„É™ÁµûËæº", options=cats, default=cats)

    feed = an_df.copy()
    if cat_filter:
        feed = feed[feed["category"].isin(cat_filter)]

    # Search box for text
    q = st.text_input("„Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢ÔºàË¶ÅÁ¥Ñ/ÂéüÊñáÔºâ")
    if q:
        feed = feed[feed.apply(lambda r: q in (r.get("summary_ja") or "") or q in (r.get("raw_snippet") or ""), axis=1)]

    # Card renderer
    for _, r in feed.iterrows():
        st.markdown("<div class='feed-card'>", unsafe_allow_html=True)
        st.markdown(f"**{r.get('category','„Åù„ÅÆ‰ªñ')}**")
        st.caption(r.get("summary_ja") or "Ë¶ÅÁ¥Ñ„Å™„Åó")
        st.caption(f"{r.get('municipality') or 'Â∏ÇÁî∫Êùë‰∏çÊòé'} / ÂèñÂæó: {r.get('fetched_at')} / conf: {r.get('confidence')}")
        st.link_button("Âá∫ÂÖ∏„ÇíÈñã„Åè", r.get("source_url"), help="ÁúåË≠¶„Éö„Éº„Ç∏")
        st.markdown("</div>", unsafe_allow_html=True)

    if show_disasters:
        st.markdown("---")
        st.subheader("ÁÅΩÂÆ≥ÊÉÖÂ†±ÔºàË≠¶Â†±„ÉªÊ≥®ÊÑèÂ†±Ôºâ")
        jma = fetch_jma_warnings_prefecture("38")
        if not jma:
            st.caption("JMAË≠¶Â†±„ÅÆÂèñÂæó„ÅØÊú™Ë®≠ÂÆö„Åß„Åô„ÄÇ„Ç®„É≥„Éâ„Éù„Ç§„É≥„ÉàÊèê‰æõÂæå„Å´ÂÆüË£Ö„Åó„Åæ„Åô„ÄÇ")
        else:
            for w in jma:
                st.write(w)

# Export area
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    if st.button("Ëß£ÊûêÁµêÊûú„ÇíJSON„Åß„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ"):
        js = an_df.to_json(force_ascii=False, orient="records", indent=2)
        st.download_button("download incidents.json", js, file_name="incidents.json", mime="application/json")
with col2:
    st.caption("¬© Data: ÊÑõÂ™õÁúåË≠¶ ‰∫ã‰ª∂‰∫ãÊïÖÈÄüÂ†± / Geocoding: Gazetteer‚ÜíOSM(Nominatim). „Åì„ÅÆ„Ç¢„Éó„É™„ÅØÂèÇËÄÉÊÉÖÂ†±„Åß„Åô„ÄÇ")

# -------- requirements.txt (reference) --------
# streamlit
# pandas
# pydeck
# requests
# beautifulsoup4
# google-generativeai>=0.8.0
# rapidfuzz
